TuRiNG
图灵程序设计M书
深度学车
的数学
[日]涌井良幸涌井贞美/著杨瑞龙/译
一本书掌握
深度学习的
数学基础知识
结合235幅插图和大量示例
基于Excel实践 直击神经网络根本原理
工中国工信出版集团
人民邮电出版社
POSTS & TELECOM PRESS涌井良幸
1950年生于东京，毕业于东京教育大学
(现筑波大学）数学系，现为自由职业
者。著有《用Excel学深度学习》（合
著）、《统计学有什么用？》等。
涌井贞美
1952年生于东京，完成东京大学理学系研
究科硕士课程，现为自由职业者。著
有《用Excel学深度学习》（合著）、《图
解贝叶斯统计入门》等。
杨瑞龙
1982年生，2008年北京大学数学科学学
院硕士毕业，软件开发者，从事软件行业
10年。2013年~2016年赴日工作3年，从
2016年开始在哆嗒数学网公众号发表《数
学上下三万年》等多篇翻译作品。
自在书装设计
深度学习的数学.indd 10
2019/7/2 10:46:32数字版权声明
图灵社区的电子书没有采用专有客户
端，您可以在任意设备上，用自己喜
欢的浏览器和PDF阅读器进行阅读
但您购买的电子书仅供您个人使用
未经授权不得进行传播
我们愿意相信读者具有这样的良知和
觉悟,1与我们共同保护知识产权
0电
如果购买者有得权行为，我们可能对
010
该用户实施包括但不限于关闭该帐号
等维权措施,并可能追究法律责任。
0
80007
308
0000
38
1010010010
电10101051
自TURiNG 图灵程序设计M书
深度学车
的数学
[日]涌井良幸涌井贞美/著
杨瑞龙/译
人民邮电出版社
北京
深度学习的数学.indd
2019/7/2 10:46:3图书在版编目(CIP)数据
深度学习的数学/(日)涌井良幸，(日)涌井贞美
著:杨瑞龙译.-- 北京:人民邮电出版社，2019.5
(2019.7重印）
(图灵程序设计丛书)
ISBN 978-7-115-50934-5
I.①深…I.国涌…②涌…日杨….D机器学
习IN.①TP181
中国版本图书馆CIP数据核字(2019)第041376号
内容提要
本书基于丰富的图示和具体示例，通俗易懂地介绍了深度学习相关的数学
知识。第1章介绍神经网络的概况:第2章介绍理解神经网络所需的数学基础
知识:第3章介绍神经网络的最优化:第4章介绍神经网络和误差反向传播法
第5章介绍深度学习和卷积神经网络。书中使用 Excel进行理论验证，帮助读
者直观地体验深度学习
本书适合深度学习初学者阅读。
◆著
[日] 涌井良幸涌井贞美
译
杨瑞龙
责任编辑杜晓静
责任印制周昇亮
◆人民邮电出版社出版发行
北京市丰台区成寿寺路11号
邮编100164
电子邮件315@ptpress.com.cn
网址
htp://ww.ptpres.com.cn
北京
印刷
◆开本:88012301/32
印张:7.375
字数:210千字
2019年5月第1版
印数:15 001-20 000册
2019年7月北京第5次印刷
著作权合同登记号
：图字:01-2018-5196号
定价:69.00元
读者服务热线:(010)51095183转600印装质量热线:（010)81055316
反盗版热线:(010)81055315
广告经营许可证：京东工商广登字20170147号
深度学习的数学.indd 2
2019/7/2 10:46:31前言
近年来，我们在媒体上到处可见人工智能（AI）这个词，而深度学
习是人工智能的一种实现方法。下面我们就来简单地看一下深度学习具
有怎样划时代的意义
下面是三张花的图片，它们都具有同一个名字，那究竟是什么呢?
答案是玫瑰。虽然大小和形状都不一样，但这些的确都是攻瑰花的
图片。看到政瑰花的图片，我们理所当然就能辨别出“这是玫瑰花”
在计算机和数学的世界中，这个玫瑰花的例子属于模式识别问题
人类每天都在进行着模式识别。比如，我们在逛街的时候就会无意识地
进行着物体的辨别：“那是电影院”“信号灯是红灯”，等等。换言之，这
就是在进行模式识别
然而，像这样的人类认为很自然的事情，一旦想让机器来做，就变
得非常困难。例如，现在让你编写一个模式识别的计算机程序，使其从
大量花的图片中单独提取出玫瑰花的图片，你可能就束手无策了。
实际上，关于模式识别的理论创建一直在碰壁。例如，对于玫瑰花
的模式识别，以前的逻辑是将“玫瑰是具有这样特征的东西”教给机器
然而效果甚微。因为政瑰花的形状实在是太多了，即使是相同品种的政
瑰花，其颜色和形状每时每刻也都在发生变化，不同品种的玫瑰花则会
有更大的差异。要从如此多样的特征之中得出“政瑰”这样一个概念，
的确是太难了。
深度学习的数学.indd 3
2019/7/2 10:46:31iv前言
后来，一种被称为神经网络的数学方法被研究出来。具体来说，就
是将模拟动物的神经细胞的神经元聚集起来形成网络,然后让这个网络
去观察大量的政瑰花的图片，进行“自学习”。相比之前的模式识别逻
辑，该方法取得了很大的成功。特别是利用称为卷积神经网络的多层结
构的神经网络，甚至可以从图片和视频中识别出人和猫。深度学习就是
用具有这种结构的神经网络实现的人工智能
虽然“自学习”听起来很难,但神经网络运用的数学理论是非常简
单的，基本上是比较基础的数学知识。然而，很多文献大量使用公式和
专业术语，令人难以看透神经网络的本质，这对于今后人工智能的发展
是莫大的不幸和障碍。本书作为人工智能的入门书，目的就是要破除这
种障碍，让所有人都能够体会到神经网络的趣味性。本书的目标是用初
级的数学知识详细地讲解深度学习的思想
只要从本质上理解了基础知识，就可以在应用中大展身手。但愿本
书能够对21 世纪人工智能的发展有所贡献。
最后，本书从策划到最终出版，得到了技术评论社渡边悦司先生的
大力支持，我们借此向他表达深深的谢意。
2017年春
笔者
深度学习的数学.indd4
2019/7/2 10:46:31本书的使用说明
。本书的目的在于提供理解神经网络所需的数学基础知识。为了便于读
者直观地理解，书中使用大量图片，并通过具体示例来介绍。因此
本书将数学的严谨性放在第二位。
·深度学习的世界是丰富多彩的，本书主要考虑阶层型神经网络和卷积
神经网络在图像识别中的应用。
●本书将Sigmoid 函数作为激活函数，除此之外也可以考虑其他函数。
。本书以最小二乘法作为数学上的最优化的基础，除此之外也可以考虑
其他方法。
●神经网络可分为有监督学习和无监督学习两类。本书主要讲解有监督
学习。
。人工智能相关的文献之所以难读，其中一个原因就是各文献所用的符
号不统一。本书采用的是相关文献中常用的符号。
。本书使用Excel进行理论验证。Excel是一个非常优秀的工具，能够在
工作表上可视化地展现逻辑，有助于我们理解。因此，相应的项目需
要以Excel的基础知识为前提。
深度学习的数学.indd 5
2019/7/2 10:46:31Excel示例文件的下载
本书中使用的 Excel 示例文件可以从以下网址下载。
http://www.ituring.com.cn/book/2593
●示例文件的内容
章节
文件名
概要
通过简单的例子确认梯度下降法的
2-11 节
2-11梯度下降法.xlsx
原理
不使用误差反向传播法，直接使用
3-5节
3-5 NN（求解器）.xlsx
Excel执行最优化，确定神经网络
4-4节
4-4NN（误差反向传播法）.xlsx
使用误差反向传播法确定神经网络
不使用误差反向传播法,直接使用
5-4节
5-4 CNN（求解器）.xlsx
Excel执行最优化，确定卷积神经网络
使用误差反向传播法确定卷积神经
5-6节
5-6 CNN（误差反向传播法）.xlsx
网络
附录A
附录A.xlsx
第4章例题的图像数据
附录B
附录B.xlsx
第5章例题的图像数据
注意
·本书基于Excel 2013执笔，不保证示例文件可在其他版本上正常运行。
·示例文件的内容可能会变更
·读者可以随意变更或改良示例文件的内容，但我们不提供支持。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd6
2019/7/2 10:46:31目录
第1章
神经网络的思想
1-1
神经网络和深度学习
2
1-2
神经元工作的数学表示
6
1-3
激活函数：将神经元的工作一般化
12
1-4
什么是神经网络
..18
1-5
用恶魔来讲解神经网络的结构
·23
1-6
将恶魔的工作翻译为神经网络的语言
.31
1-7
网络自学习的神经网络
36
第2章
神经网络的数学基础
2-1
神经网络所需的函数
..40
2-2
有助于理解神经网络的数列和递推关系式
46
2-3
神经网络中经常用到的习符号
51
2-4
有助于理解神经网络的向量基础
.3
2-5
有助于理解神经网络的矩阵基础
·61
2-6
神经网络的导数基础
.65
2-7
神经网络的偏导数基础
..72
2-8
误差反向传播法必需的链式法则
.76
2-9
梯度下降法的基础：多变量函数的近似公式
·80
2-10
梯度下降法的含义与公式
·83
图灵社区会员 ChenyangGao(2339083510@gq.com）专享 尊重版权
深度学习的数学.indd
2019/7/2 10:46:32viii

目录
2-11
用Excel体验梯度下降法
·91
2-12
最优化问题和回归分析
94
第3章
神经网络的最优化
3-1
神经网络的参数和变量
102
3-2
神经网络的变量的关系式
111
3-3
学习数据和正解
114
3-4
神经网络的代价函数
119
3-5
用 Excel体验神经网络
127
第4章
神经网络和误差反向传播法
4-1
梯度下降法的回顾
134
4-2
神经单元误差
141
4-3
神经网络和误差反向传播法
146
4-4 用Excel体验神经网络的误差反向传播法
153
第5章
深度学习和卷积神经网络
5-1
小恶魔来讲解卷积神经网络的结构
168
5-2
将小恶魔的工作翻译为卷积神经网络的语言
174
5-3
卷积神经网络的变量关系式
180
5-4
用Excel体验卷积神经网络
193
深度学习的数学.indd
2019/7/2 10:46:32目录ix
5-5

卷积神经网络和误差反向传播法
200
5-6

用 Excel体验卷积神经网络的误差反向传播法
..·
212
附录
A
训练数据(1)
222
B
训练数据（2)
223
C
用数学式表示模式的相似度
.225
深度学习的数学.indd 9
2019/7/2 10:46:32图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 10
2019/7/2 10:46:32第
S
神经网络的思想

在人工智能领域，神经网络（Neural Network，NN）是
近年来的热门话题，由此发展而来的深度学习更是每天都被经
济和社会新闻提及。本章将概述神经网络是什么，以及数学是
怎样参与其中的。为了帮助大家直观地理解，书中的类比或多
或少有些粗糙，不当之处还请见谅。
深度学习的数学.indd 1
2019/7/2 10:46:322
第1章
神经网络的思想
1-1
神经网络和深度学习
深度学习是人工智能的一种具有代表性的实现方法，下面就让我们
来考察一下它究竞是什么样的技术。
备受瞩目的深度学习
在有关深度学习的热门话题中，有几个被媒体大肆报道的事件，如
下表所示。
年份
事件
在世界性的图像识别大赛ILSVRC中，使用深度学习技术的Supervision方
2012年
法取得了完胜
利用谷歌公司开发的深度学习技术，人工智能从YouTube的视频中识别出
2012年
了猫
2014年
苹果公司将Siri的语音识别系统变更为使用深度学习技术的系统
利用谷歌公司开发的深度学习技术，AlphaGo与世界顶级棋手对决，取得
2016年
了胜利
2016年
奥迪、宝马等公司将深度学习技术运用到汽车的自动驾驶中
如上表所示，深度学习在人工智能领域取得了很大的成功。那么。
深度学习究竟是什么技术呢?深度学习里的“深度”是什么意思呢？为
了解答这个疑问,首先我们来考察一下神经网络,这是因为深度学习是
以神经网络为出发点的。
神经网络
谈到神经网络的想法，需要从生物学上的神经元(neuron）开始
说起。
深度学习的数学.indd2
2019/7/2 10:46:321-1神经网络和深度学习3
从生物学的扎实的研究成果中，我们可以得到以下关于构成大脑的
神经元的知识（1-2节）。
ü）
神经元形成网络
(iil对于从其他多个神经元传递过来的信号，如果它们的和不超过
某个固定大小的值（阈值),则神经元不做出任何反应。
ii）对于从其他多个神经元传递过来的信号，如果它们的和超过某
个固定大小的值（阈值),则神经元做出反应（称为点火)，向
另外的神经元传递固定强度的信号。
(iv）在(ii)和(i）中，从多个神经元传递过来的信号之和中，每个信
号对应的权重不一样。
当信号之和大于阈值时，
信号被输入到神经元中
细胞体判断信号之和
进行点火，并向相邻的
神经元传递信号
AR
O7
食



0
将神经元的工作在数学上抽象化,并以其为单位人工地形成网络
这样的人工网络就是神经网络。将构成大脑的神经元的集合体抽象为数
学模型，这就是神经网络的出发点。
用神经网络实现的人工智能
看过以往的科幻电影、动画片就知道，人工智能是人们很早就有
的想法。那么，早期研究的人工智能和用神经网络实现的人工智能有
哪些不同呢？答案就是用神经网络实现的人工智能能够自己学习过去
的数据。
以往的人工智能需要人们事先将各种各样的知识教给机器，这在工
业机器人等方面取得了很大成功。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 3
2019/7/2 10:46:334第1章神经网络的思想
工业机器人
多数工业机器人使用的都是“人教导
机器”类型的人工智能，很多机器人
掌握了各领域专家的技能。
而对于用神经网络实现的人工智能，人们只需要简单地提供数据即
可。神经网络接收数据后，会从网络的关系中自己学习并理解。
“人教导机器”类型的人工智能的问题
20世纪的“人教导机器”类型的人工智能，现在仍然活跃在各种领
域，然而也有一些领域是它不能胜任的，其中之一就是模式识别。让我
们来看一个简单的例子。
例题有一个用8×8像素读取的手写数字的图像，考虑如何让计算机
判断图像中的数字是否为0。
读取的手写数字的图像如下图所示
这些图像虽然大小和形状各异，但都可以认为正解是数字0。可是
如何将这些图像中的数字是0这个事实教给计算机呢?
要用计算机进行处理，就需要用数学式来表示。然而，像例题这样
的情况，如果使用20 世纪的常规手段，将“0具有这样的形状”教给计
算机，处理起来会十分困难。况目,如下所示，对于写得很难看的字、
深度学习的数学.indd
2019/7/2 10:46:331-1神经网络和深度学习5
读取时受到噪声影响的字，虽然人能够设法辨认出来是0,但要将这种辣
认的条件用数学式表达，并教给计算机，应该是无法做到的
从这个简单的例题中可以看出，“人教导机器”类型的人工智能无
法胜任图像、语音的模式识别，因为要把所有东西都教给计算机是不现
实的。
不过，在20世纪后期，对于这样的问题，人们找到了简单的解决方
法，那就是神经网络以及由其发展而来的深度学习。如前所述，具体来
说就是由人提供数据，然后由神经网络自己进行学习。
如此看来，神经网络似乎有一些不可思议的逻辑。然而，从数学上
来说，其原理十分容易。本书的目的就是阐明它的原理
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 5
2019/7/2 10:46:336第1章
神经网络的思想
1-2
神经元工作的数学表示
就像我们在1-1节看到的那样,神经网络是以从神经元抽象出来的
数学模型为出发点的。下面，我们将更详细地考察神经元的工作，并将
其在数学上抽象化。
整理神经元的工作
人的大脑是由多个神经元互相连接形成网络而构成的。也就是说
个神经元从其他神经元接收信号，也向其他神经元发出信号。大脑就
是根据这个网络上的信号的流动来处理各种各样的信息的。
轴突
突触
细胞体
树突
轴突
神经元示意图
神经元主要由细胞体、轴突、树突等构成。树突是从其他神经元接收信号的突起。轴
突是向其他神经元发送信号的突起。由树突接收的电信号在细胞体中进行处理之后，
通过作为输出装置的轴突，被输送到其他神经元。另外，神经元是借助突触结合而形
成网络的。
让我们来更详细地看一下神经元传递信息的结构。如上图所示，神
经元是由细胞体、树突、轴突三个主要部分构成的。其他神经元的信号
(输入信号）通过树突传递到细胞体（也就是神经元本体）中，细胞体把
深度学习的数学.indd
2019/7/2 10:46:331-2神经元工作的数学表示7
从其他多个神经元传递进来的输入信号进行合并加工，然后再通过轴突
前端的突触传递给别的神经元。
那么，神经元究竞是怎样对输入信号进行合并加工的呢？让我们来
看看它的构造。
假设一个神经元从其他多个神经元接收了输入信号，这时如果所接
收的信号之和比较小，没有超过这个神经元固有的边界值（称为阈值)，
这个神经元的细胞体就会忽略接收到的信号，不做任何反应。
信号被输入到神经元
细胞体判断信号之和
当信号之和小于阈值时
就忽略
.
A
一
人
大

→A
注：对于生命来说，神经元忽略微小的输入信号，这是十分重要的。反之，如果神经元
对于任何微小的信号都变得兴奋，神经系统就将“情绪不稳定”。
不过，如果输人信号之和超过神经元固有的边界值（也就是阈值)，
细胞体就会做出反应，向与轴突连接的其他神经元传递信号,这称为
点火。
当信号之和大于阈值时
信号被输入到神经元
细胞体判断信号之和
进行点火，并向相邻的
神经元传递信号



那么，点火时神经元的输出信号是什么样的呢？有趣的是，信号的
大小是固定的。即便从邻近的神经元接收到很大的刺激，或者轴突连接
着其他多个神经元，这个神经元也只输出固定大小的信号。点火的输出
信号是由0或1表示的数字信息。
深度学习的数学.indd 7
2019/7/2 10:46:338第1章神经网络的思想
神经元工作的数学表示
让我们整理一下已经考察过的神经元点火的结构
(i）来自其他多个神经元的信号之和成为神经元的输人
(ii）如果这个信号之和超过神经元固有的阈值，则点火。
il神经元的输出信号可以用数字信号0和1来表示。即使有多个输
出端，其值也是同一个。
下面让我们用数学方式表示神经元点火的结构。
首先，我们用数学式表示输入信号。由于输入信号是来自相邻神经
元的输出信号，所以根据（iii)，输入信号也可以用“有”“无”两种信息
表示。因此，用变量x表示输入信号时，如下所示。
无输入信号:x=0
有输入信号：x=1
无输
有输入
神经元的输入信
号可以用数字信
号x=0,1表示。
注：与视细胞直接连接的神经元等个别神经元并不一定如此，因为视细胞的输入是模拟
信号。
接下来，我们用数学式表示输出信号。根据(il，输出信号可以用表
示点火与否的“有”“无”两种信息来表示。因此，用变量，表示输出信
号时，如下所示。
无输出信号：y=0
有输出信号：y=l
深度学习的数学.indd 8
2019/7/2 10:46:331-2神经元工作的数学表示9
无输出（无点火
有输出（有点火）
神经元的输出信号可用
Oo
数字信号，=0,1表示。
图中神经元虽然有两个
输出端,但其输出信号
的大小相同。
最后，我们用数学方式来表示点火的判定条件。
从(i)和(ii)可知，神经元点火与否是根据来自其他神经元的输入信
号的和来判定的，但这个求和的方式应该不是简单的求和。例如在网球
比赛中，对干来自视觉神经的信号和来自听觉神经的信号，大脑是通过
改变权重来处理的。因此，神经元的输入信号应该是考虑了权重的信号
之和。用数学语言来表示的话，例如，来自相邻神经元1、2、3的输入
信号分别为x1、、x，则神经元的输人信号之和可以如下表示。
W,X, + W,X, + W,X,
(1)
式中的w、W2、w,是输人信号x、X2、X；对应的权重（weight ）。
来自神经元1的信号x1
、权重
来自神经元2的信号
对于来自其他神经元的输入
权重w2
信号、2、g,神经元将
其乘以权重w、W2、Wg作
来自神经元3的信号x（权重w
为输入信号，如式(1)所示。
根据i),神经元在信号之和超过阈值时点火，不超过阈值时不点
火。于是，利用式（1)，点火条件可以如下表示。
无输出信号（y=0):w,x,+ w,x, +wx,<0
有输出信号（y=1):w,-,+w,x,+w,x,>0
(2)
这里，0是该神经元固有的阈值。
深度学习的数学.indd 9
2019/7/2 10:46:3410
第1章神经网络的思想
例1来自两个神经元1、2的输入信号分别为变量1、2，权重为w
wz，神经元的阈值为。当w·=5,w2=3,0=4时，考察信号之和wx
+ wzx，的值与表示点火与否的输出信号，的值。
输入x
输入x2
和wx,+ W22
点火
输出信号y
0
0
5x0+3x0=0<4
无
0
0
1
5x0+3×1=3<4
无
0
1
0
5x1+3×0=5=4
有
1
1
\
5x1+3×1=824
有
\
点火条件的图形表示
下面我们将表示点火条件的式(2)图形化。以神经元的输入信号之和
为横轴，神经元的输出信号，为纵轴，将式(（2)用图形表示出来。如下图
所示，当信号之和小于0时，y取值0，反之y取值1。
将点火条件图形化。
横轴表示信号之和
0
W1Xi+ W2X2+ w3X3
9
W,Xq + W2X2 + W3X3o
如果用函数式来表示这个图形，就需要用到下面的单位阶跃函数。
0(z<0)
u(z)=
1 (z>0)
单位阶跃丽数的图形如下所示。
深度学习的数学.indd 10
2019/7/2 10:46:341-2神经元工作的数学表示11
y=u(z)
单位阶跃函数y=(z)
U
利用单位阶跃函数)，式（2)可以用一个式子表示如下。
点火的式子：y=u(w,x+ w,x, + w,x,-O
(3)
通过下表可以确认式(3)和式(2)是一样的
s
W,Xq + W2X2 + WgX3
Z= W,X,+ W2N2+WgKg=0
u(a)
0（无点火）
小于の
z<0
0
1（点火）
大于等于0
z>0

此外，该表中的z(式(3)的阶跃函数的参数）的表达式
z = W,X, + W,x, + w,x, - 0
(4)
称为该神经元的加权输入。
MMe1
备注》WX1+ W2r2+ W3rs= 0的处理
有的文献会像下面这样处理式(2)的不等号。
无输出信号（y=0): wx+wX,+w,x,<0
有输出信号（y=1):w,x,+w,x,+ w,x,>0
在生物上这也许是很大的差异，不过对于接下来的讨论而言是没有问题
的。因为我们的主角是 Sigmoid函数，所以不会发生这样的问题。
深度学习的数学.indd 1
2019/7/2 10:46:3412
第1章神经网络的思想
1-3
激活函数：将神经元的工作一般化
1-2节中用数学式表示了神经元的工作。本节我们试着将其在数学
上一般化。
简化神经元的图形
为了更接近神经元的形象,1-2节中将神经元表示为了下图的
样子。
输入x
权重w
阈值0
神经元的示意图（3个
输入x2
权重
输入、2个输出的情

输出y
况)。轴突分岔为两个输
输入xx 权重v
出端，其输出值相同。
然而，为了画出网络，需要画很多的神经元，在这种情况下上面那
样的图就不合适了。因此，我们使用如下所示的简化图，这样很容易就
能画出大量的神经元。
输入x
权重
该图是神经元的简化图用
输入x
输出，
箭头方向区分输入和输出。
输入x：-
神经元的输出由两个箭头指
阈值О
出，其值是相同的。
为了与生物学的神经元区分开来，我们把经过这样简化、抽象化的
神经元称为神经单元（unit）。
注：很多文献直接称为“神经元”。本书为了与生物学术语“神经元”区分，使用“社
经单元”这个称呼。另外，也有文献将“神经单元”称为“人工神经元”，但是由于
现在也存在生物上的人工神经元，所以本书中也不使用“人工神经元”这个称呼。
深度学习的数学.indd 12
2019/7/2 10:46:351-3激活函数：将神经元的工作一般化
13
激活函数
将神经元的示意图抽象化之后，对于输出信号，我们也对其生物上
的限制进行一般化。
根据点火与否，生物学上的神经元的输出，分别取值1和0（下图）。
无输出(无点火）
有输出(有点火）
人
点火与否用
1和0表示。
然而，如果除去“生物”这个条件，这个“0和1的限制”也应该是
可以解除的。这时表示点火与否的下式（1-2节式(3)）就需要修正
点火的式子:y=u(w,x+w.ry,+w,x,-）
(1)
这里,u是单位阶跃函数。我们将该式一般化，如下所示。
y = a(w,x, + wX5, + w,x, - 0)
(2)
这里的函数α是建模者定义的函数，称为激活函数（activation function）
x1、X2、x,是模型允许的任意数值，y是函数α能取到的任意数值。这个
式(2）就是今后所讲的神经网络的出发点。
注：虽然式(2)只考虑了3个输入，但这是很容易推广的。另外，式(1）使用的单位阶
跃函数u(z)在数学上也是激活函数的一种。
请注意，式(2)的输出，的取值并不限于 0和1，对此并没有简单的
解释。一定要用生物学来比喻的话，可以考虑神经单元的“兴奋度”“反
应度”“活性度”。
我们来总结一下神经元和神经单元的不同点，如下表所示。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 13
2019/7/2 10:46:3514
第1章
神经网络的思想
神经元
神经单元
输出值y
0或1
模型允许的任意数值
激活函数
单位阶跃丽数
由分析者给出，其中著名的是Sigmoid函数（后述）
输出的解释
点火与否
神经单元的兴奋度、反应度、活性度
输出值只有0和1
输出可以是任意数值
输入x
）权重
输入x
权重
2W1
阈值0
输入x
输入x
W2
输出y
A
输出y
任意数值
权重
只有0和1
输入x
阈值0
输入x：（权重w
y =u(z)
a(x)
1e
单位阶跃函数
Sigmoid 函数等

0T
0
激活函数
激活函数
将神经元点火的式(1)一般化为神经单元的激活函数式(2)，要确认
这样做是否有效，就要看实际做出的模型能否很好地解释现实的数据。
实际上，式(2）表示的模型在很多模式识别问题中取得了很好的效果
Sigmoid 函数
激活函数的代表性例子是 Sigmoid 函数o()，其定义如下所示。
1
o(z)=
(e=2.718281... )
(3)
1+e"
关于这个函数，我们会在后面详细讨论（2-1节)。这里先来看看它的
图形，Sigmoid函数o(z)的输出值是大于0小于1 的任意值。此外，该丽数
连续、光滑，也就是说可导。这两种性质使得 Sigmoid 函数很容易处理
深度学习的数学.indd 14
2019/7/2 10:46:351-3激活函数：将神经元的工作一般化
15
y=u(z)
y=o(z)
电
单位阶跃函数
个N
-5
0
5
OS
右图是激活函数的代表性例子 Sigmoid函数r(z)的图形。除了原点附近的部分，其余
部分与单位阶跃函数（左图）相似。Sigmoid函数具有处处可导的性质，很容易处理。
单位阶跃函数的输出值为1或0，表示点火与否。然而，Sigmoid 函
数的输出值大于0小于1,这就有点难以解释了。如果用生物学术语来解
释的话，如上文中的表格所示，可以认为输出值表示神经单元的兴奋度
等。输出值接近1表示兴奋度高，接近0则表示兴奋度低。
=(z)
输入x
输入
输b
输入
输出1
输入
阈值0
阈值0
0
神经单元的兴奋度小
神经单元的兴奋度大
本书中将 Sigmoid 函数作为标准激活函数使用，因为它具有容易
计算的漂亮性质。如果用数学上单调递增的可导函数来代替，其原理也
是一样的。
偏置
再来看一下激活函数的式(2)。
y = a(Wwx, + W,.&, + w,x; = 0)
(2)
这里的0称为阈值，在生物学上是表现神经元特性的值。从直观上讲，
表示神经元的感受能力，如果值较大，则神经元不容易兴奋（感觉迟
深度学习的数学.indd 15
2019/7/2 10:46:3616
第1章神经网络的思想
钝),而如果值较小,则神经元容易兴奋（敏感）。
然而，式(2)中只有0带有负号，这看起来不漂亮。数学不喜欢不漂亮的
东西。另外，负号具有容易导致计算错误的缺点，因此，我们将一日替换为b
y = a(w,x, + W,X, t w,x, + b)
(4)
经过这样处理，式子变漂亮了，也不容易发生计算错误。这个b称
为偏置（bias）。
输入X1
权重
输入x
输入X、2、Xg，权重
输出，
W、W、W,偏置b
以及输出，在式(4）中
输入x
偏置b
联系了起来
本书将式(4)作为标准使用。另外，此时的加权输人z(1-2节）如下所示。
z=x,+w,X, +Wjx, +b
(5)
人
式(4)和式(5)是今后所讲的神经网络的出发点，非常重要。
另外，生物上的权重w、w2、Ws和阈值0（=-b）都不是负数，因
为负数在自然现象中实际上是不会出现的。然而，在将神经元一般化的
神经单元中，是允许出现负数的
问题右图是一个神经单元。如图所
示，输人x，的对应权重是2，输入x2
输入

的对应权重是3,偏置是-1。根据下
输出y

表给出的输入，求出加权输入z和输
输入
偏置-
出y。注意这里的激活函数是 Sigmoid
：函数。
输入x,
输入x2
加权输入z
输出
0.2
0.1
0.6
0.5
深度学习的数学.indd 16
2019/7/2 10:46:361-3激活函数：将神经元的工作一般化
17
解结果如下表所示（式(3)中的e取e=2.7进行计算）。
输入x
输入X2
加权输入z
输出y
0.2
0.1
2 x0.2 + 3 x 0.1 - 1 =- 0.3
0.43
0.6
0.5
2 x 0.6+ 3 x 0.5- 1= 1.7
0.84
..(备注改写式(5)
我们将式(5)像下面这样整理一下。
z = w,x, + W,x + Wx, + b x1
(6)
这里增加了一个虚拟的输入，可
输入1
以理解为以常数1作为输入值(右图）。
于是，加权输入z可以看作下面
输入
两个向量的内积。
输入x
输出)
( wi, W2, W3, b)(x1, X2, X3, 1 )
输入x3
计算机擅长内积的计算，因此按照这种解释，计算就变容易了。
深度学习的数学.indd 17
2019/7/2 10:46:3618
第1章神经网络的思想
1-4
什么是神经网络
神经网络作为本书的主题，它究竞是什么样的呢？下面让我们来看
-下其概要。
神经网络
上一节我们考察了神经单元，它是神经元的模型化。那么，既然大
脑是由神经元构成的网络，如果我们模仿着创建神经单元的网络，是不
是也能产生某种“智能”呢？这自然是让人期待的。众所周知，人们的
期待没有被辜负，由神经单元组成的网络在人工智能领域硕果累累
在进入神经网络的话题之前，我们先来回顾一下上一节考察过的神
经单元的功能。
·将神经单元的多个输人入x., x,.,x，整理为加权输入za
z =,X, +w,K, +...+ w,x,+ b
(1)
其中 w.,w2, …, w,为权重，b为偏置，n为输入的个数
·神经单元通过激活函数a(z)，根据加权输入z输出y。
y= a(z)
(2)
输入x1
权重
输入
输出，
神经单元具有如上总结的
输入
运算功能。另外，即使有
偏置b
多个输出，其值也相同。
将这样的神经单元连接为网络状，就形成了神经网络。
深度学习的数学.indd 18
2019/7/2 10:46:371-4什么是神经网络
19
网络的连接方法多种多样，本书将主要考察作为基础的阶层型神经
网络以及由其发展而来的卷积神经网络。
注：为了与生物学上表示神经系统的神经网络区分开来，有的文献使用“人工神经网络”
这个称呼。本书中为了简便，省略了“人工”二字。
神经网络各层的职责
阶层型神经网络如下图所示,按照层（layer）划分神经单元,通过
这些神经单元处理信号，并从输出层得到结果，如下图所示。
输入层
隐藏层(中间层）
输出层
阶层型神经网络的示例。
除了阶层型以外，还有
“互相连接型”等各种类
型的网络。
构成这个网络的各层称为输入层、隐藏层、输出层，其中隐藏层也
被称为中间层
各层分别执行特定的信号处理操作
输人层负责读取给予神经网络的信息。属于这个层的神经单元没有
输入箭头，它们是简单的神经单元，只是将从数据得到的值原样输出。
隐藏层的神经单元执行前面所复习过的处理操作(1)和(2)。在神经
网络中，这是实际处理信息的部分。
输出层与隐藏层一样执行信息处理操作(1)和(2)，并显示神经网络
计算出的结果，也就是整个神经网络的输出。
深度学习的数学.indd 19
2019/7/2 10:46:3720
第1章神经网络的思想
深度学习
深度学习，顾名思义，是叠加了很多层的神经网络。叠加层有各种
各样的方法，其中著名的是卷积神经网络（第5章）
考察具体的例子
从现在开始一直到第4章，我们都将围绕着下面这个简单的例子来
考察神经网络的结构
例题建立一个神经网络，用来识别通过4×3像素的图像读取的手写
数字0和1。学习数据是64张图像，其中像素是单色二值
解 我们来示范一下这个例题如何解答。
输入层
隐藏层
输出层
O
输入各个像素
信息
图像
C
O
®
C
4×3像素
O
(
作为例题解答的神经网
络示例。这个示例将手
写数字 1作为单色二值
图像读入。
这个解答是演示实际的神经网络如何发挥功能的最简单的神经网络
深度学习的数学.indd 20
2019/7/2 10:46:371-4什么是神经网络
21
示例，但对于理解本质已经足够了。该思路也同样适用于复杂的情况。
注：例题的解答有很多种，并不仅限于这一示例。
这个简单的神经网络的特征是，前一层的神经单元与下一层的所有
神经单元都有箭头连接，这样的层构造称为全连接层（fully connected
layer)。这种形状对于计算机的计算而言是十分容易的。
下面让我们来简单地看一下各层的含义。
解答示例中输入层的含义
输入层由12个神经单元构成,对此我们立刻就能够理解，因为神经
网络一共需要读取4×3=12个像素信息
输入层
®
C2

4×3像素
输入层的神经单元总数为
12个。X1,X2，
…，X2为图
D
像数据的12 个像素的值。
输人层的神经单元的输入与输出是相同的。一定要引入激活函数a(z)
的话，可以用恒等丽数（a(z)=z）来充当。
解答示例中输出层的含义
输出层由两个神经单元构成，这是因为我们的题目是识别两种手写
数字0和1，需要一个在读取手写数字0时输出较大值（即反应较大）的
神经单元，以及一个在读取手写数字1时输出较大值的神经单元
例如，将Sigmoid 函数作为激活函数使用。在这种情况下，读取数字
0的图像时，输出层上方的神经单元的输出值比下方的神经单元的输出值
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 21
2019/7/2 10:46:3722
第1章神经网络的思想
大；而读取数字1的图像时，输出层下方的神经单元的输出值比上方的神
经单元的输出值大，如下图所示。像这样，根据输出层的神经单元的输
出的大小，对整个神经网络进行判断。
图像
输出层
图像
输出层
（对数字0的反应）
（对数字0的反应）
(）一一输出接近1的值
(）输出接近0的值

一>输出接近0的值
一》输出接近1的值
(对数字1的反应）
(对数字1的反应）
0的图像
1的图像
解答示例中隐藏层的含义
隐藏层具有提取输人图像的特征的作用。然而，隐藏层为何能够提
取输入图像的特征呢？这不是一个简单的话题。另外,在这个解答示例
中，隐藏层为何是1层而不是2层？为何是由3个神经单元构成而不是5
个?想必读者会涌现出诸多疑问。为了解决这些疑问,就需要理解下一
节所讲的神经网络的结构。
MMe
:....备注建立神经网络的经验谈
在上面的例题中，也可以考虑将输出层的神经单元整合为一个，以其输
出接近0或接近1来区分输入数字0和1。要说该方法与采用两个神经单元
的解答示例相比理论上哪一个更好，这在数学上无法判断。根据现有的经验
在用计算机进行计算时，对于两个字的识别，使用两个神经单元的神经网络
结构比较简单,识别也容易进行。
深度学习的数学.indd 22
2019/7/2 10:46:371-5用恶魔来讲解神经网络的结构
23
1-5
用恶魔来讲解神经网络的结构
上一节我们概述了神经网络的，但没有具体介绍其中最难的隐藏层
这是因为隐藏层肩负着特征提取（feature extraction）的重要职责，需要
很长的篇幅来介绍。本节我们就来好好看一下隐藏层
重要的隐藏层
如上一节考察过的那样，神经网络是将神经单元部署成网络状而形
成的。然而，将神经单元胡乱地连接起来并不能得到有用的神经网络，
因此需要设计者的预估，这种预估对于隐藏层是特别重要的。因为支撑
整个神经网络工作的就是这个隐藏层。下面让我们利用上一节考察过的
例题，来逐渐展开有关隐藏层的具体话题。
例题建立一个神经网络，用来识别通过4×3像素的图像读取的手写
数字0和1。学习数据是 64张图像，其中像素是单色二值
前面已经提到过，模式识别的难点在于答案不标准,这个例题也体
现了这样的特性。即使是区区一个4×3像素的二值图像，所读人的手写
数字0和1的像素模式也是多种多样的。例如，下列图像可以认为是读
人了手写数字0。
手写数字0的图像。
深度学习的数学.indd 23
2019/7/2 10:46:3724
第1章神经网络的思想
对于这样的数字0，即使人能设法识别出来，让计算机来判断也十分
困难。
思路：由神经单元之间的关系强度给出答案
对于这种没有标准答案、识别困难的问题，怎么解决才好呢？思路
就是“由网络进行判断”。乍一听会觉得这个方法不可思议，不过其中的
逻辑却一点都不难，我们可以用恶魔组织的信息网络来做比喻。虽然这
个比喻并不算准确,但是可以突出其本质。
假设有一个如下图所示的恶魔组织,隐藏层住着3个隐藏恶魔A、
B、C,输出层住着2个输出恶魔0和1。输人层有12个手下①～2为隐
藏恶魔A、B、C服务
注：这里将生物学中的特征提取细胞的工作抽象化为3个恶魔A、B、C.
输出层
输出恶魔0
输出恶魔1
隐藏层
隐藏
隐藏
恶魔A
恶魔C
输入层
手下
1(2)
（3
6）
10
11）（12
最下层（输入层）的12个手下分别住在4×3像素图像的各个像素
上，其工作是如果像素信号为 OFF（值为0）就处于休眠状态；如果像素
信号为ON(值为1）则变得兴奋，并将兴奋度信息传递给他们的主人隐
藏恶魔A、B、C
注：即便不是黑白二值像素的情况，处理方式也是相同的。
深度学习的数学.indd 24
2019/7/2 10:46:371-5用恶魔来讲解神经网络的结构
25
234500012
住在各个像素之上
o\bOr
的12个手下，每个
人读入自己所负责
Loo
的像素信息，信号
W寸I
为ON就变得兴奋。
住在隐藏层的3个隐藏恶魔，从下层（输入层）的12个手下那里获
得兴奋度信息。接着，将获得的信息进行整合，根据其值的大小，自己
也变兴奋，并将这个兴奋度传递给住在上层的输出恶魔
不过，隐藏恶魔A、B、C有不同的喜好。他们分别喜欢下图所示的
模式A、模式B、模式C的图案。这个性质影响了神经网络的特性。(看
清他们的不同“偏好”，就是我们最初所提及的设计者的预估。）
隐藏恶魔A、B、
C分别喜欢模式
模式A
模式B
模式C
A、B、Co
住在最上层的2个输出恶魔也是从住在下层的3个隐藏恶魔那里得
到兴度信息。与隐藏恶魔一样,他们将得到的兴奋度信息进行整合
根据其值的大小，自己也变兴奋。然后，这些输出恶魔的兴奋度就成为
整个恶魔组织的意向。如果输出恶魔0的兴在度比输出恶魔1的兴奋度
大，神经网络就判定图像的数字为0，反之则判定为1。
输出层
输出恶魔О
NA
输出恶魔1
兴奋度
X
小
→判定数字为0
兴奋度
小
*
判定数字为1
深度学习的数学.indd 25
2019/7/2 10:46:3826
第1章神经网络的思想
可见，恶魔的世界里也存在着人际关系
隐藏恶魔A、B、C对模式有着各自的偏好，与12个手下有不同的
交情。隐藏恶魔A的偏好是之前的模式A,因此与④、性情相投。因
为模式A的4号像素与7号像素是ON,所以理所当然地与对应的看守人
④、口性情相投
隐藏恶魔A喜欢
模式A，因此与
隐藏恶魔A
手下④、⑦性情
模式A
相投。
同样地，手下⑤、⑧与隐藏恶魔B,手下6、9与隐藏恶魔C性情
相投，因此他们之间传递兴奋度的管道也变粗了（下图）
隐藏层
隐藏恶魔A
隐藏恶魔B
隐藏恶魔C
输入层
手下
03567
(8)9)
(10
(11(12
粗线表示性情相投。
住在隐藏层的隐藏恶魔A、B、C与住在上层的2个输出恶魔也有着
人际关系。由于某种羁绊,输出恶魔0与隐藏恶魔A、C性情相投,而输
出恶魔 1与隐藏恶魔 B性情相投
深度学习的数学.indd 26
2019/7/2 10:46:381-5用恶魔来讲解神经网络的结构
27
输出恶魔О
输出恶魔1
隐藏
恶魔A
火
隐藏
a
隐藏
M
与之前的图一
恶魔日
恶魔C
样，粗线表示
性情相投。
以上就是恶魔组织的所有人际关系。除了隐藏恶魔A、B、C有不一
样的偏好以外，这是一个人类社会中到处都可能存在的简单的组织。
那么，这里让我们读入手写数字0。
数字0的模式
于是，作为像素看守人的手下今、⑦和手下6、⑨看到这个图像就
变得非常兴奋了（下图）。
1(2(3(4(5(6(7(8(9(10(11(12
nYOt
四OE
④、⑦、6、⑨兴奋起来了
这时，兴奋的手下④、⑦向性情相投的隐藏恶魔A传递了较强的兴奋
度信息，兴奋的手下6、9也向性情相投的隐藏恶魔C传递了较强的兴在
度信息。相对地，几乎没有手下向隐藏恶魔B传递兴奋度信息（下图）。
深度学习的数学.indd 27
2019/7/2 10:46:3828
第1章神经网络的思想
隐藏恶魔A
隐藏恶魔B
隐藏恶魔C
手7
1）(2）(3）
(4
（5)
(6)
(7
8
(10)
12
手下④、⑦和手下⑥、⑨分别向隐藏恶魔A、隐藏恶魔C传递较强的兴奋度信息。
接收了来自手下的兴奋度信息的隐藏恶魔们会怎样呢？接收了较强的兴
奋度信息的隐藏恶魔A和隐藏恶魔C自然也变兴奋了。另一方面,隐藏恶魇
B变得怎样呢？因为几乎没有从手下接收到兴奋度信息，所以一直保持冷静。
兴奋
冷静
兴奋
隐藏恶魔A、C
隐藏恶魔A
隐藏恶魔B
隐藏恶魔C兴奋，B冷静。
住在最上层的输出恶魔交得怎样了呢？输出恶魔0中于与兴奋的隐藏恶
魔A、C关系亲密，从而获得了较强的兴在度信息，所以自己也兴在起来了。
相对地，输出恶魔1与隐藏恶魔A、C关系疏远，而与之关系亲密的隐藏恶魔
B一直保持冷静，所以输出恶魔1没有获得兴奋度信息，因此也保持冷静。
兴奋
冷静
输出恶魔0
输出恶魔1
隐藏
隐藏
隐藏
恶魔之间的关系导
恶魔A
恶魔E
恶魔C
致“输出恶魔0兴
奋，输出恶魔1冷
兴奋
冷静
兴奋
静”这样的状态
深度学习的数学.indd 28
2019/7/2 10:46:381-5用恶魔来讲解神经网络的结构
29
这样一来，读取手写数字0的图像后，根据恶魔之间的连锁关系。
最络得到了“输出恶魔0兴在，输出恶魔1冷静”的结果。根据前文中
的“如果输出恶魔0的兴奋度比输出恶魔1的兴奋度大，神经网络就判
断图像的数字为0”，恶魔的网络推导出了0这个解答。
判定数字为0
输出层
兴奋
冷静
输出恶魔0
输出恶魔1
恶魔的网络成
功地推导出了
О这个解答。
恶魔的心的偏置
在这个恶魔组织中，下层的兴在度会或多或少地传递到上层。但是
除了具有亲密关系的各层之间传递的兴奋度信息以外，还遗漏了少量信
息，就是“噪声”。如果这些噪声迷住了恶魔的心，就会导致无法正确地
传递兴奋度信息。因此，这就需要减少噪声的功能。对于恶魔组织的情
形，我们就将这个功能称为“心的偏置”吧！具体来说，将偏置放在恶
魔的心中，以忽略少量的噪声。这个“心的偏置”是各个恶魔固有的值
(也就是个性)
从关系中得到信息
像上面那样，恶魔组织实现了手写数字的模式识别。我们应该关注
到，是恶魔之间的关系（也就是交情）和各个恶魔的个性（也就是心的
偏置）协力合作推导出了答案。也就是说,网络作为一个整体做出了
判断。
深度学习的数学.indd 29
2019/7/2 10:46:3830
第1章神经网络的思想
问题在图中示范一下在读取数字1的图像时，恶魔组织得出1这个解
答的全过程
解在这种情况下，也能够根据上层恶魔与下层恶魔之间交情的好环来
判断图像中的数字是1。下图就是解答。沿着下图的粗线，输出恶魔1
兴奋起来，判断出图像中的数字是1
判定数字为1
冷静
输出层
输出恶魔0
输出恶魔1
隐藏层
隐藏
恶魔
输入层
像素5、8ON
Lnoo
→手下⑤、8兴奋
寸
>隐藏恶魔B兴奋
输出恶魔1兴奋
深度学习的数学.indd 30
2019/7/2 10:46:381-6将恶魔的工作翻译为神经网络的语言
31
1-6
将恶魔的工作翻译为神经网络的语言
上一节我们通过恶魔讲解了神经网络的结构。本节我们将恶魔的工
作用神经网络的语言来描述。
恶魔之间的“交情”表示权重
上一节考察了恶魔组织识别手写数字0、1的结构。将这个组织替换
为神经网络，我们就能理解神经单元发挥良好的团队精神进行模式识别
的结构
首先，将恶魔看作神经单元。隐藏层住着3个隐藏恶魔A、B、C,
可以解释为隐藏层有3个神经单元A、B、C。输出层住着2个输出恶魔
0、1，可以解释为输出层有2个神经单元0、1。此外，输入层住着12个
恶魔的手下，可以解释为输入层有 12个神经单元（下图）。
输入层
隐藏层输出层
Q
e
③
前出
④
A
®
O
®
O
Q
®
③
®
13466689002
0y
®
接下来，将恶魔的“交情”看作神经单元的权重。隐藏恶魔A与手
下④、⑦性情相投，这样的关系可以认为是从输人层神经单元④、⑦指
深度学习的数学.indd 3132
第1章神经网络的思想
向隐藏层神经单元A的箭头的权重较大。同样地，隐藏恶魔B与手下5
⑧性情相投，可以认为是从输人层神经单元⑤、⑧指向隐藏层神经单元B
的箭头的权重较大。隐藏恶魔C与手下6、9性情相投，可以认为是从
输入层神经单元6、⑨指向隐藏层神经单元C的箭头的权重较大。
输入层
隐藏层
a
2
®
④
A
隐藏恶魔A
隐藏恶魔
隐藏恶魔C
⑤
®
⑦
®
®
C
10
1234560900鱼
粗线表示性情相投。
S
®
粗线表示权重较大。
注：关于权重，请参考1-2节、1-3节。
隐藏恶魔A、C与上层的输出恶魔0 性情相投，这个关系表示从隐藏
层神经单元A、C指向输出层神经单元0的箭头的权重较大。同样地，隐
藏恶魔B 与输出恶魔1性情相投，这个关系表示从隐藏层神经单元B指
向输出层神经单元1的箭头的权重较大。
隐藏层
输出层
输出恶魔0
输出恶魔1
隐藏
隐藏
隐藏
恶魔A
恶魔
恶魔
深度学习的数学.indd 32
2019/7/2 10:46:391-6将恶魔的工作翻译为神经网络的语言
33
这样解释的话，神经网络读人手写数字0时，神经单元A和C的输
出值较大，输出层神经单元0的输出值较大。于是，根据神经网络整体
的关系，最终识别出数字0。
输入层
隐藏层
输出层
O
2
③
输入各个
像素信息
4
®
OS
较大值
⑥
BX
>输出0

(1
较小值
4×3像素
®
O
C
厂
0
m
2
根据神经单元的关系能够识别出数字。
在像这个神经网络那样前一层与下一层全连接的情况下,在输入0
的图像时，原本不希望做出反应的隐藏层神经单元B以及输出层神经单
元1也有信号传递，因此需要禁止这样的信号并使信号变清晰，这样的
功能就是偏置，在恶魔组织中表现为“心的偏置”
如上所述，权重和偏置的协力合作使得图像识别成为可能。这就是
“由神经网络中的关系得出答案”的思想。
模型的合理性
如上所述，我们将上一节考察过的恶魔的工作翻译为了神经网络的
深度学习的数学.indd 33
2019/7/2 10:46:3934
第1章神经网络的思想
权重与偏置,但不要认为这样就万事大吉了。即使将恶魔的活动转换为
了神经网络，也无法保证可以求出能够实现恶魔的工作的权重和偏置
而如果能够实际建立基于这个想法的神经网络,并能够充分地解释所给
出的数据，就能够验证以上话题的合理性。这需要数学计算，必须将语
言描述转换为数学式。为此,我们会在第2章进行一些准备工作,并从
第3章开始进行实际的计算。
恶魔的人数
住在输出层的输出恶魔的人数是2人。为了判断图像中的数字是0
还是1，2人是合适的
住在隐藏层的隐藏恶魔的人数是3人。为什么是3人呢？如本节开
头所讲的那样，这是由于存在某种预估，如下图所示。
0的特征
1 的特征
0的特征
模式A
模式B
模式C
存在这样的预估：图
像中的手写数字是0
还是1，可以通过是
否包含模式A、B、C
数字0
数字1
来判断。
根据该图可以预估数字0包含了图中的模式A和C,数字1包含了
模式B。因此，只要准备好对上图的模式A、B、C做出反应的神经单
元，就能够判断图像中的数字是0还是1。这3个神经单元正是隐藏恶魔
A、B、C的本来面目
上一节中为隐藏恶魔 A、B、C设定分别喜欢模式A、B、C的特征，
也是出于这个原因。
深度学习的数学.indd 34
2019/7/2 10:46:391-6将恶魔的工作翻译为神经网络的语言

35
以上是在隐藏层部署3个神经单元的理由。通过让这个神经网络实
际读取图像数据并得出令人信服的结论，可以确认这个预估的正确性。
关于具体的确认方法，我们将在第3章考察。
神经网络与生物的类比
让我们从生物的观点来看神经网络。
请想象一下生物看东西时的情形。可以认为,输入层神经单元相当
于视细胞,隐藏层神经单元相当于视神经细胞,输出层神经单元相当于
负责判断的大脑神经细胞群
不过，相当于隐藏层神经单元的视神经细胞实际上存在吗？例如，
第一个神经单元对前面图中的模式A做出反应，像这样的视神经细胞存
在吗?
实际上，1958年美国生理学家大卫·休伯尔（David Hunter Hubel)
和托斯坦·威泽尔（Torsten Wiesel)发现存在这种细胞，这种细胞被命
名为特征提取细胞。对某种模式做出强烈反应的视神经细胞有助于动物
的模式识别。想到本节考察的“恶魔”在大脑中实际存在，这真是非常
有意思的事情。
..:.(备注人工智能研究中的几次热潮
人工智能的研究大约是从20 世纪50年代开始的，其发展史与计算机的
发展史有所重合，可以划分为以下3次热潮。
世代
年代
关键
主要应用领域
第1代
20世纪50 ～60年代
逻辑为主
智力游戏等
第2代
20世纪80年代
知识为主
机器人、机器翻译
第3代
2010年至今
数据为主
模式识别、语音识别
深度学习的数学indd 35
2019/7/2 10:46:3936
第1章神经网络的思想
1-7
网络自学习的神经网络
在前面的1-5节和1-6节中，我们利用恶魔这个角色，考察了识别
输人图像的机制。具体来说,就是根据恶魔组织中的关系来判断。不过，
之前的讲解中事先假定了权重的大小，也就是假定了各层恶魔之间的人
际关系。那么，这个权重的大小（恶魔的关系)是如何确定的呢?神经
网络中比较重要的一点就是利用网络自学习算法来确定权重大小。
从数学角度看神经网络的学习
神经网络的参数确定方法分为有监督学习和无监督学习。本书只介
绍有监督学习。有监督学习是指，为了确定神经网络的权重和偏置，事
先给予数据，这些数据称为学习数据。根据给定的学习数据确定权重和
偏置，称为学习
注:学习数据也称为训练数据。
那么,神经网络是怎样学习的呢？思路极其简单:计算神经网络得出
的预测值与正解的误差,确定使得误差总和达到最小的权重和偏置。这
在数学上称为模型的最优化(下图)。
关于预测值与正解的误差总和，有各种各样的定义。本书采用的
是最古典的定义：针对全部学习数据，计算预测值与正解的误差的平
方（称为平方误差)，然后再相加。这个误差的总和称为代价函数（cost
function)，用符号C·表示（T是Total的首字母）。
利用平方误差确定参数的方法在数学上称为最小二乘法,它在统计
学中是回归分析的常规手段。
深度学习的数学.indd 36
2019/7/2 10:46:391-7网络自学习的神经网络
37
学习数据

神经网络
平方误差
正解
学习实例1
=(正解1-预测值1）
0
学习实例2
C2=(正解2-预测值2)°
O
OhO
E
学习实例k
-（正解k-预测值k）
算出预测值
误差总和（代价函数Cr)=C.+C.+
-C+·
最优化是指确定使得误差总和最小的参数的方法。
我们将在2-12 节以回归分析为例来具体考察什么是最小二乘法
另外，本书以手写数字的模式识别为例进行说明。因此，学习数据
是图像数据，学习实例是图像实例
需要注意的是，神经网络的权重是允许出现负数的，但在用生物学
进行类比时不会出现负数，也难以将负数作为神经传递物质的量。可以
看出，神经网络从生物那里得到启发，又飞跃到了与生物世界不同的另
-个世界。
NLE
.备注奇点
奇点（singularity）被用来表示人工智能超过人类智能的时间点。据预测
是2045年，也有不少人预测这个时间点会更早到来。
深度学习的数学.indd 37
2019/7/2 10:46:39图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 38
2019/7/2 10:46:39第
2
章
神经网络的数学基础
本章我们将梳理一下神经网络所需的数学基础知识，其中
大多数内容没有超出高中所学范围，因此读起来不会吃力。
深度学习的数学.indd 39
2019/7/2 10:46:3940
第2章神经网络的数学基础
2-1
神经网络所需的函数
本节我们来看一下神经网络世界中频繁出现的函数。虽然它们都是
基本的函数，但是对于神经网络是不可缺少的。
一次函数
在数学函数中最基本、最重要的就是一次函数。它在神经网络的世
界里也同样重要。这个函数可以用下式表示。
y=ax+b (a、b为常数，a≠0
(1)
a称为斜率，b称为截距。
当两个变量x、y满足式(1)的关系时，称变量，和变量x是一次函数

关系
-次函数的图像如下图的直线所示。
x+I
-次函数y=ax+b的图
像为直线。
例1一次丽数y=2x+1的图像如右图所示
截距为1，斜率为2。
深度学习的数学.indd 40
2019/7/2 10:46:402-1 神经网络所需的函数
41
以上是一个自变量的情形。这个一次函数关系也同样适用于多个自
变量的情形。例如，有两个变量x、x，当它们满足下式的关系时，称，
和x、心是一次函数关系。
y=ax,+bx,+c (a、b、 c为常数，a≠0, b≠0)
我们将会在后面讲到，在神经网络中，神经单元的加权输人可以表
示为一次函数关系。例如，神经单元有三个来自下层的输入，其加权输
入z的式子如下所示（1-3节）。
z=W,X,+w,X, + w,x5,+ b
如果把作为参数的权重wi、w、w,与偏置b看作常数，那么加权输
人z和x、2、心是一次函数关系。另外，在神经单元的输入r、X2、
作为数据值确定了的情况下,加权输入z和权重w、w2、w3以及偏置b
是一次丽数关系。用误差反向传播法推导计算式时，这些一次函数关系
使得计算可以简单地进行
问题1作出一次函数y=-2x-1的图像
解如石图所示，截距是-1，斜率是-2
M.!。金进)自变量
有两个变量x和y，如果对每个x都有唯一确定的，与它对应，则称y是
x的函数，用y=f(x）表示。此时，称x为自变量，y为因变量。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 41
2019/7/2 10:46:4042
第2章神经网络的数学基础
二次函数
在数学丽数中，二次函数与一次函数同样重要。本书中的代价函数
使用了二次函数。二次函数由下式表示。
y=ax-tbr+c (a、b、 c为常数，a≠)
(2)
二次丽数的图像是把物体抛出去时物体所经
过的轨迹，也就是抛物线（右图)。这个图像中
重要的一点是，α为正数时图像向下凸，从而存
最小值
在最小值。这个性质是后面讲到的最小二乘法的
基础。
例2二次丽数，=(x-1·+2的图像如右图所示
从图像中可以看到，当x=1时，函数取得最小
值2.
以上考察了一个自变量的情形。这里考察的
性质在推广到多个自变量的情形时也是不变的。
例如，有两个自变量x、x,时，称下面的函数为
关于x、,的二次丽数。
例3y=af t bxx, + cx, t px, t gx, + r
3)
这里,a、b、G、p、I、r为常数，a≠0,c≠0
对于有两个以上的自变量的情形，就难以在
纸面上画出图像了。例如，只能像右图那样画出
式(3)的图像。
实际的神经网络需要处理更多变量的二次函数。不过，记住这里
考察的二次函数的图像后，在理解多变量的情形时应该不难。
注：式（3)所示的图像并不仅限于上图所示的抛物面
深度学习的数学.indd 42
2019/7/2 10:46:42-1
神经网络所需的函数
43
问题2试作出二次函数y=2_的图像。
解图像如右图所示。
单位阶跃函数
神经网络的原型模型是用单位阶跃函数作为激活函数的（1-2节）
它的图像如下所示。
单位阶跃函数的图像。在应用数
学的世界里，这个函数活跃于线
J
性响应理论之中。
我们用式子来表示单位阶跃函数
O(x<0)
u(x)
1
(x>0)
(4)
从这个式子我们可以知道，单位阶跃函数在原点处不连续，也就是在原
点不可导。由于这个不可导的性质，单位阶跃函数不能成为主要的激活函数
问题3在单位阶跃函数u(x)中，求下面的值
①u(-1)
② u(1)
Bu(0)
解答案依次为0、1、1
深度学习的数学.indd 43
2019/7/2 10:46:4144
第2章神经网络的数学基础
指数函数与Sigmoid函数
具有以下形状的函数称为指数函数
y=a* (a为正的常数，α≠1)
常数α称为指数数的底数。纳皮尔数e是一个特别重要的底数，其近似值如下。
e=2.71828...
这个指数函数包含在以下的 Sigmoid 函数(x)的分母中。Sigmoid 函
数是神经网络中具有代表性的激活函数（1-3节）。
\
/
O(x)=
1+e
(5)
1+ exp(-x)
注：exp是 exponential function（指数函数）的简略记法，exp(x)表示指数函数e。
1^V=o(z)
这个丽数的图像如右图所示。可
以看出，这个函数是光滑的，也就是
处处可导。函数的取值在0和1之间
因此函数值可以用概率来解释。
0
Sigmoid函数的图像。
问题4在Sigmoid函数(x)中，求以下函数值的近似值。
①o(-1)
②0(0)
®o(1)
解取e=2.7作为近似值，答案依次为0.27、0.5、0.73
正态分布的概率密度函数
用计算机实际确定神经网络时，必须设定权重和偏置的初始值。求
初始值时，正态分布（normal distribution）是一个有用的工具。使用服从
这个分布的随机数，容易取得好的结果。
正态分布是服从以下概率密度函数(x)的概率分布。
深度学习的数学.indd 44
2019/7/2 10:46:412-1神经网络所需的函数
45
(x-u)
f(x)
202
(6)
√2no
其中常数u称为期望值（平均值)，。称为标准差。它的图像如下图所示
由于形状像教堂的钟，所以称为钟形曲线。
^f(x)
期望值为u，标准差为。
的正态分布。另外，这个
α 与 Sigmoid 函数名，的
u-2o u-o
pu + o u + 20
含义不同。
问题5试作出期望值u为0、标准差·为1的正态分布的概率密度函
数的图像。
解如下图所示，这个正态分布称为标准正态分布
0.4
0.2
0
u=0，G=1的正态分布概
5
0
率密度函数的图像。
按照正态分布产生的随机数称为正态分布随机数。在神经网络的计
算中，经常用到正态分布随机数作为初始值。
.. 备注Excel中的正态分布随机数
在 Excel中，可以像下面这样产生正态分布随机数。
= NORM.INV(RAND(), u, )（u、o是期望值和标准差）
深度学习的数学.indd 45
2019/7/2 10:46:4146
第2章神经网络的数学基础
有助于理解神经网络的数列
2-2
和递推关系式
熟悉了数列和递推关系式之后，就很容易理解误差反向传播法(第4
章、第5章）的内容了。因此，下面我们通过简单的例子来回顾一下。
熟悉递推关系式,对于用计算机进行实际计算有很大的帮助。这是
因为计算机不擅长导数计算,但擅长处理递推关系式。
数列的含义
数列是数的序列。以下是被称为偶数列的数列
例12, 4, 6, 8, 10. ..
t
数列中的每一个数称为项。排在第一位的项称为首项，排在第二位
的项称为第2项,排在第3位的项称为第3项,以此类推,排在第n位
的项称为第n项。在上面的例1中,首项为2,第2项为4.
在神经网络的世界中出现的数列是有限项的数列。这样的数列称为
有穷数列。在有穷数列中，数列的最后一项称为末项。
例2考察以下有穷数列的例子：
1, 3, 5, 7,9
这个数列的首项为1,末项为9,项数为5元
数列的通项公式
数列中排在第n位的数通常用a表示，这里a是数列的名字（数列
名α是随意取的，通常用一个拉丁字母或希腊字母来表示)。当棋要表示
深度学习的数学.indd 46
2019/7/2 10:46:412-2有助于理解神经网络的数列和递推关系式
47
整个数列时，我们使用集合的符号{a}来表示。
将数列的第n项用一个关于n的式子表示出来，这个式子就称为该
数列的通项公式。例如，例1的数列的第，项能够用如下关于n的式子
写出来，这就是它的通项公式。
a, = 2n
问题1试求以下数列{b,}的通项公式。
1,3,5,7, 9, 11,.
解通项公式b=2n-1.
在神经网络中，神经单元的加权输入及其输出可以看成数列（1-3
节)，因为可以像“第几层的第几个神经单元的数值是多少”这样按顺序
来确定值。因此，我们用类似数列的符号来表示值，如下例所示。
例3 α,表示第1层的第j个神经单元的输出值。
数列与递推关系式
通项公式就是表示数列的项的式子。除此之外数列还存在另一种重要的
表示法，就是用相邻项的关系式来表示，这种表示法称为数列的递归定义。
一般地，如果已知首项a┐以及相邻两项an、an+1的关系式，就可以
确定这个数列，这个关系式称为递推关系式。
例4 已知首项a，=1以及关系式an+1=a,+2，可以确定以下数列，这个
关系式就是数列的递推关系式。
a,=1,$4,=41=a,+2=1+2=3, 4,=41=4,+2=3+2=5
4,=4,=4,+2=5+2=7...
深度学习的数学.indd 47
2019/7/2 10:46:4248
第2章
神经网络的数学基础
+2
十
合出
a2
24
递推关系式可以形象地表示为多米诺骨牌。数列由首项以及前后项的关系（也就是递推关
系式）确定。此外，图中的数列表示问题1的数列。
例5 已知首项c，= 3以及递推关系式c,+1 = 2c，求这个数列{c}的前4项
,=3, C,=G|=2c,=2.3=6, G,=C+=2c,=2.6=12
Cq=G1=2+12=24....
这样，这个数列就确定了。
给出
3×2
2×2
\
6×2
6
=9
一寸
C
C2
C3
C4
Cn
Cn+
数列由首项以及递推关系式。
=2c确定。
问题2请递归地定义以下数列{an}.
2, 4, 6, 8, 10，（这是例1的数列）
解an=2,4n+1=a+2
联立递推关系式
我们来看看下面的例子。
例6求由以下两个递推关系式定义的数列的前3项，其中a =b=1。
深度学习的数学.indd 48
2019/7/2 10:46:422-2有助于理解神经网络的数列和递推关系式
49
(a,.,=a,, +2b,+2
b,.1=2a,+3b,+1
可以像下面这样依次计算数列的值a,、b,。
[a,=a,+26,+2=1+2.1+2=5
b,=2a,+36,+1=2.1+3.1+1=6
4,=4,+2b,+2=5+2.6+2=19
b,=2a,+3b,+1=2.5+3.6+1=29
像这样，将多个数列的递推关系式联合起来组成一组,称为联立递
推关系式。在神经网络的世界中，所有神经单元的输人和输出在数学上
都可以认为是用联立递推式联系起来的。例如，我们来看看1-4节的
例题中考察过的神经网络的一部分，如下图所示。
隐藏层（层2）
输出层（层3
e3
我们在第1章考察过的神
经网络示例的一部分。此
外，有关变量名的内容将
在3-1节详述。
在箭头前端标记的是权重，神经单元的圆圈中标记的是神经单元的
输出变量。于是，如果以a(z)为激活函数，b、以为第3层各个神经单
元的偏置，根据1-3节，以下关系式成立。
a`=a(w,a?+w,a3+w,a3+b)
a` =a(w;,ai+ w;,az + w2,a, +b)
根据这些关系式，第3层的输出含和·由第2层的输出a、d
以决定。也就是说，第2层的输出与第3层的输出由联立递推关系式联
深度学习的数学.indd 49
2019/7/2 10:46:4250
第2章神经网络的数学基础
系起来。第4章和第5章将要考察的误差反向传播法就是将这种递推关
系式的观点应用在神经网络中。
问题3对于由以下联立递推关系式定义的数列a,、b，求第3项a3、
bs，其中aa=2,b=1o
a,+1 =3a,, +b
b,+1=a,+3b,
解可以像下面这样依次进行计算。
a,=3a,+b=3.2+1=7
b,=a,+3b=2+3.1+1=5
a,=3a,+b,=3.7+5=26
b,=a,+3b,=7+3.5=22
Mer
：备注计算机擅长递推关系式
计算机擅长关系式的计算。
例如，我们来看一下阶乘的计算。自然数n的阶乘是从1到n的整数的
乘积，用符号 n！表示。
n!=1x2x3x…xn
在多数情况下，人们是根据上面的式子来计算n!的，而计算机则通常用
以下递推关系式来计算。
a1=1,4n+1=(n+1)a,
后述的误差反向传播法就是通过计算机所擅长的这一计算方法来进行神
经网络的计算的。
深度学习的数学.indd 50
2019/7/2 10:46:432-3神经网络中经常用到的》符号
51
2-3
神经网络中经常用到的习符号
习是一个需要下功大来熟悉的符号。如果不理解工，在阅读神经网络
相关的文献时就比较麻烦。这是因为将加权输人用符号来表示会简洁得
多。下面我们就来复习一下这个习符号。
注：本书不使用习符号来进行讲解，因为习符号使人难以看到数学式的本质。因此，本
书中的写法会变得兄长，不便之处还请读者见谅。
三符号的含义
习符号可以简洁地表示数列的总和。除了表示总和以外，并没有别的
含义，然而这样过于简洁的表示经常使神经网络的初学者感到苦恼。
注：习为希腊字母，读作 Sigma，对应拉丁字母S，即 Sum（总和）的首字母。

对于数列{a}，习符号的定义式如下所示。
>a,=a,+a,+a,++a,i+a,
(I)
:=1
以上用习符号表示的和之中，字母k并不具有实质的含义。实际上，
在上式的右边没有出现字母k，在这里仅用于表明关于它求和。因此，这
个字母并非必须是k，在数学上通常用i、k、、m、。
例12,=a,+a4,+a,+a,+4;
n
例2
k3=1*+2+3+4-+53+63+72
k=1
例3
]2'=2'+2'+2'++2"
深度学习的数学.indd 51
2019/7/2 10:46:4352
第2章神经网络的数学基础
●习符号的性质
习符号具有线性性质。这是与微积分共通的性质，可以在式子变形中使用
Sa+b,)=2a+o,Zca,=
（c为常数)
(I）
注：用语言来表述的话，就是“和的习为习的和”“常数倍的习为习的常数倍”。这与导数公
式“和的导数为导数的和”“常数倍的导数为导数的常数倍”是一致的（2-6节）。
证明根据>符号的定义，有
E(a; +b,)=(a,+b,)+(a,+b)+:+(a,+b,)
=(a,+4 +:+a,)+(b,+b,+:.+b,)
-艺a.+乐
ca,=ca, +ca, +:++ca,,=c(a, +a, t:w+a,)=
下面我们通过例子来验证式(I)。
例4)(2k+1)=(2.1+1)+(2-2+1)+.+(2n+1)
=2(1+2+3+.+n)+(1+1+1+...+1)=
问题证明下式成立。
2k*-3&+2)=2k3-32*+22
f
t=1
k=1
角解
在2
-3k+2)=(1?-3.1+2)+(2^-3.2+2)+.+(n?-3n+2)
=(1'+2'+3*++n')-3(1+2+3+.+n)
+(2+2+2+.+2)
7k'-35k+2
深度学习的数学.indd 52
2019/7/2 10:46:432-4有助于理解神经网络的向量基础
53
2-4
有助于理解神经网络的向量基础
向量的定义为具有大小和方向的量。这里我们主要关注神经网络中
用到的内容，弄清向量的性质。
有向线段与向量
有两个点A、B，我们考虑从A指向B的线段，这条具有方向的线段
AB 叫作有向线段。我们称A为起点，B为终点。
B（终点）
A(起点）
有向线段
^
有向线段 AB具有以下属性:起点A的位置、指向B的方向，以及
AB的长度，也就是大小。在这3个属性中，把方向与大小抽象出来，这
样的量叫作向量，通常用箭头表示，总结如下：
向量是具有方向与大小的量，用箭头表示。
有向线段AB所代表的向量用AB表示，也可以用带箭头的单个字母
a或者不带箭头的黑斜体字母α表示。本书主要使用最后一种表示方法
ABA



表示向量的几种方法。
深度学习的数学.indd 53
2019/7/2 10:46:4454
第2章神经网络的数学基础
向量的坐标表示
把向量的箭头放在坐标平面上，就可以用坐标的形式表示向量。把
箭头的起点放在原点，用箭头终点的坐标表示向量，这叫作向量的坐标
表示。用坐标表示的向量α如下所示（平面的情况）
a =(a, a,)
(1)
nJ
=(a1,42)
向量的坐标表示，即把起点放
在原点，通过终点的坐标来表
示。这应该不难理解，在应用
时想必也不会发生问题
例1 a=(3,2）表示的向量
例2 b=(-2, -1）表示的向量
例3在三维空间的情况下也是同样的。
例如，a=(1,2,2）表示右图所示的向量。
1,2, 2
深度学习的数学.indd 54
2019/7/2 10:46:442-4有助于理解神经网络的向量基础
55
向量的大小
从直观上来讲，表示向量的箭头的长度称为这个
向量的大小。向量α的大小用a丨表示。
注：符号||是由数的绝对值符号一般化而来的。实际上，数可
以看成一维向量。
例4根据右图，如下求得a=(3,4)的大小|alo
|a|=/3+4'=5
例5在三维空间的情况下也是同样的
例如，如下求得右图所示的向量a=
邮
(1, 2, 2）的大小|a。
电
|a|=1?+22+2'=3
注：例4、例5都使用了勾股定理。
问题1求右图所示的向量α、b的大小
解
a|=\/2*+1'=/5,
b|=/3+(-1)?=/10
向量的内积
我们在考虑具有方向的向量的乘积时，包含了“方向与方向的乘积”
这样不明确的概念。因此，我们需要一个新的定义-
内积。两个向量
深度学习的数学.indd 55
2019/7/2 10:46:4456
第2章神经网络的数学基础
a、b的内积α·b的定义如下所示。
a·b=|a||b|cos0
(2)
(日为a、b的夹角)
注:当α、b有一个为0或两者都为0时，内积定义为0元
例6 考虑边长为1的正方形ABCD, AB=a, AD=b,
AC=c，于是有
|a|=|b|=1,|c|=√2
此外,a与a的夹角为0°,a与b的夹角为
90°，a与c的夹角为45°，因此有
a+a=|a|a|cos0*=|a?=1'=1
a+b=|a|b|cos90*=1-1·0=0
a+c=|a||c|cos45*=1:2.
√2
问题2在上述例6中,求b·C.
解 b.c=|b||c|cos45*=1./2
2
在三维空间的情况下也是同样的
例7在边长为3的立方体ABCD- EFGH中，有
AD.AD =|AD|4D|cos0*=3.3.1=9
AD.AF=|AD|AF cos90*=3.3/2.0=0
AF.AH=|AF |AH|cos60*=3/z.3v/2.
深度学习的数学.indd 56
2019/7/2 10:46:452-4有助于理解神经网络的向量基础
57
问题3有一个边长为2的正四面体OABC,
求内积OA·OB
解OA与OB的夹角为60°，因此有
OA.OB =|OAOB|cos60*=2.2.
柯西一施瓦茨不等式
根据内积的定义式(2)，我们可以推导出下式，该式在应用上十分
重要。
柯西一施瓦茨不等式：-lal|b|sa·bslal|b
(3)
证明根据余弦函数的性质，对任意的日,有-1< cos0≤1，两边同时乘
以|allb|，有
-absa|b |coso <|a| b
利用定义式(2)，我们可以得到式（3)。
让我们通过图形来考察式(3)。两个向量α、b的大小固定时，有下图
(1)、（2)、（3）的3种关系。
①0=180°
20<0<180°
30=0
(cos0 =-1)
(-1 < cos0  1)
(cos0 = 1)
根据柯西-施瓦茨不等式(3),可以得出以下事实。
①当两个向量方向相反时，内积取得最小值
②当两个向量不平行时，内积取平行时的中间值。
3当两个向量方向相同时，内积取得最大值
深度学习的数学.indd 57
2019/7/2 10:46:4558
第2章神经网络的数学基础
性质①就是后述的梯度下降法（2-10节以及第4章、第5章）的基
本原理。
另外，可以认为内积表示两个向量在多大程度上指向相同方向。如
果将方向相似判定为“相似”，则两个向量相似时内积变大。后面我们考
察卷积神经网络时，这个观点就变得十分重要（附录C）。
非常不相似
有点不相似
有点相似
非常相似
通过内积可以知道两个向量的相对的相似度。
内积的坐标表示
下面我们使用坐标表示的方式来表示定义式(2)。在平面的情况下，
下式成立。
^
当a=(a, a,), b=(b, b,)时,
b= (b1,b2)
人
a .b=ab +a,b
(4)
=（a1,a2)
例8当a=(2,3)， b =(5,1)时
a.-b=2.5+3.1=13,4.4=2.2+3.3=13,6.6=5.5+1.1=26
在三维空间的情况下，内积的坐标表示如下所示，只是在平面情况
下的式(4）中添加了z分量。
当a=(a. as, az), b=(b1, bs, b)时
B= (bi, b2, b)
a·b=a,bh +a,b,+azb,
(5)
A= (a1,a
、a3


注：这里我们省略了式(4)、(5)的证明。此外，也有很多文献使用式(4)、（5)作为内积
的定义。
深度学习的数学.indd 58
2019/7/2 10:46:462-4有助于理解神经网络的向量基础
59
例9当a=(2,3,2), b=(5,1,-1)时,
a+6=2.5+3.1+2.(-1)=11,4.a=2.2+3.3+2.2=17
问题4求以下两个向量α、b的内积。
Da=(2/3,2),b=(1,/3)
2$a=(-3,2,1),$b=(1,$-3,2)
解根据式(4)、（5),可得
0a.b=2/3.1+2.3=4/3
2a.b=-3.1+2.(-3)+1.2=-7
向量的一般化
到目前为止，我们考察了平面（也就是二维空间）以及三维空间中
的向量。向量的方便之处在于，二维以及三维空间中的性质可以照搬到
任意维空间中。神经网络虽然要处理数万维的空间，但是二维以及三维
空间的向量性质可以直接利用。出于该原因，向量被充分应用在后述的
梯度下降法中（2-10节以及第4章、第5章。
为了为后面做好准备，我们将目前考察过的二维以及三维空间中的
向量公式推广到任意的 维空间。
·向量的坐标表示：a = (a, r ,a,)
·内积的坐标表示：对于两个向量a =(a, a- .a,), b (b b  .,bn)
其内积α·b如下式所示。
a +b=aqb,+a2b,++a,b,
·柯西-施瓦茨不等式:-|a|blsa·bsallb
例10神经单元有多个输入x,x2,…,,时，将它们整理为如下的加
权输人。
Z= W]X1 + W2X2 +
t WnXnt b
深度学习的数学.indd 59
2019/7/2 10:46:460
第2章神经网络的数学基础
其中，w w. …", w，为权重，b为偏置。
输入
权重
偏置
使用w=(w, w ,w,),  (1 ,   , n)
输入
这两个向量，我们可以将加权输人表示为内积形
式，如下所示。
z=w·x+b
从例10可以看出，在神经网络的世界中，向量的观点是十分有益的
Me.!
:备注张量
张量（tensor）是向量概念的推广。谷歌提供的人工智能学习系统
TensorFlow的命名中就用到了这个数学术语。
"tensor”来源于“tension"（物理学中的“张力”）。向固体施加张力时，
会在固体的截面产生力的作用，这个力称为应力。这个力在不同的截面上大
小和方向各不相同。
应力
法向量
、
法向量是垂直于面的向量，根据
这个向量的方向（也就是法向)，
应力的方向和大小各不相同。
因此，当面的法向为x、、z轴时，作用在面上的力依次用向量表示为
tI
T12
T12
rata，t
，
E.1
T,
可以将它们合并为以下的量。
TnT12 7is
21 722 T23
T31T3273
我们称这个量为应力张量。
张量是应力张量在数学上的抽象。我们不清楚谷歌将人工智能学习系统
命名为TensorFlow的原委，不过在神经网络的世界里，经常用到附带多个
下标的变量，这与张量的计算相似，可能也是出于这个原因，TensorFlow 才
这样命名的吧。
深度学习的数学.indd 60
2019/7/2 10:46:462-5有助于理解神经网络的矩阵基础
61
2-5
有助于理解神经网络的矩阵基础
神经网络的文献中会用到矩阵(matrix)。矩阵可以使数学式的表示
变简洁。下面我们来梳理一下阅读文献时所需要的矩阵知识。
注：本书从第3章以后的讲解不需要矩阵的知识作为前提。
●什么是矩阵
矩阵是数的阵列，如下所示。
314
15
265
横排称为行,竖排称为列。在上例中，矩阵由3行3列构成，称为3
行3列的矩阵。
特别地，如上例所示，行数与列数相同的矩阵称为方阵。此外，如
下所示的矩阵X、Y分别称为列向量、行向量，也可以简单地称为向量。
(3
X=
Y=(271)
4
我们将矩阵A推广到更一般的情形，如下所示。
a1
412
4n
a21
a2
4,
·
ml
4m2
Q mn
这是m行n列的矩阵。位于第i行第j列的值（称为元素）用α,表示。
有一种有名的矩阵称为单位矩阵,它是对角线上的元素a为1、其
深度学习的数学.indd 61
2019/7/2 10:46:4762
第2章神经网络的数学基础
他元素为0的方阵，通常用E表示。例如，2行2列、3行3列的单位矩
阵E(称为2阶单位矩阵、3阶单位矩阵)分别如下表示。
(100
E=
101
E=0 1 0
001
注:E为德语中表示1的单词 Ein的首字母。
●矩阵相等
两个矩阵A、B相等的含义是它们对应的元素相等，记为A=B。
例1当4=
L
/
B=
时，如果A=B,则x=2, y=7, u=l,v=8o
●矩阵的和、差、常数倍
两个矩阵A、B的和A+B、差A-B定义为相同位置的元素的和、
2
差所产生的矩阵。此外，矩阵的常数倍定义为各个元素的常数倍所产生
的矩阵。我们通过以下例子来理解。
例2当4=
(27
B=
1 8
13
(28时
A+B=
(2+2 7+8(4 15
1+1 8+32 11
4-B=
(2-2 7-80 -1
1-1 8-30 5
34=3
2 73x2 3x76
21
1$83x1$3x8\3 24
●矩阵的乘积
矩阵的乘积在神经网络的应用中特别重要。对于两个矩阵A、B,将
A的第i行看作行向量，B的第；列看作列向量，将它们的内积作为第i
行第；列元素，由此而产生的矩阵就是矩阵A、B的乘积AB
深度学习的数学.indd 62
2019/7/2 10:46:472-5有助于理解神经网络的矩阵基础
63
第j列
第j列
将A的第i行的行向量与B的

第j列的列向量的内积作为矩
第
第i行
阵AB的第，行第；列的元素。
AB
两个矩阵的乘积。
请通过下面的例子弄清矩阵乘积的含义
27
28
例3当=
B:
时
18
13
12728
(2.2+7.1 2.8+7.
37
4B=
(1$8/1$3=1:2+8.1 1.8+8.3
10 32
(2$8(2$7(2.2+8.1 2.7+8.8
12
78
BA=
1 31 81.2+3.1 1.7+3.8
31
从这个例子中可以看出，矩阵的乘法不满足交换律。也就是说，除
了例外情况，以下关系式成立。
^
AB= BA
而单位矩阵E 与任意矩阵 的乘积都满足以下交换律。
AE=EA=A
单位矩阵是具有与1相同性质的矩阵。
●Hadamard 乘积
对于相同形状的矩阵A、B，将相同位置的元素相乘，由此产生的矩
阵称为矩阵A、B的Hadamard 乘积，用AOB表示。
28
例4当4
,B=
时
2
13
4oB=
(2.2 7.8\ （4 56
1.1 8.3
124
深度学习的数学.indd 63
2019/7/2 10:46:4764
第2章神经网络的数学基础
●转置矩阵
将矩阵A的第i行第j列的元素与第j行第i列的元素交换，由此产
生的矩阵称为矩阵4的转置矩阵（transposed matrix )，用'A、At等表示。
下面我们使用4。

例5当4=
2
/
2
例6当B=
时，
,B=(1 2)
注：阅读神经网络的文献时需要注意，转置矩阵有各种各样的表示方法。
问题4=
4
L/
2 1).B=|
8 2
8
时，进行以下计算
①A+BDAB BAOB
解①A+B
(1+2 4+7 1+1)
(3 11 2
4+8 2+2 1+8
12
4
9
(14
(2
1.2+4.8 1.7+4.2
1.1+4.8
24B=
7
4
4.2+2.8
4.7+2.2 4.1+2.8
\
82
8
1.2+1.8
1.7+1.2
1.1+1.8
34 15 33
24 32 20
10 9
9
1.2 4.7
1.1
2
28
BAoB=
4.8 2.2 1.8
32
4
8
深度学习的数学.indd 64
2019/7/2 10:46:482-6神经网络的导数基础
65
2-6
神经网络的导数基础
之前我们提到过，神经网络会自己进行学习，这在数学上的含义是
指，对权重和偏置进行最优化（2-12 节)，使得输出符合学习数据。而对
于最优化而言，求导是不可缺少的一种方法。
注：本章所考察的函数都是充分光滑的函数、
导数的定义
陈数=f(x)的导函数"(x)的定义如下所示。
f"(x)= kin
f(x+Ax)-f(x)
(1)
Ax
注：希腊字母A读作delta，对应拉丁字母D。此外，带有'(prime）符号的函数或变
量表示导函数。
"lim(x的式子)”是指当Ax无限接近0时“（Ax的式子)”接近的值
例1当/(x)=3x时，
3(x+ Ax)-3x
3Ax
f"(x)= lim
im
= lim 3=3
Ax
ANS0Ax
例2当/x)=x2时
(x+ Ar)' -x?
2xAx+(Ax)
f"(x)= lim
lim
lim(2x + Ar) = 2.x
A
A
已知函数f(x)，求导函数f"x)，称为对函数f(x)求导。当式（1）的值
存在时，称函数可导
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学indd 65
2019/7/2 10:46:4866
第2章神经网络的数学基础
导丽数的含义如下图所示。作出函数（x)的图像，f"（x)表示图像切
线的斜率。因此，具有光滑图像的函数是可导的
y=f(x)
PQ的斜率
y个
f(x+Ax)-f(x)
Ax
Q
f(x+Ax)
1的斜率「"（x)
f(x)
八3
0
x+Ax
导函数的含义。"x）表示图像切线的斜率。实际上，如果Q无限接近P（也就是Ax-→0),
那么直线PQ无限接近切线lo
神经网络中用到的函数的导数公式

我们很少使用定义式(1)来求导函数,而是使用导数公式。下面我们
就来看一下在神经网络的计算中使用的函数的导数公式（x为变量、为
常数）。
(e)'=0, (x)'=1, (x?)'=2x, (e*)'=*, (e"*)'=-e"*
(2)
注:这里省略了证明。e为纳皮尔数(2-1节)。
导数符号
在式(1)中，函数y=f(x)的导函数用"(x)表示，但也存在不同的表
示方法，例如可以用如下的分数形式来表示
/"(tr)= dJ
dx
深度学习的数学.indd 66
2019/7/2 10:46:482-6神经网络的导数基础
67
这个表示方法是十分方便的，这是因为复杂的函数可以像分数一样
计算导数。关于这一点，我们会在后文中说明。
圆图式(2）中的(e)=0，也可以记为“
=0（c为常数)
dx
例4式(2)中的（r'=1，也可以记为
=1
dx
导数的性质
利用下式，可导函数的世界得到了极大的扩展。
{f(x)+g(x)}'=f"(x)+g'(x),{f(x)}'=cf"(x) (c为常数）
(3)
注:组合起来也可以简单地表示为（fw)- g(x)}'= f'tr)-g'x)。
式(3)称为导数的线性性。用文字来表述可能更容易记忆，如下所示。
和的导数为导数的和，常数倍的导数为导数的常数倍。
导数的线性性是后述的误差反向传播法背后的主角。
例5当C=(2 -)（y为变量）时，
C'=(4-4y+y?)'=(4)'-4(y)'+(y?)'=0-4+2y=-4+2)
问题1对下面的函数f(x)求导。
Df(x)=2x'+3x+1
@f(x)=1+e`
解根据式(2)、式(3)，可得
f"(x)=(2x?)'+(3x)'+(1)'=2(x?)'+3(x)'+(1)'=4x+3
2 f'(x)=(1+e*)'=(1)'+(e"*)'=-e"*
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 67
2019/7/2 10:46:4868
第2章神经网络的数学基础
备注公式(e)'=-e
利用后述的链式法则(复合函数的求导公式)（2-8节)，我们可以简单
地推导出标题中的公式（即式(2) )，如下所示。
y=ec', u=-x, y= oy u
=e".(-1)=-e
du dr
分数函数的导数和 Sigmoid函数的导数
当函数是分数形式时，求导时可以使用下面的分数函数的求导公式。
1
f"'(x)
f(x)
{/f(x)}
(4)
注：这里省略了证明。函数(x)不取О值。
Sigmoid 函数(x)是神经网络中最有名的激活函数之一，其定义如下

所示（2-1节）。
O(x)=
1+e
在后述的梯度下降法中，需要对这个函数求导。求导时使用下式会
十分方便。
o'(x)=c(x)(1-(x))
(5
利用该式，即使不进行求导，也可以由。(x)的函数值得到 Sigmoid
丽数的导函数的值。
证明将1+e-*代人式(4)的（(x)，利用式(2)的指数丽数的导数公式
(e-)'=-e-x，可得
深度学习的数学.indd 68
2019/7/2 10:46:492-6神经网络的导数基础
69
(1+e"*)'
e
o'(x)=
(1+e-*)?=
（1+e*)?
上式可以像下面这样变形。
l+e*-1
1
o"(x)=
=0(x)-O(x)?
(1+e-*)2
1+e"*
(1+e-*)2
将o(x)提取出来，就得到了式(5)。
最小值的条件
由于导函数（x）表示切线斜率，我们可以得到以下原理，该原理在
后述的最优化（2-12节）中会用到。
当函数（)在.x=a处取得最小值时，f'a)=0
(6)
证明导函数/"(a)表示切线斜率，所以根据下图可以清楚地看出a)=00
/y=f(x)
f'(a)=
切线的斜率=0
当fx)在x=a处取最小值时，该
函数在该点的切线的斜率(即导函
大个
数的值）为。
应用时请记住以下事实。
f"(a)=0是函数(x)在.x=a处取得最小值的必要条件
注：已知命题p、g，由p可以推出q，则q称为，的必要条件。
从下面的函数y=f(x)的图像可以清楚地看出这一点。
深度学习的数学.indd 
2019/7/2 10:46:4970
第2章
神经网络的数学基础
/y=f(r)
切线的斜率=0
3切线的斜率=0
虽然f"a)=0（切线
极大值
斜率为0,即切线
极小值
最小值
与x轴平行),但在
切线的斜率=0
切线的斜率=0
①、2、③的情况
,X
下函数不取最小值。
在通过后述的梯度下降法求最小值时，这个性质有时会成为很大的
障碍
例题求以下函数f(x)的最小值
f(x) = 3xt -4x3- 12x3 + 32
解首先我们求出导丽数。
f"(x)=12x?-12x? - 24x = 12x(x + 1)(x - 2)
然后，我们可以做出以下表格（称为增减表)）
X
·
-1
·
0
.
2
f"()
0
0
0
f(x)
y
27
7
32
y
0

（极小）
（极大）
（最小）
注:增大、减小用入
表示，区间用…表示。
从表中可以看出，f(x)在点x=2处取得最小值0
如果已知增减表，就可以画出函数图像的大体形
状。这里我们使用例题中的增减表，画出函数
20
f(x)= 3x1 - 4x3- 12x2 + 32
的图像，如右图所示。
10
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 70
2019/7/2 10:46:492-6神经网络的导数基础
71
问题2求f(x)=2x2-4x+3的最小值。

解首先我们求出导丽数。
f'(x)= 4x- 4
然后，我们可以做出如下的增减表。从表中可以看出，f(x)在点
x=1处取得最小值1
s
…
f"(x)
0
fw)
（最小）
作为参考，
我们在增减表的右边画出了函数图像
深度学习的数学.indd 71
2019/7/2 10:46:5072
第2章神经网络的数学基础
2-7
神经网络的偏导数基础
神经网络的计算往往会涉及成千上万个变量，这是因为构成神经网
络的神经单元的权重和偏置都被作为变量处理。下面我们就来考察一下
神经网络的计算中所需的多变量函数的导数
注：本节所考察的函数是充分光滑的函数。
多变量函数
如前所述（2-1节)，在函数y=f(x)中，x称为自变量，，称为因变
量。上一节我们讲解求导方法时考虑了有一个自变量的函数的情形。本
节我们来考察有两个以上的自变量的函数。有两个以上的白变量的函数
人
称为多变量函数。
例1z=x?+y2
多变量丽数难以直观化。例如，即使是像例1那样简单的函数，其
图像也是非常复杂的，如下图所示。
2+y的图像。
描述神经网络的函数的变量有成千上万个，因此难以从直观上理解
这些丽数。不过，只要理解了单变量的情况，我们就可以将多变量的情
深度学习的数学.indd 72
2019/7/2 10:46:502-7神经网络的偏导数基础
73
况作为其扩展来理解,这样就不会那么困难了。
单变量函数用(x)表示，仿照单变量函数，多变量函数可以如下
表示。
例2 f(x,y）：有两个自变量x、y的函数。
例3f(x ,xn,):有n个自变量x,x …,.x，的函数。
偏导数
求导的方法也同样适用于多变量函数的情况。但是，由于有多个变
量，所以必须指明对哪一个变量进行求导。在这个意义上，关于某个特
定变量的导数就称为偏导数（partial derivative）。
例如，让我们来考虑有两个变量x、y的函数z=f(x,y)。只看变量x
将，看作常数来求导，以此求得的导数称为“关于x的偏导数”，用下面
的符号来表示。
十
0z0f(x;, y)
lim
f(x+Ax, y)- f(x,y)
Ox
Ox
ar→0
Ar
关于y的偏导数也是同样的。
0z @0f(x, )
lim
f(x, y+Ay)-f(x,y)
Oy
Oy
Ay
下面，我们通过例4和问题1、问题2来看一下神经网络中用到的
偏导数的代表性例子。
例4当z=wx+b时，
0z
0z
0z
=w
=x
=1
Ox
Ow
ab
阿题1当/八.3） 32-4时，求（/）
0f'(x, y)
ox
ay
解（r(r )
6x
af(x, y)
8y
Ox
Oy
深度学习的数学.indd 73
2019/7/2 10:46:5074
第2章神经网络的数学基础
问题2当z
= wx,+ wx,+b,时，求关于x、W、b.的偏导数。
解
Oz
0z
0z
w
X,,
=1.
ox
Ow
b
多变量函数的最小值条件
光滑的单变量函数 =/(x)在点x处取得最小值的必要条件是导丽数
在该点取值0(2-6节)，这个事实对于多变量函数同样适用。例如对于
有两个变量的丽数，可以如下表示。
两数- /1u:）得最小值的必要条件是创
=0
of
=0。
(1)
Ox
0y
上述(1)很容易扩展到一般的具有n个变量的情形。
此外，从下图可以清楚地看出上述(1)是成立的。因为从x方向以及
y方向来看，函数z=f(x.)取得最小值的点就像葡萄酒杯的底部。
= f(x, y)
f(x, y)
切线的斜率
切线的斜率
=0
=0
上述(1)的含义。
就像我们之前所确认的那样（2-6节)，上述(1)所示的条件是必要
条件。即使满足上述(1),也不能保证函数x.)在该点处取得最小值。
例5求丽数z=2+·取得最小值时x、，的值。
首先，我们来求关于x、y的偏导数
dz
z
Ox
2x
=2y
oy
深度学习的数学.indd 74
2019/7/2 10:46:52-7神经网络的偏导数基础
75
根据上述(1),函数取得最小值的必要条件是x=0,y=0。此时丽数
值z为0。由于z=+≥0,所以我们知道这个函数值0就是最小值
通过前面的例1的函数图像，我们也可以确认这个事实
Me1
t....《备注)拉格朗日乘数法
在实际的最小值问题中，有时会对变量附加约束条件，例如下面这个与
例5相似的问题。
例6当2+y2=1时，求x+y的最小值。
这种情况下我们使用拉格朗日乘数法。这个方法首先引入参数，创建
下面的函数L。
L=f(x,y)-Ag(x,y)=(x+y)-2(x^+y?-1)
然后利用之前的（1)。
2L =1-2x=0.
oL
Ox
Oy
-=1-2y=0
根据这些式子以及约束条件x2+2=1，可得x=y=.= ±1/√2。
因而，当x=y=-1/√2时，x+y取得最小值-√2。
在用于求性能良好的神经网络的正则化技术中，经常会使用该方法。
深度学习的数学.indd 75
2019/7/2 10:46:5176
第2章神经网络的数学基础
2-8
误差反向传播法必需的链式法则
下面我们来考察有助于复杂函数求导的链式法则。这个法则对于理
解后述的误差反向传播法很有必要
注：本节考察的函数是充分光滑的函数。
神经网络和复合函数
已知函数y=f(u)，当u表示为u=g(x)时，y作为x的函数可以表示
为形如y=f(g(x)的嵌套结构（u和x表示多变量）。这时，嵌套结构的函
数(g(r)称为()和g(x)的复合函数

例1函数z=(2-y)是函数w=2-y和丽数z=的复合函数。
:=(2-3)?
函数z=(2-y是函数u=2-3
和函数z=水的复合函数。此
外，这个函数示例在后面的代价
函数中会用到。
例2对于多个输入xy, x …,. n，将a(x)作为激活函数，求神经单元的输
出，的过程如下所示（1-3节）。
y = a(WX1 + W2X2 : + W.xn + b)
,w，为各输人对应的权重，b为神经单元的偏置。这个输出所数
是如下的 x. .x，的一次函数f和激活函数α的复合函数。
z= f(x, ` , x,,)=WX,+WX, +"+w,x,+b
y=a(z)
深度学习的数学.indd 76
2019/7/2 10:46:512-8误差反向传播法必需的链式法则
77
输入
加权输入
输出
 = f r x  .,x,)
X N·+x,
y=a(z)
=W,X,+WX, ++w,x;, +b
单变量函数的链式法则
已知单变量函数y=f(u)，当w表示为单变量函数w=g(x)时，复合
丽数(g(:）的导函数可以如下简单地求出来
dydy du
dx du dr
(1)
这个公式称为单变量丽数的复合函数求导公式，也称为链式法则。
本书使用“链式法则”这个名称。
dy
dx
(u
y
du
dy
单变量函数的链式法则。导数
dx
du
可以像分数一样进行计算
观察式(1)的右边，如果将dx、dy、du都看作一个单独的字母，那
么式(1)的左边可以看作将右边进行简单的约分的结果，这个看法总是成
立的。通过将导数用dx、dy等表示，我们可以这样记忆链式法则：“复合
丽数的导数可以像分数一样使用约分。”
注：这个约分的法则不适用于dx、dy的平方等情形。
例3当，为w的函数，w为v的函数，，为x的函数时
dy dy du dv
dxdu dv dx
dy
dx
三个函数的复合函数的
链式法则。与两个变量
X
v
>（
u
>9
dv
du

ow
%
的情形一样，可以像分
数一样进行计算。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 77
2019/7/2 10:46:5278
第2章神经网络的数学基础
问题对x的函数=
(w、b为常数）求导。
1+e"(wxr+b)
解我们设定以下函数
V
u= wx + b
1+e
由于第1个式子为 Sigmoid 函数，根据2-6节的式(5)，可得
dy
=y(1-y)
du
此外，由于山
= w，所以可得
dr
dydy du
=y(1-y)w
dxdu dx
1+e
l+e
多变量函数的链式法则
在多变量函数的情况下，链式法则的思想也同样适用。只要像处理
分数一样对导数的式子进行变形即可。然而事情并没有这么简单，因为
必须对相关的全部变量应用链式法则。
我们来考察两个变量的情形。
变量z为wv的函数，如果u、v分别为x、y的函数，则z为x、y
的函数，此时下式（多变量函数的链式法则）成立。
0z 0z Ou, 0z Ov
(2)
Ox Ou Ox Ov Ox
ou
0z
X
X
L
z
dh
x
y
Ov
变量z为w、v的函数，比、v分别为x、，的函数，z关于x求导时，先对体、v求导，然
后与z的相应导数相乘，最后将乘积加起来。
深度学习的数学.indd 78
2019/7/2 10:46:522-8误差反向传播法必需的链式法则
79
例4与上面式（2)一样，下式也成立。
0z 0z Ou,0z 0v
ay Ou Oy Ov Oy
例4中各变量的关系如下图所示。
z
x
(u
x
Bu
2
九
Ov
ay
例5当C=e+vr, u=axtby, v=px+y (a、 b、p、q为常数）时
aC_ aC du oCOv
Ox
=2u.a+2v.p=2a(ax+by)+2p(px+qy)
ou Ox Ov Ox
oC
oC ou OC ov
=2u.b+2v.q=2b(ax+by)+2q(px+qy)
oy
ou Oy Ov Oy
上式在三个以上的变量的情况下也同样成立。
例6当C=t+v?+wP, u=ax+by+Gz, v=ax+by+Gz, W=ayx+b,y+C3Z
(a、b、G,为常数，i=1,2,3）时，
dC dCou dCOv OCow
Ox Ou Ox Ov Ox Ow Ox
=2u.a,+2v.a,+2w·as
=2a,(a,x+b,y+cz)+2a,(a,x+b,y+C,z)+2a,(a,x+b,y+c,z)
ou
Ox
oC
x
（u
(r
(u）
ou
dC
九
C
60
C
®
y
O
ax
③
N
③
Ow
例6的变量关系。
深度学习的数学.indd 79
2019/7/2 10:46:5380
第2章神经网络的数学基础
2-9
梯度下降法的基础:
多变量函数的近似公式
梯度下降法是确定神经网络的一种代表性的方法。在应用梯度下降
法时，需要用到多变量函数的近似公式。
注：本节所考察的函数是充分光滑的函数。
单变量函数的近似公式
首先我们来考察单变量函数y=f(x)。如果x作微小的变化，那么函
数值，将会怎样变化呢？答案就在导函数的定义式中（2-6节）。
f(x+Ax)-f(x)
f"(x)= lim 4
Ax

在这个定义式中，Ax为“无限小的值”，不过若将它替换为“微小的值”，
也不会造成很大的误差。因而，下式近似成立。
/"(r)= (tx+Ar)-/()
Ax
将上式变形，可以得到以下单变量函数的近似公式。
f(x+Ax)=f(x)+f"(x)Ar (Ax为微小的数）
(1)
例1当/(x)=『时，求x=0附近的近似公式。
将指数丽数的求导公式/x)=e(2-6节）应用在式(1)中，如下所示。
etw=e"+e*Ax (Ax为微小的数)
取x=0,重新将Ax替换为x，可得e°=1+x（x为微小的数）。
深度学习的数学.indd 80
2019/7/2 10:46:532-9梯度下降法的基础:多变量函数的近似公式
81
这就是例1的解答。
下面的图像是将y=e·与y=1+x画在一张图上。在x=0附近两个
丽数的图像重叠在一起，由此可以确认例1的解答是正确的。
在x=0附近，y=e*与y=1+x的
图像重叠
多变量函数的近似公式
下面我们将单变量函数的近似公式（1)扩展到两个变量的函数。如果
x、，作微小的变化，那么函数z=f(x,y）的值将会怎样变化呢？答案是以
下的近似公式。Ax、Ay为微小的数
/(&+&,+Ay)=-/c;,)0(&)
@J(x,)Ay
(2)
Ox
Oy
例2当z=e*"时，求x=y=0附近的近似公式。
将指数函数的求导公式名_
=er+y(2-6节）应用在式(2)中，可得
axOy
e*Awt+AW'=e*"'+e*"^Ax+e"Ay (Ax、Ay为微小的数）
取x=y=0，重新将Ax替换为x，将Ay替换为y，可得
e*tV-1+x+y (x、y为微小的数）
以上就是例2的解答。下面我们试着化简式(2)。首先定义如下的Az.
Az=f(x +Ax,y +Ay)-.f(x, y)
上式表示当、，依次变化Ar、Ay时函数z=f(x. y）的变化，于是式（2)可
以像下面这样简洁地表示。
深度学习的数学.indd 81
2019/7/2 10:46:5382
第2章神经网络的数学基础
0z
AzL
A
Ox
O}
(3)
通过这样的表示方式，就很容易将近似公式(2)进行推广。例如，变
量z为三个变量w、x、y的函数时，近似公式如下所示。
AsL%Aw+
O& x*
G&LN
(4)
Ow
Ox
Oy
近似公式的向量表示
三个变量的函数的近似公式(4)可以表示为如下两个向量的内积
Vz·Ax的形式。
OzOz
CZ
Vz=
Ax=(Aw, Ax, Ay)
(5)
ow'axOy
注：V通常读作nabla（2-10节）。
我们可以很容易地棋象，对于一般的n变量函数,近似公式也可以像这
样表示为内积的形式。这个事实与下一节要考察的梯度下降法的原理有关
V
:备注泰勒展开式
将近似公式的一般化公式称为泰勒展开式。例如，在两个变量的情况下，
这个公式如下所示。
/(t+&;,+Ay=(c,)+g
of
-Ax-
-A
Ox
Oy
1 of
H(ar*+2B'
0-1
-AxAy+
-(Ay)
2! Ox
Oxoy
oy
1 |o*f
-(x)"(Ay)+3
、oif
"3!(@qxr(ary'+30'y
>^r(y)`+ 8'
(Av)
OxOy
Oxay
O
在泰勒展开式中，取出前三项，就得到式（2)
,0f
d of
a*f
d of
此外，我们约定
Ox?
ax Ox'
Ox Oyax Oy
深度学习的数学.indd 82
2019/7/2 10:46:542-10梯度下降法的含义与公式
83
2-10
梯度下降法的含义与公式
应用数学最重要的任务之一就是寻找函数取最小值的点。本节我们
来考察一下著名的寻找最小值的点的方法一
-梯度下降法。在第4章和
第5章中我们将会看到，梯度下降法是神经网络的数学武器
本节主要通过两个变量的函数来展开讨论。在神经网络的计算中，往
往需要处理成千上万个变量，但其数学原理和两个变量的情形是相同的
注：同样，本节考察的函数是充分光滑的函数。
梯度下降法的思路
已知函数z=/fx,y)，怎样求使函数取得最小值的x、y呢？最有名的
方法就是利用“使函数z=f(x,）取得最小值的x、y满足以下关系”这个
事实（2-7节）。
0f(x, )=0，
0f(x,)=0
(1)
Ox
Oy
这是因为，在函数取最小值的点处，就像葡萄酒杯的底部那样，与
函数相切的平面变得水平。
z=f(x, y)
01-0. 0-0
Oy
式(1)的含义。在函数取
最小值的点的附近，函数
的增量为0。不过，这个
式子终归只是必要条件。
深度学习的数学indd 83
2019/7/2 10:46:5484
第2章神经网络的数学基础
然而，在实际问题中，联立方程式(1)通常不容易求解，那么该如何解
决呢？梯度下降法是一种具有代表性的替代方法。该方法不直接求解式()
的方程，而是通过慢慢地移动图像上的点进行摸索，从而找出函数的最小值。
我们先来看看梯度下降法的思路。这里我们将图像看作斜坡，在斜
坡上的点P处放一个乒乓球，然后轻轻地松开手，球会沿着最陡的坡面
开始滚动，待球稍微前进一点后，把球止住，然后从止住的位置再次松
手，乒乓球会从这个点再次沿着最陡的坡面开始滚动。
z=f(x, y)
的图像
将函数图像的一部分放
大，并看作坡面。球沿
着最陡的坡面(PQ方
向）开始滚动。
这个操作反复进行若干次后，乒乓球沿着最短的路径到达了图像的
底部，也就是函数的最小值点。梯度下降法就模拟了这个球的移动过程。

人按照乒乓球的移动轨迹
来走的话，就会沿着最短
路径 R1到达图像的底部
（最小值）。
在数值分析领域，梯度下降法也称为最速下降法。这个名称表示沿
着图像上的最短路径下降。
近似公式和内积的关系
让我们依照前面考察过的思路来将梯度下降法正式化。
函数z=f(x,)中，当x改变Ax,y改变Ay时，我们来考察函数frx, y)
的值的变化Az。
深度学习的数学.indd 84
2019/7/2 10:46:542-10梯度下降法的含义与公式
85
Az=f(x+Ax,y+Ay)-f(x,y)
根据近似公式（2-9节)，以下关系式成立。
Az=
Of (x, y)
Ox
Ax-
af(x,Y) Ay
(2)
Oy
z=f(x,y)
图中，根据2-9节的公式，
Ay
Az=fx+ Ax, y+ Ay)-fx
y）与Ax、Ay之间的关系式
(2)成立。
我们在上一节也提到过，式(2)的右边可以表示为如下两个向量的内
积（2-4节）形式。
Of(x,y）
0f(x, y)
, (Ax, Ay)
(3)
Ox
dy
请大家注意这个内积的关系，这就是梯度下降法的出发点。
Of(x, y) f (x, y)
Ox
Oy
→A=
g0I(+)^x+f(&,D2y
内积
ax
Oy
(Ax, Ay)
式（2)左边的Az可以用式(3)的两个向量的内积形式来表示。
向量内积的回顾
我们来考察两个固定大小的非零向量α、b。当b的方向与α相反时
内积α·b取最小值（2-4节）。
深度学习的数学.indd 85
2019/7/2 10:46:5486
第2章神经网络的数学基础
向量a、b的内积为|a |b lcost
b
一（0为两个向量的夹角）（左图）。
0为180°时（即a、b方向相
反)，内积的值最小（右图）。
换句话说，当向量b满足以下条件式时，可以使得内积α·b取最小值。
b=-ka (k为正的常数）
(4)
内积的这个性质(4)就是梯度下降法的数学基础。
二变量函数的梯度下降法的基本式
当x改变Ax,，改变△y时，丽数(r,)的变化Az为式（2)，可以表
示为式(3)的两个向量的内积。根据式(4)，当两个向量方向相反时，内
积取最小值。也就是说，当式(3)的两个向量的方向恰好相反时，式（②)
的Az达到最小（即减小得最快）。
Of(x,y) f(r.)
Ox
Oy
当式(3)的两个向量方向相反时：
式(2)的Az最小，换言之，就是
￥（Ar, Ay)
沿着图像最陡的坡度减小。
根据以上讨论我们可以知道，从点（x,）向点(x+Ax, y+Ay）移动时，
当满足以下关系式时，函数z=f(x,y)减小得最快。这个关系式就是二变
量函数的梯度下降法的基本式。
(Ax,Ay)=
Of(x;, y)f(x, y)
(1为正的微小常数）
(5)
ox
dy
注：希腊字母，读作ita，对应拉丁字母i。这里也可以像式(4)那样使用字母k，不过
大多数文献中采用。
利用关系式(5)，如果
从点(x,y)向点(x+Ax,y+Ay)移动
(6)
深度学习的数学.indd 86
2019/7/2 10:46:552-10梯度下降法的含义与公式
87
就可以从图像上点(x,)的位置最快速地下坡。
f(x;, y)
的图像
当满足关系式(5)
Of(x; y)df(x, y)
时，函数图像减小
Ax,Ay)
Ox
Oy
得最快。
式（5)右边的向量
0f(x,y) @f(x,y)
称为函数f(x,y)在点(x,y）处
Ox
Oy
的梯度（gradient）。这个名称来自于它给出了最陡的坡度方向。
例题设△x、Ay为微小的数。在丽数z=+2中，当x从1变到1-
Ax、y从2变到2+Ay时，求使这个函数减小得最快的向量（Ax, Ay)。
解根据式(5)，Ax、Ay满足以下关系：
z 0z
(Ax, Ay)=-1
"ar'ey）

(n为正的微小常数)
图为
=2x
0z
=2y，依题意可知x=1，y=2，于是有
Ox
Oy
(Ax, Ay) =- n(2, 4）（n为正的微小常数)
梯度下降法及其用法
为了弄清梯度下降法的思路，前面我们考察了兵乓球的移动方式
由于在不同的位置陡坡的方向也各不相同，通过反复进行“一边慢慢地
移动位置一边寻找陡坡”的操作，最终可以到达函数图像的底部，也就
是函数的最小值点。
下山的情形也是一样的。最陡的下坡方向在每个位置各不相同。因
深度学习的数学.indd 87
2019/7/2 10:46:5588
第2章神经网络的数学基础
此，要想通过最短路径下山，就必须一边慢慢地下坡一边在每个位置寻
找最陡的坡度
在函数的情况下也完全一样。要寻找函数的最小值，可以利用式(5)
找出减小得最快的方向，沿着这个方向依照上述(6)稍微移动。在移动后
到达的点处，再次利用式(5)算出方向，再依照上述(6)稍微移动。通过反
复进行这样的计算，就可以找到最小值点。这种寻找函数x,）的最小值
点的方法称为二变量函数的梯度下降法
P,(x, Yo)
从初始位置P.出发,利用式(5)
P,(x,, y2
(6)求出最陡坡度的点P，然后
P(x, y)
从P,出发，利用式(5)、（6)进
B(x;, y,)
步求出最陡坡度的点P，即反复
利用式(5)、（6)，最终得以最快速
最小值点
地到达最小值点。这就是梯度下

降法。
下一节我们将用Excel 来体验梯度下降法，以便更具体地理解上面讲
解的内容
将梯度下降法推广到三个变量以上的情况
二变量函数的梯度下降法的基本式(5)可以很容易地推广到三个变量
以上的情形。当函数/由n个自变量x. s…,x构成时，梯度下降法的
基本式(5)可以像下面这样进行推广。
设，为正的微小常数，变量×y 2 …, x，改变为 xi+Ar,.2 A,"
x,+ Axn，当满足以下关系式时，函数，减小得最快。
of
(Ax, Ax, , .., Ar,)=
(of Ff....
(7)
Ox,' Oxy,
Oxr,
这里,以下向量称为函数/在点（Ar, , )处的梯度
图灵社区会员 ChenyangGao(2339083510@gq.com）专享 尊重版权
深度学习的数学.indd 88
2019/7/2 10:46:552-10梯度下降法的含义与公式
89
O .. f
OxOx,
Ox
与二变量函数的情况一样，利用这个关系式（7),如果
从点(,2,·,Xn)向点(X1+Ax1, X2+AN2, ·,xn+Axn)）移动
(8)
就能够沿着函数减小得最快的方向移动。因此,反复依照上述(8)来移
动，就能够在n维空间中算出坡度最陡的方向，从而找到最小值点。这
就是n变量情况下的梯度下降法。
此外，由于式（7、（8）是n维的，难以在纸上画出其图像。大家可以
利用二变量情况下的式(5)、（6)来直观地理解。
哈密顿算子V
在实际的神经网络中，主要处理由成千上万个变量构成的函数的最
小值。在这种情况下，像式()那样的表示往往就显得十分冗长。因此我
们来考虑更简洁的表示方法。
在数学的世界中，有一个被称为向量分析的领域，其中有一个经常
用到的符号又。又称为哈密顿算子，其定义如下所示。
off
Vf=

f
ax,'Ox,'
'x,
利用这个符号，式（)可以如下表示
(Ax, Ax, ., Ax,)=-Tf (n为正的微小常数）
注：如前所述（2-9节)，V通常读作nabla，来源于希腊竖琴的形象
例1对于二变量函数（x,y),梯度下降法的基本式(5)如下所示。
(Ax, Ay)=-7Vf(x, y)
深度学习的数学.indd 89
2019/7/2 10:46:5690
第2章神经网络的数学基础
例2对于三变量函数（r,y,z),梯度下降法的基本式（7)如下所示。
(Ax, Ay, Az)==7Vf(x, y, z)
其中，左边的向量（Ax, ., …, As）称为位移向量，记为Ax
Ax = (Axi1, A2,  , Axn)
利用这个位移向量，梯度下降法的基本式（7)可以更简洁地表示。
Ax=-nVf（n为正的微小常数）
(9)
n的含义以及梯度下降法的要点
到目前为止，1只是简单地表示正的微小常数。而在实际使用计算机
进行计算时，如何恰当地确定这个，是一个大问题。
从式(5)的推导过程可知，n可以看作人移动时的“步长”，根据，的
值，可以确定下一步移动到哪个点。如果步长较大，那么可能会到达最
小值点，也可能会直接跨过了最小值点（左图）。而如果步长较小，则可
能会滞留在极小值点（右图）。
1大
小
诚达
在神经网络的世界中，，称为学习率。遗憾的是，它的确定方法没有
明确的标准，只能通过反复试验来寻找恰当的值
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 90
2019/7/2 10:46:562-11 用Excel体验梯度下降法
91
2-11
用Excel体验梯度下隆法
梯度下降法是神经网络计算的基础，下面我们就通过Excel来弄清它的
含义。在观察逻辑过程时，Excel是一个优秀的工具，通过工作表我们可以
直观地看出梯度下降法是什么样的。例如，我们用Excel 来求解以下问题
例题对于函数z=x2+yP，请用梯度下降法求出使函数取得最小值的
x、y值。
注：我们在2-7节的例5中考察过，正确答案为（(x, y)=（0, 0)。另外，
2-7节中画了这
个函数的图像，大家可以参考一下。
解首先求出梯度
一
涕度
(2x,2y)
(1)
接下来，我们逐步进行计算。
①初始设定
随便给出初始位置(x.y)(i=0)与学习率
IA
B
C
D
梯度下降法
（例)z=x^+y2
N n
n
0.1
设置学习率
4
的
No
位置
梯度
位移向量
函数值
6
0z/0x
0z/0y
Ax
Ay
7
o/
3.00
2.00
设置初始位置
②计算位移向量
对于当前位置(x,y)，算出梯度式(1)，然后根据梯度下降法的基本
式（2-10节式(5))，求位移向量△r=（Ax, Ay)。根据式(1)，可得
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 91
2019/7/2 10:46:5692
第2章神经网络的数学基础
(Ax, Ay;)=-n(2x;, 2y,)=(-n.2x;, -n.2y,)
(2)
E7
-C
j&=2*C7
B
D
梯度下降法
（例）z=x?+y2
0.1
计算梯度(1）
No
位置
梯度
位移向量
函数值
6
Xi

2zl0xGzlbv
Ax
Ay
3.00
2.00
6.00
4.00
-0.60
-0.40
13.00
计算式(2）
3更新位置
根据梯度下降法，由下式求出从当前位置(x.y）移动到的点（xi+1.Vi+)。
(X;n+ }n)=(x;, V,)+(Ax;, Ay;)
(3)
O
C8
G
f& =C7+G7
B
C
H
梯度下降法
（例）z=x*+y2
2
3
0.1
4
计算式(3)
5
No
位置
梯度
位移向量
函数值
6
X;
0z10x
0z/0y
Ax
Ay
7
S
3.00
2.00
6.00
4.00
-0.60
-0.40
13.00
8
2.40
1.60
1
:备注单变量函数的梯度下降法
梯度下降法也可以用于单变量函数，只要将2-10节的式(7)解释为一维
向量（n=1）的情况就可以了。也就是说，将偏导数替换为导数，将得到的
下式作为梯度下降法的基本式。
Ax=-f'(x)（n为正的微小常数）
深度学习的数学.indd 92
2019/7/2 10:46:562-11 用Excel体验梯度下降法
93
4反复执行2～3的操作
下图是反复执行9～B的操作 30次后得出的坐标(xso. )的值。这
与2-7节的例5的正解（x, y)= (0,0）一致。
AAB
C
D
S
$
G
H
1
梯度下降法
（例)z=x+y
2
3
n
0.1
4
5
No
位置
梯度
位移向量
函数值
6
\
Xi
Vi
0z/0x
0z/0y
Ax
Ay
7
0
3.00
2.00
6.00
4.00
-0.60
-0.40
13.00
8
2.40
1.60
4.80
3.20
-0.48
-0.32
8.32
9
1.92
1.28
3.84
2.56
-0.38
-0.26
5.32
10
1.54
1.02
3.07
2.05
-0.31
-0.20
3.41
11
1.23
0.82
2.46
1.64
-0.25
-0.16
2.18
12
0.98
0.66
1.97
1.31
-0.20
-0.13
1.40
35
28
0.01
0.00
0.01
0.01
0.00
0.00
0.00
36
29
0.00
0.00
0.01
0.01
0.00
0.00
0.00
37
30
0.00
0.00
0.01
0.00
0.00
0.00
0.00
使函数取得最小值的（rx, y）
函数的最小值
Mer
...备注川与步长
我们在2-10 节提到可以将，看作步长，实际上这并不正确，正确的说
法是2-10节的式(5)(或者更一般的式(7)）的右边整个向量的大小为步长。
不过，虽然人的步长大体上是固定的，但梯度下降法的“步长”是不均匀的。
因为梯度在不同的位置大小不同。因此，在应用数学的数值计算中，有时会
将式(5)进行如下变形。
(Ax, Ay)=-1
(8/(x;,)@(x.,)|/|[0(.,)]`[(#(.,)
Ox
oy
V
Ox
oy
这样一来，梯度被修正为单位向量，我们也就可以将〃看作步长了。
深度学习的数学.indd 93
2019/7/2 10:46:5794
第2章神经网络的数学基础
2-12
最优化问题和回归分析
在为了分析数据而建立数学模型时，通常模型是由参数确定的。在
数学世界中，最优化问题就是如何确定这些参数
从数学上来说，确定神经网络的参数是一个最优化问题，具体就是
对神经网络的参数（即权重和偏置）进行拟合，使得神经网络的输出与
实际数据相吻合。
为了理解最优化问题，最浅显的例子就是回归分析。下面我们就利
用简单的回归分析问题来考察最优化问题的结构。
什么是回归分析
由多个变量组成的数据中，着眼于其中一个特定的变量，用其余的变
量来解释这个特定的变量，这样的方法称为回归分析。回归分析的种类有
很多。为了理解它的思想，我们来考察一下最简单的一元线性回归分析
·元线性回归分析是以两个变量组成的数据为考察对象的。下图给
出了两个变量x、y的数据以及它们的散点图。
名称
x
力
1
X
V1
2
X2
J2
>
X3
'3
.
.'
之
X,
yn
左边数据的散点图
数据
-元线性回归分析是用一条直线近似地表示右图所示的散点图上的
点列，通过该直线的方程来考察两个变量之间的关系。
这条近似地表示点列的直线称为回归直线。
深度学习的数学.indd 94
2019/7/2 10:46:572-12最优化问题和回归分析
95
用一条直线近似地表示散点
图上的点列，通过该直线的
方程来考察两个变量的关系
这样的分析方法就是一元线
X
性回归分析。这条直线称为
回归直线。
这条回归直线用一次关系式表示如下
y=px+q (p、q为常数）
(1)
这个式子称为回归方程。
x、y是为了将构成数据的各个值代人而设定的变量，右边的x称为
自变量，左边的y称为因变量。常数p、q是这个回归分析模型的参数，
由给出的数据来决定
注：p称为回归系数，称为截距。
通过具体例子来理解回归分析的逻辑
下面让我们通过具体的例子来看看回归方程(1)是如何确定的。
例题右表是7个高中三年级女学生的身
编号
身高x
体重y
高与体重数据。根据这些数据,求以体重
1
153.3
45.5
1y为因变量、身高x为自变量的回归方程
N
164.9
56.0
y=px+q (p、q为常数）。
3
168.1
55.0
4
151.5
52.8
9
157.8
55.6
7个学生的身高
o
156.7
50.8
与体重数据。
7
161.1
56.4
解设所求的回归方程如下所示。
y=px+q (p、q为常数）
(2)
深度学习的数学.indd 95
2019/7/2 10:46:5796
第2章神经网络的数学基础
将第飞个学生的身高记为xk，体重记为y/，可以求得第飞个学生的回
归分析预测的值（称为预测值)，如下所示。
pXk+ q
(3)
我们将这些预测值加以汇总，如下表所示。
编号
身高x
体重y
预测值px+4
1
153.3
45.5
153.3p + q
N
164.9
56.0
164.9p + q
cn
168.1
55.0
168.1p + q
4
151.5
52.8
151.5p + q
ln
157.8
55.6
157.8p + q
y的实测值和预测值。在考
6
156.7
50.8
156.7p +q
虑数学上的最优化问题时，
理解实测值和预测值的差异
161.1
56.4
161.1p+q
是十分重要的。
如下算出实际的体重，与预测值的误差e。
\
ek= yk-(pXk tq)
(4)
体重y
实际数据点
（实测值
yk
ek
（预测值）pxt t q
回归方程
用图说明式(3)和式(4)
身高x
的关系。第k个学生的
X`k
Nk、Jk、e的关系图。
这些e；的值既可以为正也可以为负。接下来我们来考虑下面的值C，
这个值称为平方误差
C;=;(e,)=
,{Vk=(px&+q)}
(5)
注：系数一是为了方便进行之后的处理，这个值对结论没有影响。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 96
2019/7/2 10:46:572-12最优化问题和回归分析
97
遍历全体数据，将它们的平方误差加起来，假设得到的值为Cr。
Cr=C1+C2++C7
根据之前的表以及式(5),用p、g的式子表示误差总和Cr,如下所示。
C=
{45.5-(153.3p+q)}?+
{56.0-(164.9p+q)}
(6)
-{50.8-(156.7p+q)}+
{56.4-(161.1p+q)}'
我们的目标是确定常数，、g的值。回归分析认为，p、g是使误差总
和式(6)最小的解。知道这个解的思路后，后面就简单了。我们利用以下
的最小值条件即可（2-7节）。
aC
-=0.
aCy=0
Op
aq
(7)
aC
=0
2
oc
=0
0q
式(7)的图形含义。
我们来实际计算一下式(6)。根据偏导数的链式法则（2-8节)，可得
aC
-=-153.3{45.5-(153.3p+q)}-164.9{56.0-(164.9p+q)}-
Op
-156.7{50.8-(156.7p+q)}-161.1{56.4-(161.1p+q)}=0
dC.
aq
-=-{45.4-(153.3p+q)}-{56.0-(164.9p+q)}
-{50.8-(156.7p+q)}-{56.4-(161.1p+q)}=0
深度学习的数学.indd 97
2019/7/2 10:46:5898
第2章神经网络的数学基础
整理后得到下式。
1113.4p+7q =372.1 , 177 312p+1113.4g =59 274
解这个联立方程，可得
p= 0.41, q=-12.06
从而求得目标回归方程(2),如下所示
y = 0.41x- 12.06
注：这时C-= 27.86。
60
与
y = 0.41x-12.06
So
◆
40
30
150
155
160
165
170
作为例题的解的回归直线。
以上就是一元线性回归分析中使用的回归直线的确定方法。这里的
重点是最优化问题的求解思路。这里所考察的最优化方法在后面的神经
网络的计算中也可以直接使用。
代价函数
在最优化方面，误差总和C.可以称为“误差函数”“损失丽数”“代
价函数”等。本书采用代价函数（cost function）这个名称。
注：之所以不使用误差函数（errorfunction)、损失函数（lost function）的叫法，是因为
它们的首字母容易与神经网络中用到的熵（entropy)、层（layer）的首字母混淆。
此外，除了这里所考察的平方误差的总和C.之外，根据不同的思路，
代价函数还存在其他多种形式。利用平方误差的总和C.进行最优化的方法
称为最小二乘法。本书中我们只考虑将平方误差的总和 C.作为代价函数。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 98
2019/7/2 10:46:582-12最优化问题和回归分析
99
问题如右表所示，已知3名学生的数学成
编
数学
理科
绩和理科成绩。根据这些数据，求以理科
成绩x
成绩J
成绩，为因变量、数学成绩x为自变量的
^
8
，线性回归方程
2
5
4
3
9
8
解请参考3-4节
模型参数的个数
我们再来看看之前的例题。模型有2个参数、q，而已知的条件
(数据的规模）有7个。也就是说，模型的参数的个数（2个）小于条件
的个数（7个)。反过来说，回归方程是根据大量的条件所得到的折中结
果。这里所说的“折中”是指，理想中应该取值0的代价函数式（6)只能
取最小值。因此，模型与数据的误差C.不为0也无须担心。不过，只要
误差接近0,就可以说这是合乎数据的模型。
此外，模型的参数个数大于数据规模时又如何呢?当然，这时参数就
不确定了。因此,要确定模型，就必须准备好规模大于参数个数的数据
Mer
.备注常数和变量
在回归方程(1)中，x、y分别称为自变量、因变量，p、q为常数。不过
在代价函数式(6)中，p、q是被作为变量来处理的。正因为这样，我们才能
考虑式(6)的导数。
像这样，根据不同的角度，常数、变量是变幻不定的。从数据的角度来看
回归方程的x、y为变量，从代价函数的角度来看，D、q为变量。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 99
2019/7/2 10:46:58深度学习的数学.indd 100
2019/7/2 10:46:58=3.
神经网络的最优化
第1章我们考察了什么是神经网络以及它的设计思想。本
章我们来考察在数学上是怎样确定神经网络的。
深度学习的数学.indd 101
2019/7/2 10:46:58102第3章神经网络的最优化
3-1
神经网络的参数和变量
第1章我们考察了神经网络的思棋和工作原理。不过，要在数学上
实际地确定其权重和偏置,必须将神经网络的思想用具体的式子表示出
来。作为准备，本节我们来弄清权重与偏置的变量名的标记方法。
参数和变量
从数学上看，神经网络是一种用于数据分析的模型，这个模型是由
权重和偏置确定的(1-4节)。像权重和偏置这种确定数学模型的常数称
为模型的参数。
除了参数以外，数据分析的模型还需要值根据数据而变化的变量
但是参数和变量都用拉丁字母或希腊字母标记，这会引起混乱。而区分

用于代人数据值的变量和用于确定模型的参数，对于逻辑的理解是不可
或缺的。让我们通过以下例子来看一下。
例1在一元线性回归分析模型中，截距和回归系数是模型的参数，自变
量和因变量是代入数据值的变量（2-12节)。
参数
回归方程
回归方程的常数八、q为参数
变量
代入数据值的x、y为变量。
例2在神经网络中，当输入为x、、x时，神经单元将它们如下整合为
加权输人z,通过激活函数a)来处理（1-3节）。
Z1=WiX1+W2X2+ W3xg + b （W1、W2、Ws为权重，b为偏置）
a,=a(z,)
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 102
2019/7/2 10:46:583-1神经网络的参数和变量103
输入
权重
偏置b
输出a, =a(z,)
输入层的神经单元的图，
权重和偏置为参数。
此时，权重w、W2、W,与偏置b为参数，输人X、X2、心、加权输
入z1、神经单元的输出，为变量，变量的值根据学习数据的学习实例而
变化。
神经网络中用到的参数和变量数量庞大
在实际进行神经网络的计算时，往往会被数量庞大的参数和变量所
困扰。构成神经网络的神经单元的数量非常大，相应地表示偏置、权重、
输入、输出的变量的数目也变得非常庞大。因此，参数和变量的表示需
要统一标准。本节我们就来进行这一工作。
注：到目前为止的表示方法都没有考虑统一性。
神经网络领域现在还处于发展的早期阶段，还没有确立标准的表示
方法。下面我们将介绍一下多数文献中采用的表示方法，并将其应用在
本书中。
神经网络中用到的变量名和参数名
本书主要考察阶层型神经网络（1-4节)。这个网络按层区分神经单
元，通过这些神经单元处理信号，并从输出层得到结果。
下面我们就来确认一下这个神经网络中的变量和参数的表示方法：
首先，我们对层进行编号，如下图所示，最左边的输入层为层1，隐
藏层（中间层）为层2、层3……最右边的输出层为层1（这里的（指last
的首字母，表示层的总数）。
深度学习的数学.indd 103
2019/7/2 10:46:59104
第3章 神经网络的最优化
输入层
隐藏层（中间层）
输出层
层3
云
阶层型神经网络的各层的名称。
进行以上准各后，我们将如下表所示来表示变量和参数
符号
含义
表示输人层（层1）的第i个神经单元的输入的变量。由于输人层的神经
X
单元的输入和输出为同一值，所以也是表示输出的变量。此外，这个变
量名也作为神经单元的名称使用
从层1-1的第i个神经单元指向层1的第j个神经单元的箭头的权重。请
wj,
注意i和j的顺序。这是神经网络的参数
zy
表示层1的第j个神经单元的加权输入的变量
3
层1的第j个神经单元的偏置。这是神经网络的参数
层1的第j个神经单元的输出变量。此外，这个变量名也作为神经单元的
q
名称使用
表格中各符号的含义如下图所示。
层编号
偏置b
输出a
权重w
1-1层
l层
层1中神经单元的
层1-1中发出箭头的
位置
神经单元的位置
偏置b
加权输入z/对应的神经单元的输出为
神经单元与输出变量共用名称。
af= alz)（a(z)为激活函数）。
深度学习的数学.indd 104
2019/7/2 10:46:593-1 神经网络的参数和变量105
下面让我们利用第1章考察过的例题来确认一下上表中的变量名和
参数名的含义
例题建立一个神经网络，用来识别通过4×3像素的图像读取的手写
数字0和1，其中像素是单色二值
注：这是1-4 节考察过的例题。下图是解答示例。
解
输入层
隐藏层
输出层
O
（中间层）

图像
O
O
O
O
4×3像素
O
C
识别通过4×3像素
(单色二值）的图像读
取的手写数字0、1的
神经网络（1-4节）。
输入层相关的变量名
输人层为神经网络的数据入口，如果表示输入层的输人的变量名依
次为x1,.2,…，由于输人层中神经单元的输入和输出为同一值，那么它们也
是输出的变量名。本书中神经单元的名称也使用输入变量名x,x,…来表示。
深度学习的数学.indd 105
2019/7/2 10:46:59106
第3章 神经网络的最优化
输入层
O
国
(X3人
输入层的神经单元的输入的变量
名依次为x,2,,F12,它们也
4×3像素
是输出的变量名。在例题中，它
GD
们表示代入像素值的变量。
隐藏层、输出层相关的参数名与变量名
这里我们截取神经网络的一部分，并按照前面表格中的约定标注变
量名和参数名，如下图所示。
层1
层2
层3
（输入层）
（隐藏层）
（输出层)
X1
输入层、隐藏层与输出层的简
略图。我们根据前面表格中的
约定，在图中标注相关的符
号。其中，圆圈中的神经单元
名使用的是输出变量名。
下表列举了图中几个符号的具体含义
符号示例
符号示例的含义
b?
层2(隐藏层）的第1个神经单元的偏置
b?
层3(输出层）的第1个神经单元的偏置
从层1的第2个神经单元指向层2的第1个神经单元的箭头的权重，也就
w,
是层2（隐藏层）的第1个神经单元分配给层1（输入层）的第2个神经单
元的输出x的权重
从层2的第2个神经单元指向层3的第1个神经单元的箭头的权重，也就
w,
是层3(输出层）的第1个神经单元分配给层2(隐藏层）的第2个神经单
元的输出的权重
深度学习的数学.indd 106
2019/7/2 10:46:593-1 神经网络的参数和变量107
例3 右图为前面出现过的神经网络的一部分
输入层
隐藏层
x为输人层（层1）的第3个神经单元的输入和输
(层1）
(层2)
出。从这个神经单元指向隐藏层（层2）的第
2个神经单元的箭头的权重为w3。此外，隐藏
层的第2个神经单元的输出为a，偏置为b。
偏置b
问题右图为前面出现过的神经网络的一部
分。请说明图中a、w、a』、b号的含义
隐藏层
输出层
(层2
(层3
权重w
解a公为隐藏层（层2）的第3个神经单元
的输出。从这个神经单元指向输出层（层3)
的第2个神经单元的箭头的权重为w。此
扁望
外，输出层的第2个神经单元的输出为α，偏置为b窝
变量值的表示方法
在前面讲解变量名的表格中，x、z、α4为变量，它们的值根据学习
数据的学习实例而变化。通过例题来说明的话，若具体地给出了学习数
据的一个图像,则x:z、α就变成了数值，而不是变量。
例4在例题中，假设给出了下面的图像作为学习实例。在将这个图像输
人到神经网络中时，求隐藏层（层2）的第1个神经单元的加权输入动
的值。
灰色部分为1，白色部分为0，于是可得x，=1,
X2=1,Xg=0,Xa=0,Xs=1,Xg=0,X,=0,Xg=1
g=0,.x1,=0,x=1,x12=0
深度学习的数学.indd 107
2019/7/2 10:47:00108第3章神经网络的最优化
根据前面的变量名的一般约定，加权输人才可以如下表示
z?=wjix,+ 2x&,+ jx;+:+ Wi2Xiz+b?
(1)
由于读取图像后，输入层的x,x..x1,的值就确定了，所以加权输
入学的值可以像下面这样确定。
考的值=w,x1+w,x1+wg ×0++wn×0+b
(2)
= wi+ w, + wi, + ws +wn +b?
这样一来，加权输人学的具体值就可以通过式(2)给出。这就是例4
的解答。
注：权重（w，等）和偏置〆为参数，它们都是常数。在不清楚变量和常数的关系时，请
参考2-12节的回归分析的相关内容（参考本节末尾的备注）。
从例4中可以知道，我们需要区分变量x；、z、α以的符号与它们的
值的符号。在后面计算代价函数时，这一点非常重要。在给定学习数据

的第k个学习实例时，各个变量的值可以如下表示。
x,[k]：输入层的第i个神经单元的输入值（一输出值）
z,[k]：层1的第j个神经单元的加权输人的值
(3)
a[k]：层l的第j个神经单元的输出值
注：这种表示方法是以C语言等编程语言的数组变量的表示方法为依据的。
例5在例4中，假设输入图像为学习数据的第7张图像。这时，根据
(3)的约定，输入层的变量的值以及加权输人。的值可以如下表示。
x,[7]=1, x,[7]=1, x,[7]=0, xa[7]=0, x,[7]=1l, x[7]=0,
x,[7]=0, x;[7]=1, x,[7]=0, xo[7]=0, xy|[7]=1, x,[7]=0,
z?[7]=w?,+w,+w;+wg+win+b?
以上为例5的解答。它们的关系如下图所示。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 108
2019/7/2 10:47:003-1 神经网络的参数和变量109
图像7
输入层(层1)
加权输入
77
627
617
o
O
将第7个图像输入
隐藏层(层2）
神经网络时变量值
的表示方法。
例6在例题中，输出层（层3）的第；个神经单元的加权输入为z，输
出变量为α；，在将学习数据的第1张图像作为图像实例输入时，输出层
(层3）的第j个神经单元的加权输入的值为 z[1，输出值为[1，如下
图所示。
第1张图像

变量

值
aj
V
a`[1]
输入第1张图像时输出层的
zj
z}[1]
值的表示方法。
本书中使用的神经单元符号和变量名
到目前为止，本书中的示意图都是将参数和变量写在一个神经单元
的周围，这就导致图看起来非常吃力。因此，之后我们将根据情况使用
如下所示的标有参数和变量的神经单元示意图。
权重
右图将权重w、加权输入z
二
偏置b
偏置/和输出值α,紧凑地整
输出a
合在了一起。
深度学习的数学.indd 109
2019/7/2 10:47:01110
第3章神经网络的最优化
利用这种整合了参数和变量的示意图,就可以简洁地表示两个神经
单元的关系，如下所示。
层1-1的第i个
层/的第j个
神经单元
神经单元
利用整合了参数和变量的示
意图来表示两个神经单元的
关系。
-...《备注回归分析中的变量与变量值的关系
在2-12节的回归分析中，回归方程如
数据名
X
y
下所示。
1
X,
y1
y=px+q (p、q为常数）
(4)
:
`.
:
其中，p为回归系数，α为截距。另外，x为
k
Xk
'k
自变量，，为因变量，用于代入数据。
:
:
在2-12节介绍的回归分析中，如右表
所示，x、y的变量值标记为x、Jk。k表示数
n
戈
y,
据的第k个元素。例如，第1个元素表示为x1、yo
在神经网络中，第k个变量值不能像回归分析那样用下标形式表示，这
是因为下标太多了。实际上，若对输入变量x；、加权输入变量/、输出变量
a，以下标形式附加“第飞张图像”这样的信息，看起来会非常吃力。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 110
2019/7/2 10:47:013-2神经网络的变量的关系式
111
3-2
神经网络的变量的关系式
要确定神经网络,就必须在数学上确定其权重和偏置,为此需要用
具体的式子来表示神经单元的变量的关系。我们利用上一节的约定来实
际尝试一下。
与上一节一样，我们通过第1章考察过的如下例题来展开讨论。
例题建立一个神经网络，用来识别通过4×3像素的图像读取的手写
数字0和1，其中像素是单色二值
注：变量和参数根据3-1节的约定来命名。
层1
层2
层3
(输入层）
（隐藏层）
(输出层）
图像实例
例题的解答示例的神经网络的简略图。神经单元名使用的是输出变量名。
输入层的关系式
输人层（层1）神经网络的信息入口。这个层的第i个神经单元的输
入与输出为同一值x(i=1,2,,12）。
下面，我们将变量名α的约定（3-1节）推广并应用到输人层。将
a/定义为层1的第j个神经单元的输出值，由于输入层为层1(即1=1)
深度学习的数学.indd lll
2019/7/2 10:47:0112
第3章神经网络的最优化
所以前面的x；可以如下表示。
Xi=a
这个表示方法在后面的误差反向传播法中会用到。
隐藏层的关系式
我们来写出例题中的隐藏层（层2）相关的变量、参数之间的关系
式。以a(z)作为激活函数，根据1-4节，变量和参数的关系可以表示为
如下式子。
z?=wjx,+wjx&+mjx,++ mji2x,+b?
z2 =Yjix,+w2x, +W;;x, +.+ w22Xi, +b3
(1)
z? =wjx,+w2x, + w,x ++ wg12xrp, +b?
a?=a(z), a&=a(zi), a}=a(z})
输入层(层1)
隐藏层(层2
输出
偏置b3
图中给出了隐藏层（层2）的
第1个神经单元的加权输入
才和输出a。
输出层的关系式
下面我们来写出例题中的输出层（层3）相关的变量、参数之间的
关系。与式(1)一样,如下所示
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 112
2019/7/2 10:47:013-2神经网络的变量的关系式
113
zi=wi,a? +wi,a, + w,a? +bi
z2=wj,a^+ w,,a3+w;,a,+b;
(2)
a`=a(zj), a;=a(z;)
隐藏层(层2）输出层(层3)
输出
a5
图中给出了输出层（层3）的
偏置
第2个神经单元的加权输入z
bi
和输出a。
从以上的式(1)、式(2）可以知道，在理解关系式时要常常回想起神
经网络，这一点十分重要。否则，这些关系式看起来就像是蚂蚁的队列，
无法看出其深意。
V
/...：备注神经网络的变量的矩阵表示
如果将式（1)、式(2)用矩阵（2-5节）来表示的话，就很容易看清式子
的整体关系。下面我们就试着将式(1)、式(2)用矩阵来表示。
*
wi
wi?2
wi
...
wn
X2
43
州
'
wi
W3,
wi
..
w3
X
b
z}
wj
w32
w
w 2
b3
x
a
勿饼
wiz
wi,
b

古
w
n;,
w2,
b5
as
计算机编程语言中都会有矩阵计算工具，所以将关系式变形为矩阵形式
会有助于编程。另外，用矩阵表示关系式，还具有容易推广到一般情形的好处，
因为式子的全部关系变得很清楚。
深度学习的数学.indd 113
2019/7/2 10:47:02114
第3章神经网络的最优化
3-3
学习数据和正解
在神经网络进行学习时，为了估计神经网络算出的预测值是否恰当。
需要与正解进行对照。本节我们就来考察正解的表示方法。
回归分析的学习数据和正解
利用事先提供的数据（学习数据)来确定权重和偏置,这在神经网
络中称为学习（1-7节)。学习的逻辑非常简单，使得神经网络算出的预
测值与学习数据的正解的总体误差达到最小即可
不过，第一次听到“预测值”“正解”时，可能难以想象它们的关
系，这种情况下可以利用回归分析。下面我们来考察一下下面的例1。
例1如右表所示，已知3名学生的数学
编号数学成绩x理科成绩，
成绩和理科成绩。以数学成绩为自变量
7
8
求用于分析这些数据的线性回归方程
5
4
3
9
8
3
注：这个问题的解答请参考下一节。此外，关于回归分析的内容请参考2-12节
解回归分析的学习数据是例1的表中的全部数据。数学成绩和理科成绩
分别用x、y表示，线性回归方程如下所示
y=px+q （p、q为常数）
(1)
我们以第1个学生为例来考察。这个学生的数学成绩为7分，利用
式(1)对理科成绩进行预测，如下所示。
7p+4
这就是第1个学生的理科成绩的预测值。因为这个学生的实际理科
深度学习的数学.indd 114
2019/7/2 10:47:023-3 学习数据和正解
115
成绩为8分，所以这个8分就是预测值对应的正解。
-般地，将第太个学生的数学成绩和理科成绩分别表示为x、J
(k= 1,2,3 )，则pxxtg为预测值，次为正解，二者的关系如下图所示
正解
回归直线
y =pxtq
预测值
第k个学生的数学成绩和理科成
绩分别为x、y，.时预测值与正解
的关系
神经网络的学习数据和正解
在回归分析的情况下，如上所示，由于全部数据都整合在表格里
所以预测值和正解的关系很容易理解。而在神经网络的情况下，则通常
无法将预测值和正解整合在一张表里。
例如，我们来考虑下面的例2，该例题在前两节也出现过。
例2 建立一个神经网络，用来识别通过4×3像素的图像读取的手写
数字0和1,其中像素是单色二值
解这里以下面的3张图像作为学习实例。我们可以判断出数字依次是0、
1、0，但刚刚建立好的神经网络则无法做出判断。
图像模式
对于刚刚建立好的神经网络
而言，图像的含义不明
因此，需要将图像的含义，也就是正解教给神经网络，如下所示。
深度学习的数学.indd 115
2019/7/2 10:47:02116
第3章
神经网络的最优化
1号
2 号
3号
图像模式
正解
0
1
0
学习数据的图像
和正解的示例。
那么，如何将这些正解教给神经网络呢？这个问题不像前面的回归
分析的例子那样简单，需要想点办法来解决
正解的表示
神经网络的预测值用输出层神经单元的输出变量来表示。以例2的
神经网络为例，它的输出层的神经单元如下图所示（3-1节、3-2节）。
输出层

例2的神经网络的简略图。输出层的第1个
神经单元的目标是检测出数字0，第2个神
经单元的目标是检测出数字1。此外，神经
单元名使用的是输出变量名。
我们希望输出层的第1个神经单元·对手写数字0产生较强反应
第2个神经单元a·对手写数字1产生较强反应（1-4节)。使用Sigmoid
函数作为激活函数时，预测的值如下表所示。
预测的值
图像为0时
图像为1时
a
接近1的值
接近0的值
a
接近0的值
接近1的值
深度学习的数学.indd 1l6
2019/7/2 10:47:023-3 学习数据和正解
117
如上表所示，输出变量有2个，分别为a、。而例2的正解只
有1个，为0或1。那么如何将1个正解和2 个输出变量对应起来呢?
对于这个问题，解决方法是准备2个变量1、12作为正解变量，分别
对应输出层的2个神经单元。
输出层
正解
ai)
t为teacher的首字母。
对照输出层神经单元的输出变量、定义变量1、t2，如下所示
含义
图像为0
图像为1
1
0的正解变量
1
0
t2
1的正解变量
0
\
下图所示为2个图像实例的各变量值。
数字О
输出层
正解
数字1
输出层
正解
a
a?
4t=0
t2=
以上就是神经网络的正解的表示方法。
，通过这样的方式来定义正
解，就可以像下面这样表示神经网络算出的预测值和正解的平方误差
(2-12节）。
;{(,-=?)}*+(#,-=!)`}
(2)
其中，系数
一是为了方便后面的计算
深度学习的数学.indd 117
2019/7/2 10:47:03118
第3章
神经网络的最优化

.....备注交叉熵
本书使用上述式(2)的平方误差作为实际数据和理论值的误差指标。虽
然这个指标容易理解，但由于存在计算收敛时间长的情况，所以也有难点。
为了克服这个缺陷，人们提出了各种各样的误差指标，其中特别有名的一个
指标就是交叉熵
交叉熵将上述误差函数(2)替换为下式。
,[{, loga, +(1-t,)log(1-a,)}+{t,loga,+(1-t,)log(1-a,)}]
77
上式中，n为数据的规模。利用这个交叉熵和 Sigmoid函数，可以消除Sigmoid
函数的冗长性，提高梯度下降法的计算速度。
此外，交叉熵来源于信息论中熵的思想。
深度学习的数学.indd 118
2019/7/2 10:47:033-4神经网络的代价函数
119
3-4
神经网络的代价函数
向神经网络提供学习数据，并确定符合学习数据的权重和偏置,这
个过程称为学习。这在数学上一般称为最优化，最优化的目标函数是代
价函数。本节我们就来看一下代价函数的相关内容
表示模型准确度的代价函数
用于数据分析的数学模型是由参数确定的。在神经网络中，权重和
偏置就是这样的参数。通过调整这些参数，使模型的输出符合实际的数
据（在神经网络中就是学习数据)，从而确定数学模型，这个过程在数学
上称为最优化（2-12节)，在神经网络的世界中则称为学习（1-7节）。
不过，参数是怎样确定的呢？其原理非常简单，具体方法就是，对
于全部数据，使得从数学模型得出的理论值（本书中称为预测值）与实
际值的误差达到最小
实际值
从数学模型求出的
理论值
an
N

对于全部数据,使实际值与
TTTTTTIT
预测值(即理论值）的误差
达到最小，以此来确定数学
←误差→
模型的参数。
在数学中，用模型参数表示的总体误差的函数称为代价函数，此外
也可以称为损失函数、目的函数、误差函数等。如前所述（2-12节)，本
书采用“代价函数”这个名称。
回归分析的回顾
我们可以使用2-12节考察的回归分析来理解最优化的含义和代价函
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 119
2019/7/2 10:47:03120第3章神经网络的最优化
数。这里我们通过下面的例1来回顾一下回归分析。
例1如右表所示，已知3名学生的
编
号
数学成绩x
理科成绩j
数学成绩和理科成绩。根据这些数
7
8
据，求以数学成绩为自变量的线性
2
5
4

回归方程。
3
9
8
注：这个例1在2-12节作为问题出现过。此外，在上一节也提到过。
解数学成绩和理科成绩分别记为x、y，则线性回归方程如下所示。
y=pxtq (p、q为常数）
第飞个学生的数学和理科成绩分别记为xh、Jk。于是，这名学生的实
际理科成绩，，与从回归分析得到的理科成绩的预测值pxk+g的误差e可
以如下表示（k=1,2,3）。
e:=Jk-(pXk+） （p、q为常数）
(1)
X
以上关系可以通过下表具体地表示出来
编
号
数学成绩x
理科成绩y
预测值
误差e
\
7
8
7p + q
8 - (7p + q)
2
5
4
5p + q
4 - (5p + q)
3
9
8
9p + q
8 - (9p + g)
根据式(1)，求得第k个学生的实际成绩与预测值的平方误差C,如
下所示。
C
一(e)
{J^-(px;+q)} (k=1、2、3)
(2)
注:系数
是为了方便进行导数计算，这个系数的不同不会影响结论。
不过，对于如何定义全部数据的误差，有各种各样的方法，其中最
标准、最简单的方法就是求平方误差的总和。利用式(2)，平方误差的总
深度学习的数学.indd 120
2019/7/2 10:47:033-4 神经网络的代价函数

121
和可以如下表示。这就是本书中的代价函数C.(2-11节）。
C,=C,+C,+C,
{8-(7p+q)}*+]
,{4-(5>+9)}?+]
{8-(9p+q)}
(3)
使得C.达到最小的，、g满足下式（2-12节）。
dCp
=-7{8-(7p+q)}-5{4-(5p+q)}-9{8-(9p+q)}=0
Op
8C+=-8-(7+)-4-(5>+9)}-8-(9+9)}=0
(4)
Oq
整理得
155p+21q=148、21p+3q=20
解这个联立方程组，可得p=1，g=- 1/3，于是回归方程为
y=x-
o
O
表示回归方程的回归直线。
Me.
.备注代价函数的差异
很多函数都可以作为代价函数。如前所述（3-3节)，神经网络的世界中
有名的代价函数是交叉熵。不论采用怎样的代价函数，神经网络学习的方法
与本例题都是相同的。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 121
2019/7/2 10:47:04122第3章神经网络的最优化
最优化的基础：代价函数的最小化
在例1的回归分析中，确定数学模型的参数是回归系数，和截距g.
它们通过将代价函数(3)最小化来确定。这个过程称为最优化。
相应地，确定神经网络的数学模型的参数是权重和偏置。重要的是，
确定权重和偏置的数学原理与回归分析是相同的，具体来说，就是使得
从神经网络得出的代价函数C.达到最小。最优化的思想可以形象地表示
为下图。
拧之→
2
代价函数C.
误差的平方和
最优化的思想：确定权重和偏置
的原理与回归分析相同。求使得
权重w
表示误差总和C┐的代价函数达到
偏置b
最小的最优的参数。
这里我们来对比一下例1的回归方程与3-2节的例题中的神经网络
(简略图),如下图所示。
回归方程
V=Dx+Q

输入
权重
偏置
h
输出
神经网络
回归分析和神经网络中
确定模型的方式是相同
的。神经网络的权重
偏置相当于回归方程的
输入层
隐藏层
输出层
回归系数、截距。
深度学习的数学.indd 122
2019/7/2 10:47:043-4 神经网络的代价函数
123
神经网络的代价函数
接下来需要求出神经网络的代价函数的具体式子。为了详细地展开
讨论，我们来考虑前面出现过的下述例2。
例2已知一一个用于识别通过4×3像素的图像读取的手写数字0、1的
神经网络（下图),求它的代价函数C.。学习数据为64张图像,像素
为单色二值。此外，学习数据的实例收录在附录A中
解例2的解答示例的神经网络如下图所示
层1
层2
层3
输入层）
（隐藏层）
（输出层）
图像
作为例2的解答
示例的神经网络
(简略图)(3-1
节、3-2节）。此
外，这里的神经
单元名使用的是
输出变量名。
具体来说，神经网络算出的预测值用输出层的神经单元的输出变量α
a；来表示。设这些输出变量对应的正解为1、12。于是，预测值与正解的
平方误差C可以如下表示（3-3节。
c=;(-d}"+(4;-4)}
(5)
以第飞张图像作为学习实例输入时，将平方误差C记为C,如下所示。
G=-{(i[]-=&*k])*+:[!]-&][},:-1,.,.,64)
(6)
式中的64来源于例2题意中的图像数目。此外，关于1,[k]、[k]、a[k]、
a[k]的表示方法，请参考3-1节。
注：式(5)、式(6)的系数在不同的文献中会有所差异，但最优化的结果是相同的。
深度学习的数学.indd 123
2019/7/2 10:47:04124
第3章 神经网络的最优化
式(6)的含义如下所示。
层1
层2
层3
（输入层
(隐藏层）
输出层）
正解
(t,[k]-a`[k])'
t,[k]
(t,[k]=a,[k])^
t,[k]
平方误差CA
={4[k]-q`[k])*+(t,[k]=a][k])3}
式（6)的含义。
对于全部学习数据，将式(6)加起来，就得到代价函数Cr
C,=C,+C,+w+C%a
(7)
式(7)的含义如下所示。
正解
t,[64]
(t,[64]-a;[64])"
$,[64]
儿廣
c=
一4[k]
(t[k]= q{[k])}
,[k
正解
G=y
<←,[]
(;[U]=-q`[u}3
(z,[1]=-az[])*
-4,[]
代价函数Cr=C+
式（7)的含义：代价函数的求法。关于全部数据的平方误差的总和就是代价函数。
此外，无法用权重和偏置的具体的式子来表示式（7)。
深度学习的数学.indd124
2019/7/2 10:47:053-4神经网络的代价函数125
以上就是代价函数的求法的全部内容。剩下的工作就是确定使得代
价函数C·达到最小的参数（权重和偏置)。因为确定参数的方法需要较
长篇幅来讲述，所以我们到下一章再详细讨论这个话题。
注：例2 的解答式(7)相当于前面回归分析的例1的式（3)。
参数的个数和数据的规模
下面我们来考察一下确定例2 的神经网络的模型的参数个数，并汇
总在下表中
层
含
义个数
注
隐藏层的神经单元的个数为3个，输人层的12个神经
权重
12 ×3
隐藏层
单元都有箭头指向隐藏层的各个神经单元
偏置
3
隐藏层的神经单元的个数为3个
输出层的神经单元的个数为2个，隐藏层的3个神经
权重
3×2
输出层
单元都有箭头指向输出层的各个神经单元
偏置
2
输出层的神经单元的个数为2个
根据上表，可以求得参数的总数，如下所示。
参数的总数=(12×3+3） +(3×2+2） =47
我们在2-12节考察过，如果数据的规模（即构成数据的元素个数）
小于确定数学模型的参数个数的话，就无法确定模型。因此在例2中
学习用的图像至少需要47张。
神经网络和回归分析的差异
虽然神经网络和回归分析确定模型的原理相同，但是它们也存在以
下差异。
i）
相比回归分析中使用的模型的参数，神经网络中使用的参数的
数目十分巨大。
深度学习的数学.indd 125
2019/7/2 10:47:05126第3章神经网络的最优化
(ii）线性回归分析中使用的函数为一次式，而神经网络中使用的函
数（激活函数)不是一次式。因此，在神经网络的情况下，代
价函数变得很复杂。
差异(i)反映在式(3)和式(7)中。回归分析中作为代价函数的式(3)
可以用参数的函数表示出来。而在神经网络的情况下，如式(7)所示，不
能用参数（权重和偏置）的式子将代价函数表示出来。非要写出来的话，
式子会变得无比复杂。
差异(ii)）也反映在式(3)和式(7)中。由于式（3)为简单的二次式，所
以可以简单地进行求导，容易求得式(4)的结果。然而，如果简单地对式
(7)进行求导，计算将变得非常麻烦。而且，由于引入了激活函数的导数，
所以得到的结果不会漂亮。
鉴于存在以上差异,相比回归分析，神经网络需要更强大的数学武
器，其中代表性的一种方法就是误差反向传播法。我们将在下一章具体
介绍。
用 Excel将代价函数最小化
幸运的是，对于例2这样简单的神经网络，用Excel等通用软件就
可以直接将代价函数式(7 最小化。即使不知道软件用了什么数学方法也
不要紧。在下一节，为了理解神经网络的最优化,也就是神经网络的学
习的含义，我们将试着用Excel将代价函数最小化，求出权重和偏置。
V
....备注激活函数用单位阶跃函数会如何呢?
我们在第1章考察过，作为神经网络的出发点的激活函数是单位阶跃函
数。然而，如果使用单位阶跃函数，本节所考察的代价函数的最小化方法就
不会被发现。因此，Sigmoid函数等可导函数成为了激活函数的主角。
深度学习的数学.indd 126
2019/7/2 10:47:053-5 用Excel体验神经网络127
3-5
用Excel体验神经网络
到目前为止，我们用同一个例题考察了神经网络。本节我们就用
Excel来确认这个神经网络是实际存在并且发挥作用的。对于例题那种程
度的简单神经网络，用Excel就可以直接确定权重和偏置。Excel是一个
便于直观地看清理论结构的优秀工具。下面我们就用Excel来实际体验一
下神经网络。
用 Excel 求权重和偏置
真正的神经网络是不可能用Excel来确定其权重和偏置的。然而，如
果是简单的神经网络，因为它的参数个数比较少，所以可以用Excel的标
准插件求解器简单地执行最优化操作。为了确认目前为止考察过的内容
本节我们利用下面的例题来实际地求出神经网络的权重和偏置，并体验
神经网络的行为。
例题对于3-1节～3-4节的例题中的神经网络，用Excel来确定它
的权重和偏置。学习数据的64张图像实例收录在附录A中
我们一步一步地进行讲解。
①读入学习用的图像数据
为了让神经网络进行学习，学习数据当然必不可少。如下图所示
我们将学习数据读人工作表。
由于图像是单色二值的，所以我们将图像灰色部分转换为1，白色部
分转换为0。将正解代人到变量1、左中，当输入图像的手写数字为0时
(6,12)=(1,0)，当数字为1时(t.6)=(0,1)(3-3节）
深度学习的数学.indd 127
2019/7/2 10:47:05128
第3章 神经网络的最优化
编号
IJKLMNO
编号
正解
我们将图像数据全部放在计算用的工作表上，如下图所示。
IJKLMN
PQR
TUV
TZTATE  JC TDJEJF JG
2设置权重和偏置的初始值
下面我们来设置权重和偏置的初始值。根据设置的不同，存在求解
器计算不收敛的情况，这时需要重新设置初始值。
IABCD
G
0和1的识别
隐藏层的权重和偏置
3.214
-4.562
-0.541
-1.076
ot- oo
-2.359
-1.071
-2.808
-1.382
3.99
2.218
5.730
5.310
-2.286
-4.044
-3.275
1.017
0.687
O
隐
-1.716
5.457
-1.821
初始设置使用的是服从
11
撼
5.361
0.303
0.940
12
-0.289
3.505
1.463
标准正态分布的正态分
13
-1.712
3.601
-0.774
-1.189
布随机数(2-1节）
14
-1.456
-0.836
-2.440
1.496
-0.193
3.128
5
16
0.423
-3.249
2.292
17
翰1
-3.575
4.446
5.666-5.578
18 2 -0.46 2.93089 -3.4101 -2.2691
3从第1张图像开始计算各个神经单元的加权输入、输出、平方误差
对于第1张图像，我们来计算各个神经单元的加权输人z的值、输出
值、平方误差C.
深度学习的数学.indd 128
2019/7/2 10:47:053-5 用Excel体验神经网络129
027
f =SUMXMY2(019:020,09:010)/2
计算时利用3-2节
的式(1)、式（2）以及
ABC
HIJKLMNO
0和1的识别
3-4节的式(6)。
隐藏层的权重和偏置
编号
ino
3.214
4.562
-0.541
-1.076
1.071
学习数10
11
7
玛
算出隐藏层神经单
391
据
10
8
5.310
州
-2.808
元的输入和输出
福
4.044
3.275
1.017
0.687
雲攥惠
1.716
5.457
1.821
乐
(3-2节式(1))
5.361
0.303
0.940
-2.979
力
12
0.289
3.505
1.463
1.828
13
-1.712
3.601
-0.74
-1.189
3
0.121
14
1.456
0.836
-2.40
0.048
算出输出层神经单
15
1.496
-0.193
3.128
a
0.862
16
0.423
-3.249
2.292
^5
元的加权输入和输
17
输1
-3.575
4.446
5.666
-5.578
\
1.083
出(3-2节式(2))
18
层2
-0.9406 2.93089 -3.4101 -2.2691
2
-1.597
19
9
0.747
算出平方误差C
20
C,
0.168
个
18.347
0.046
3-4节式(6)）
4对全部数据复制3中建立的函数
将处理第1张图像时建立的函数复制到所有图像实例，求出代价函
数C的值（3-4节式(7))
321
& =SUM(L21:JC21)
AB C
E
RCEIJKLMN O
PQRS
JDTETF
算出代价
0和1的识别
函数(3-4
隐藏层的权重和偏置
→口
节式(7)）
-1.076
orooS
小味草桥司
客美利明脑国多鲜
编号
*通
E:
旺
398
S
灵峰装学肉
复制到64张图像上
0.68]
凹糍
6
人
6.193
(
12
雲擦谜
质的脑国际医料|
2
福
5.372
名出3
A
1.712
3.60
.77
.189
壁课
0.12\
.833
1.456
0836
2.440
1.496
-0.193
县
0.048
02
3.128
Fs
0.862
0.9
日9与
0.423
3.249
2.292
0.530
08gy
川马3客家店
新1
-3.575
4.446
.66
-5.578
18
层
2.93089
3.410
婚
;
1.083
8.733
AR
a3
E
云
18.347
0.046
0.005(4
国包国国店
0.383
⑤利用求解器执行最优化
利用Excel的标准插件求解器算出代价函数C.的最小值。如下设置
单元格，然后运行求解器。
深度学习的数学.indd 129
2019/7/2 10:47:05130
第3章 神经网络的最优化
规划求解参数
求解器的设置（Excel
求解器在“数据”选
设置目标:O
$G$21
国
项中）。
到
〇最大值最小直1◎目标值:0
通过更改可变单元格：6）
设置代价函
$sD;:C,L,GSG$9,G$,$D$HT:$5$81
国
数的单元格
遵守约束:0
添加）
更改C）
删除
设置权重
全部重置）
和偏置的
装入/保存心）
单元格
口使无约束变里为非负数®
选择求解方法；但）
非线性GRG
]
选项〕
G21

=SUM(L21:JG21)
求解器算出
的权重和偏
ABC
HI
0和1的识别
置。注意代
隐藏层的权重和偏置
价函数C

4
U
编号
的值为0。
no
-2.064
3.362
-0.629
-0.987
-6.624
-0.328
-6.008
学习数
10
-2.576
12.225
-1.782
1 0
中国
-8.619
13.895
-1.377
111
2.093
0.148
1.095
0.841
t
-1.265
10.411
1.366
味
t,
日日9
雲瘵唢
9.774
0.339
0.981
-13.410
求解器
-0.286
5.199
1.272
18.484
的计算
-0.669
-10.110
0.683
隐
-3.588
3
17.343
结果
14
7.166
-4.143
13.077
藏
0.000
S
10.521
-0.361
4.721
1.000
16
0.795
-7.235
1.981
1.000
17

1
-14.252
3.699
9.334
6.644
12.279
18
层2
2.77813
7.15097 -17.322
-0.3271
输
2
-10.498
19
出
1.000
20
0.000
21
0.000
C
0.000
代价函数的计算结果
求解器的“可变单元格”计算出的值就是最优化之后的神经网络的
权重和偏置。比外，由于代价函数C.的值为0,所以这个神经网络完全
拟合了学习数据。
深度学习的数学.indd 130
2019/7/2 10:47:053-5 用Excel体验神经网络131
测试
我们来看看步骤5中得到的权重和偏置确定的神经网络是否正确，
输入手写数字0和1，看看是否能得到我们想要的解。
下图是输入右边的像素图像时所得的结果。这个神经网络
判定手写数字为0,与我们的直观感觉一致
021
加=IF(O19>=020,0,1)
测试用的工作表
IABC
FGHIJKLMNO
o和1的识别
的小口0-0小9日国鸟支国鸟与
隐藏层的权重和偏置
U
编号
-2.064
3.362
-0.629
-0.987
01 0
-6.624
0.328
-6.008
学习数据具口上
-2.576
12.225
-1.782
1 0
519
3.895
2.093
0.148
1.095
0.841
如果输出层的神
1.265
10.411
1.366
馋糖吧
9.774
0.339
经单元1的输出
0.981
-9.339
0.286
5.199
1.272
14.024
值比神经单元2
0.69
10.110
0.683
-3.588
15.347
7.166
4.143
13.07
00
的输出值大，则
10.521
-0.361
4.721
1.000
输入的图像被判
0.795
7.235
1.981
1000
定为数字0
-14.252
-3.699
9.334
6.644
12.278
!@
2
2.77813
7.15097 -17.322 -0.3271
-10.497
9R
1.000
0.000
21
数字的判定结果
MeY
备注 Excel求解器的局限性
Excel求解器对于少量的计算是非常方便的，然而对于神经网络的计算
则远远不够，因为参数的个数被限制为 200 多个。但在神经网络的世界中
权重和偏置等的个数成千上万，Excel无法处理这么多参数。
深度学习的数学.indd 131
2019/7/2 10:47:06图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 132
2019/7/2 10:47:06第
As
神经网络和误差反向传
播法
沿着最陡的坡度下山，就能以最少的步数到达山脚。梯度
下降法就是将这个原理应用在数学上的数值分析方法。为了求
出梯度的方向，需要进行求导，但在神经网络的世界中，导数
计算的计算量十分巨大。误差反向传播法就解决了这个难题
深度学习的数学.indd 133
2019/7/2 10:47:06134第4章神经网络和误差反向传播法
4-1
梯度下降法的回顾
神经网络的参数（权重和偏置)是通过将代价函数最小化来确定的
(3-4 节)。最小化的方法中最有名的就是我们在第2章考察过的梯度下降
法。本节我们将简单地复习一下梯度下降法，并据此来确认新方法的必
要性。
问题的回顾
求函数最小值的通用方法中，最有名的就是利用最小值条件。例如，
要求光滑函数z=f(x,y)的最小值，考虑以下方程就可以了（2-7节）。
0f=0,
0f=0
(1)

ax
Oy
我们在回归分析中使用了这个方法（2-12节）
在神经网络中，代价函数相当于式(1)的函数,权重和偏置相当于
变量x、y。如前所述，权重和偏置的总数十分庞大，而且代价函数中包
含了激活函数，所以求解像式(1)这样的方程是十分困难的。通过前面考
察过的以下例题，就可以知道其难度
例题已知一个用于识别通过4×3像素的图像读取的手写数字0、1的
神经网络，其代价函数为Cr。尝试进行求代价函数最小值的计算。学
习用的图像数据为64张图像，像素为单色二值
前面已经考察过，我们可以建立如下图所示的神经网络作为这个
例题的解
注：神经单元名使用的是输出变量名。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 134
2019/7/2 10:47:064-1 梯度下降法的回顾
0
135
层1
层2
层3
（输入层）
（隐藏层）
（输出层）
图像
作为例题的
解答示例的
神经网络的
简略图。
如下列出描述这个神经网络的关系式（3-2节)，其中激活函数为a(z)。
<隐藏层
z?=Wix,+wj2x;+:+Win2Xz+b?
z2=wx,+w2x& ++ w2px, +b?
z3 =wgixy wjgx +:+ "g121i, + b?
a?=a(z}) (i=1,2,3)
(2)
之输出层>
zi=wj,a? +w%,a}+w,aj+b?`
z`=w2,aj+w2,a3 + w;,a; +b?
a}=a(z;) (i=1,$2)
此外，神经网络计算出的预测值与学习数据的正解的平方误差C如
下所示（3-4节）。
C=
,{(1,-a{)`+(t,-a2)}
(3)
代价函数十分复杂
将图像实例代人到式(2)、式(3)中，得到代价函数C·(3-4节）。这
个函数是本章的主角。
深度学习的数学.indd 135
2019/7/2 10:47:06136
第4章神经网络和误差反向传播法
C-=C,+C,+:+C4
(4)
C.是将第飞张图像数据代入到平方误差的式(3)后得到的值，如下所示。
C:=
,{(t,[k]-a[k])}+(t,[k]-a,[k])}
(5)
上式中，变量附带的[k]表示从第k张图像实例得到的值（k=1, 2,3. …，
64)（3-1节）
学习数据
①

误差
正解

神经网络
C,=
{;{G[1]=a]}+({;[1]=d[)^}
T`,]=1
O
4[]=0
®
=,{(;[k]-aq`[k])+(t,[k]=a[*])}
4,[k]=1
C
t,[k]=0
t,[64]=0
@
,{(;[64]-<,[64])}*+(t,[64]-=`;[64])`}
算出预测值
1,[64]=1
↓
代价函数Cr=Gt…+C+…+Ca
神经网络和代价函数的关系。
代价函数的式(4)由式(5)构成，而式(5)由式(2)、式(3)构成。代
价函数C.是非常复杂的函数的集合体。此外，从式(2)可以知道，要确
定的参数（权重和偏置）共有47个。如果想要根据式(1)这样的方程来
确定参数的话，就需要47个方程，如下所示。
aC
owi
=0,
OCr
aC
=0,..
=0,
OWiz
0b
oC
(6)
=0,
dC,
=0,.
aC
=0,
ow
Owia
Ob
求解这些方程是极其困难的，于是梯度下降法应运而生
深度学习的数学.indd 136
2019/7/2 10:47:064-1 梯度下降法的回顾137
在神经网络中应用梯度下降法
把丽数图像看作斜坡，沿着坡度最陡的方向一步一步地下降，将这
个想法在数学上表示出来，就是梯度下降法，如下所示（2-10节）。
对于光滑函数（(y, x ,x，使各变量分别作微小的变化，如下
所示。
X1 + A1, X2 + A3, , n + Ax,
当以下关系式成立时，函数减小得最快。为正的微小常数。
(Ar,, As,, ., Ar,)=-
C .ne
(7)
OxOx.
ox
of of
此外，不
称
of
为丽数f的梯度
Ox'Oxs'
'Oxm
我们试着将梯度下降法的基本式（)应用到例题中。将C·作为式（4)
给出的代价函数，式（7)表示为如下形式。
(Aw., ., wi., -, , , b`, )
aC
ac
aC-
aC.
(8)
own
owin
db'
db?
w,、等表示式(2)中的权重和偏置。此外，正的常数，称为学习
率，这些我们已经考察过了
如果利用关系式(8)，用计算机实实在在地进行计算的话，寻找使C
取得最小值的权重和偏置这个目的看起来是可以达到的。用变量的当前
位置（w ….wii.…. b2，…，.bi.，加上式（8）左边求得的位移向量
得到新的位置：
(wi wj -, wi, w, -, 6 + , .., 6i t Ab`, .)
(9)
深度学习的数学.indd 137
2019/7/2 10:47:07138第4章神经网络和误差反向传播法
将它再一次代入式(8)进行计算，如此反复操作就可以了（2-10节）。与
求解方程组(6)相比，这是很大的进步。
实际的计算十分困难
然而，事情并没有这么简单。由于式(2)中有47个参数（权重和偏
置)，所以式(8)表示的梯度也有47个分量。而且，计算这个梯度的分量
是十分麻烦的。我们来试着实际地计算式(8)右边的梯度的其中一个分量。
囫1计算2G
owi
从第k张图像得到的输出与正解的平方误差C·由式(5)给出（k=1,
2,,.64 )。利用偏导数的链式法则（2-8节)，进行如下变形。
OC
0COaj[k] Oz}[k] Oa?[k] dz][k]
OwOa`[k] Ozj[k] Oa?[k] Oz}[k] Owi
(10)
0C, a,[k] 0z;[k] Oa}[k] @z,[k]
@a;[k] Oz;\[k] Oa?[k] Oz?[k] Owj
输入层
隐藏层（层2）
输出层（层3)
学习数据的
正解
代价函数
C
在式（10)中利用偏导数的链式法则时变量的关系。神经单元用3-1 节的符号表示。
将其代入式(4),得到
深度学习的数学.indd 138
2019/7/2 10:47:074-1 梯度下降法的回顾

139
oCr C ecC i. Ca
owi Owi owi
awi
OC,Oaf[1] dzj[1] 0a?[1] dz-[1]
oa;[1] 0z;[1 daq[1] dz2[1] owi
dC, Oa,[] 0z,1] da?[1] 0z31]l
da2[1] 0z,[1] 0a [1] 0z?[1 owi 
(11)
OC%a Ou`[64] 0z`[64] Oa?[64] 0z?[64
Oa;[64] 0z{[64] 0a?[64] 0z}[64] Ow,
0CaOa[64] 0z;[64] 0a?[64] 0z,[64]
6a;,[(64] 0z,[64] 0a;[64] 0z}[64] 0wi.
将式(2)代人这些导数项中进行计算，(虽然非常麻烦）就可以用权
重和偏置的式子表示偏导数的结果。以上就是例1的解答。
从例1我们可以知道，用具体的式子来求梯度分量是一件非常困难
的工作。虽然单个的计算比较简单，但是会被导数的复杂与繁多所压倒，
进入所谓“导数地狱”的世界。为了解决这个问题，人们研究出了误差
反向传播法。关于这个算法，我们将在下一节详细考察。
棘手的计算
导数地狱
梯度下降法的式(8）
误差反向传播法
达到目的！
梯度计算:先求导再求和
通过式（10)、式(11)的计算，我们了解到以下事实。
梯度分量是一个一个学习实例的简单的和。
也就是说，代价函数C-的偏导数是从各个学习实例得到的偏导数的和。
这是一个非常好的性质。一般地，为了求式（8)中的梯度分量，可以首先求
式(3)的平方误差C的偏导数，然后代人图像实例,最后对全体学习数据求
深度学习的数学.indd 139
2019/7/2 10:47:07140第4章神经网络和误差反向传播法
和即可。逻辑上需要64 次偏导数计算，这里仅用1次偏导数计算就完成了。
计算方法1
64次偏导数计算
oC
onn
将数据代入
cC
dCr
0C
ac
aCe
7
式(3)的C中
om.
owi Owin
Owii
OW,
:
计算方法2
ow,
OC&
从式391得到武司
aC
dCraCaC,
aC
代入数据
on; owm on
分+"+Ow
1次偏导数计算
推荐使用计算方法2，利用“图像实例的简单的和”这个性质，极大地减少了偏导
数计算的次数。
鉴于以上原因，此后的导数计算将不再标注图像编号k（k1.,2…,
64)。此外，只有在实际地计算梯度分量的值时，我们才会根据需要标注
清楚
Me
备注误差反向传播法的历史
误差反向传播法是1986年美国斯坦福大学的鲁梅尔哈特（Rumelhart)
等人命名的神经网络学习方法。该方法虽然看起来很难，但内容其实十分简
单，大家在读到之后的章节时就会发现这一点。
深度学习的数学.indd 140
2019/7/2 10:47:084-2 神经单元误差141
4-2
神经单元误差
梯度下降法对于寻找多变量函数的最小值的问题是有效的。然而在
神经网络的世界中，变量、参数和函数错综复杂，无法直接使用梯度下
降法，于是就出现了误差反向传播法。作为应用这个方法的准备工作
本节将引入一个名为神经单元误差的变量。
引入符号
误差反向传播法的特点是将繁杂的导数计算替换为数列的递推关系
式，而提供这些递推关系式的就是名为神经单元误差（error）的变量8
利用平方误差C(4-1节)，其定义如下所示。
6/=
0C
(1=2, 3.,.)
az!
(1)
注：希腊字母6读作 delta，相当于拉丁字母d。此外，虽然神经单元误差与4-1节的
式(3)的平方误差同为误差，但它们的含义却完全不一样。
下面我们来具体地考察神经单元误差
例题对于4-1节的开头的例题中的神经网络，考察神经单元误差8,
与平方误差C关于权重、偏置的偏导数的关系
注：本节中使用的变量和式子等的含义与4-1节相同。
这个例题的平方误差C如下所示（4-1节式(3)）
(一
={(,-aq)}+(t2=a;)}
(2)
aC
oC
例1根据定义，有子
6}=
z'

0z3
深度学习的数学.indd 141
2019/7/2 10:47:08142第4章神经网络和误差反向传播法
我们用下图来说明例1 中变量的关系。
隐藏层(层2)
输出层(层3）
a'-OC
0z
平方误差
B'.0C
az3
神经单元等遵循3-1节的表示方法。
用6/表示平方误差关于权重、偏置的偏导数
平方误差式(2)关于权重、偏置的偏导数与式(1)定义的，关系密
切。我们通过下面的例2、例3来弄清楚它们的关系。
囫2例题中，我们用/米表示C
owi
根据偏导数的链式法则，可以得到下式（上图）。
ecoc az
Owiozf Owi
(3)
这里我们利用4-1节的式(2)中的以下变量关系式。
z?=wji.x,+wj2x, +-+wi,Xqz +b?
根据这个关系式，可得
深度学习的数学.indd 142
2019/7/2 10:47:084-2 神经单元误差143
0z7
owi
=X
(4)
根据子的定义式(1)以及上述的式(3)、式(4)，可得
o
(5)
这就是例2 的解答。变量的关系如下图所示
oC_OC, azi
ow02 on.
输入层(层1）
oC,
z
平方误差
C
隐藏层(层2)
输出层(层3)
a
Ow,
例2的变量关系图。神经单元等遵循3-1节的表示方法。
此外，在输入层（层1）中，由于它的输入和输出相同，所以我们利
用变量名a的约定（3-1节)，将输入和输出表示如下。
ij=q
将上式与式(4)结合起来，式（5)可以如下表示。
own
@C.=8$4
(6
例3例题中，我们用〆来表示
oC
Ow
根据偏导数链式法则，可以得到下式（例1的图）。
aC aC ozi
(7)
OwiOzl Owin
这里我们利用4-1节的式(2)中的以下变量关系式。
z=wijai +wigaz+wias +bi
深度学习的数学.indd 143
2019/7/2 10:47:09144第4章神经网络和误差反向传播法
根据这个关系式，可得
Ozi
=a
(8)
Owp
根据舍的定义式(1)以及上述的式（、式(8)，可得
aC
Gw,
=8`a?
(9)
这就是例3的解答。变量的关系如下图所示。
aCaCaz
owiGz` Ow
平方误差
C
例3的变量关系图。
=a
aC
神经单元等遵循3-1
Owi,
Ozj`
节的表示方法。
通过同样的计算，平方误差式（2)关于2、孕的偏导数可以如下表示。
dC
aC az?
aC
=8,
oC azi
ab
0z? 0b3
abi
0z` 0b
=8`
(10)
aC
oC
&j'`"
0z?
平方误差
方误
h
b
dC
dh
aC
db
根据这些式(6)、式(9)和式(10),我们可以得到如下的一般公式。
aC
ac
=8a-,
=8}$(1=2, 3.,..)
(11)
Ow}n
ab,
这样，8/与平方误差C关于权重和偏置的偏导数就建立起了关系
深度学习的数学.indd 144
2019/7/2 10:47:094-2神经单元误差。
145
dC
问题1利用链式法则，请用子来表示
owz
解
aC

aC z?
owi. oa, ow.
=8a,=8x,
问题2利用链式法则，请用号来表示
oC
aC
ow,,
`
ob,
解C
oC Oz;
owj, o2z, ow,,
=8a},
dC

oC oz,
ob,az, 0b3
=83
注:推荐使用式(11)进行问题1、
、问题2的实际计算
8'与o"的关系十分重要
本节我们唐突地引人了科等符号进行计算，结果得到式(11)。从这
个式(11)，我们了解到以下重要事实：如果神经单元误差/能求出来
那么梯度下降法的计算所必需的平方误差式(2)的偏导数也能求出来。因
此，下面的目标就确定了，那就是计算出神经单元误差。
计算出
根据式(11，计算出平方
对于全体数据，将式(11）相加，从而
误差C的偏导数
计算出代价函数C,的梯度
我们将在下一节考察/的计算方法，即误差反向传播法。该方法根
据与"的关系来求司。
M.!金建)S的含义与神经单元误差
我们来考虑一下将6}=6C/0z/称为神经单元误差的含义。从这个定义
可知，/表示神经单元的加权输入；给平方误差带成的变化率。如果神经网
络符合数据,根据最小值条件,变化率应该为0。换言之，如果神经网络符
合数据，神经单元误差 8/也为0。那就是说，可以认为6；表示与符合数据
的理想状态的偏差。这个偏差表示为“误差”。
深度学习的数学.indd 145
2019/7/2 10:47:10146第4章神经网络和误差反向传播法
4-3
神经网络和误差反向传播法
梯度下降法为寻找多变量函数的最小值提供了一种实际可行的方法：
然而如4-1节考察的那样，在神经网络中不能直接使用梯度下降法。于
是就出现了误差反向传播法(BP法),具体来说,就是建立4-2节引入
的神经单元误差/的递推关系式，通过这些递推关系式来回避复杂的导
数计算。
通过递推关系式越过导数计算
误差反向传播法以梯度下降法为基础。我们用图来说明它的位置。
通过递推关系式

求偏导数的值
误差反向传播法
梯度下降法
最小值条件的方程
沿着梯度
aCr OCr aCp
8C"-=0, 8C1=o, BC.
Ox
ay'z
Ox
ov
0z
的方向一点一点地移动
代价函数的
最小化问题
误差反向传播法的位置，它是梯度下降法的一个分支。
误差反向传播法的特点是将繁杂的导数计算替换为数列的递推关系
式。关于递推关系式的内容我们在第2章已经考察过了，但是如果对数
列不熟悉，可能依然会觉得不顺手。不过请别担心，具体地思考一下就
会发现其实并不难。我们利用前面考察过的以下例题来分析其结构
深度学习的数学.indd 146
2019/7/2 10:47:104-3神经网络和误差反向传播法

147
例题已知一个用于识别通过4×3像素的图像读取的手写数字0、1的
神经网络，尝试对其代价函数应用误差反向传播法。其中，学习用的
数据为 64张图像，像素为单色二值
注：本节使用的符号和式子的含义与4-1节、4-2节相同
误差的复习
我们在4-2 节考察过，在误差反向传播法中，首先要定义如下变量
8/，该变量称为第1层第j个神经单元的误差。
"& C
az/
(1)
如果我们能得到神经单元误差，根据下式就可以得到作为梯度下
降法基础的平方误差的偏导数（4-2 节式（11)）。
aC
=8q;+,SC
=8}$(1=2,3....)
(2)
ow/,
Ob'
注：如4-2 节的式（6)所示，当1=2时，α以约定如下：a=x,（x,为输入层（即层1）
的第i个神经单元的输入输出变量)）
计算输出层的
如果我们能得到神经单元误差/，根据式(2)就可以得到梯度的分
量。那么，如何求/呢？这里我们利用数学中有名的数列递推关系式
(2-2节）的思想
数列为数的序列，其第一项称为首项，最后一项称为末项。有趣的
是，将式(1）定义的/看作数列时，可以简单地求出它的“末项”。
深度学习的数学.indd 147
2019/7/2 10:47:10148第4章神经网络和误差反向传播法
我们现在考虑的例题中，神经网络的层数为3。因此，我们试着计
算相当于数列{}末项的误差；（j=1,2)。这就是输出层的神经单元
误差。以a(z)为激活丽数，根据链式法则，有
E'=S
ac ocdi Care)
oz, oaj oz; oaj
(3)
这里我们利用了4-1 节的式(2)中的关系式。
像这样，如果给出平方误差C和激活函数，就可以具体地求出相当
于“末项”的输出层神经单元误差
以L作为输出层的层编号，可以将式(3)一般化，如下所示。
8=
aC
za'(z;)
(4)
a,
输出层
aC,
Sf-
z
平方误差
输出层第j个神经单元通过
C
上面的路径与平方误差C相
连接，就得到式(4)的左边
通过下面的经过a，的路径相
dC.
连接，就得到式(4)的右边。
=
-a'(z})
day
下面让我们试着实际地计算前面例题中的神经单元误差。
例1在例题中，我们来计算
根据4-1节的式(3)，平方误差C为
C'=;{(-2`?)-+(:,-4})}
(5)（4-1节式(3)）
深度学习的数学.indd 148
2019/7/2 10:47:114-3神经网络和误差反向传播法

149
因此，有
dC
da
=a`-t
(6)
将式(5)、式(6)代入式(3)，可得
o`=(a?`-t,)a'(z`)
(7)
这就是例1的解答。
问题1在例题中，尝试计算。激活函数为 Sigmoid函数o(z)。
1
解与推导式(7)同样，有
8}=(a`-t,)a'(z)
(8)
此外，根据题意，激活丽数为 Sigmoid函数o(z)（2-6节)，所以
有
S
a'(z})=o'(z;)=o(z){l-o(z})}
(9)
将式(9)代入式(8)，可得
8`=(a;=t,)c'(z;)=(a;-t,)o(z){1-o(z)}
中间层8的“反向”递推关系式
神经单元误差，具有非常好的性质。它通过简单的关系式与下一层
的神经单元误差"联系起来。比如，我们试着考察一下例题的子。
首先，根据偏导数链式法则（2-8节)，有
°=
dC
dC oz aai
oC az, a?
zf
ozi oaf ozi
oz, oa? ozi
(10)
深度学习的数学.indd 149
2019/7/2 10:47:11150
第4章神经网络和误差反向传播法
隐藏层（层2）
输出层（层3）
平方误差
C
式（10)中相关变量的
位置。在式（10)中利
用链式法则时，通过两
条路径到达平方误差
C。图中带有圆圈的地
方表示相关的变量。
我们来看看式(10)右边的各项。根据。、的定义式(1)，有
aC
=',C
(11)
Oz福
Q小
此外，根据z；与/（i=1,2,3）的关系（4-1节式(2))，有
ai
or=w,,
oai
0:) =i.
(12)
再利用激活函数a(z)，有
ar, =d'?)
0z2
(13)
将式（11)～式(13)代入式(10)，可得
8P=8}w,a'(z?)+6}w;,a'(z})
这样就得到了以下关系。
8"=(6`wi+8'w)a'(z)
(14)
深度学习的数学.indd 150
2019/7/2 10:47:124-3神经网络和误差反向传播法
151
隐藏层(层2)
输出层(层3)
代价函数
C
式（14)中相关变量的
位置。图中带有圆圈
的地方表示相关的变
量、参数。
对于号、学，也可以得到同样的关系式。我们加以总结，如下所示。
8}=(8w;+8wz)a'(z?) (i=1, 2, 3)
(15)
这样我们就得到了第2层的2与第3层的/的关系。这个关系式可
以如下推广为层1与下一层1+1的一般关系式。
8'={8{t"w#'+O$'w# +:++ 8*w#'}a'(z}
(16)
注:m为层1+ 1的神经单元的个数。1为2以上的整数。
中间层的：不求导也可以得到值
我们来观察式(15)。第3层的。、考的值可以通过式(7、式()得
到。因此，利用式（15)，不用进行麻烦的导数计算，也可以求出第2层的
学的值，这就是误差反向传播法。只要求出输出层的神经单元误差，其
他的神经单元误差就不需要进行偏导数计算!
深度学习的数学.indd 151
2019/7/2 10:47:12152
第4章神经网络和误差反向传播法
层2
层3
误差反向传播法的结构。如果求
出第3层的,那么第2层的8
也可以简单地求出。
式(16）一般是按照层编号从高到低的方向来确定值的。这与第2章
考察过的数列的递推关系式的想法相反，这就是反向传播中“反向”的
由来。
层2
层3
层I
式(16)的含义。可以说它表示
“反向”递推式的关系。
问题2在例题中，尝试用、号表示。激活丽数为 Sigmoid 丽数d。
解根据式(15),有
8}=(6wj2+6"w;)'(z2)
(17)
此外，根据题意，激活丽数为Sigmoid 函数o(z)（2-6节)，所以
有
a'(z})=c'(z;)=o(z}){1-o(z})}
将它代入式（17)，可得
8,=(6*w,+8jw;,)c(z}){1-o(z})}
如上所示，在问题2的解答过程中，导数计算一个也没有！
深度学习的数学.indd 152
2019/7/2 10:47:134-4 用Excel体验神经网络的误差反向传播法153
用Excel体验神经网络的误券
4-4
反向传播法
利用4-3节考察的误差反向传播法，我们试着用Excel实际计算代价
丽数的最小值。如前所述，Excel非常适合用来直观地观察计算的结构。
注：我们用平方误差的总和作为代价函数，用Sigmoid函数作为激活函数。不过，如果各
层的激活函数相同，则这里考察的逻辑也可以直接应用到一般的神经网络的计算中。
首先，我们总结一下前面学习过的误差反向传播法的算法
①准备好学习数据。
2进行权重和偏置的初始设置。
输入各个神经单元的权重和偏置的初始值。初始值通常使用随机
数。此外，设置适当的小的正数作为学习率n。
3计算出神经单元的输出值以及平方误差C.
计算出加权输入z、激活函数的值α（4-1节式（2))。此外，计算出
平方误差C(4-1节式(3)。
·根据误差反向传播法，计算出各层的神经单元误差。
利用4-3 节的式(3)计算出输出层的神经单元误差6。接着，利用
4-3节的式（16)计算出隐藏层的神经单元误差。
⑤根据神经单元误差计算平方误差C的偏导数。
利用④中计算出的神经单元误差à以及4-2节的式（11)，计算平方
误差C关于权重和偏置的偏导数。
⑥计算出代价函数C.和它的梯度VC.
将3～⑤的结果对全部数据相加，求出代价函数C，和它的梯度VC。
⑦根据6中计算出的梯度更新权重和偏置的值。
利用梯度下降法更新权重和偏置（4-1节式(9)）
⑧反复进行3～⑦的操作。
反复进行③～⑦的计算，直到判定代价函数C.的值充分小为止。
深度学习的数学.indd 153
2019/7/2 10:47:13154第4章神经网络和误差反向传播法
以上就是利用误差反向传播法确定神经网络的权重和偏置的算法
①读入图像数据
②设置初始值一》③计算出C等
图像64
⑧返回到③
图像2
图像
输出层
良
(隐藏层
⑦利用梯度下降法更新
输入屋
权重和偏置
4用误差反向传播法计算出心
A
6根据8计算C的偏导数一
>国计算出代价函数C和
它的梯度VCT
3的处理称为前向传播，④～5的处理称为反向传播。误差反向传播法是将这二者组
合起来的计算方法。
用 Excel确定神经网络
我们试着用 Excel确认上述算法。本节利用前面考察过的以下例题作
为具体例子。
例题对于4-1～4-3节考察过的神经网络，利用误差反向传播法确定
它的权重和偏置。学习数据的64 张图像实例收录在附录A中
我们已经在4-1～4-3节考察过这个神经网络的变量的具体关系式
在Excel中，只要将其用式子或函数来表示就可以了。下面，我们来考虑
具体的计算方法。
①读入图像
要确定神经网络，就必须根据学习数据确定权重和偏置（如前所述
深度学习的数学.indd 154
2019/7/2 10:47:134-4 用Excel体验神经网络的误差反向传播法155
这称为“学习”)。为此，我们在Excel工作表中读人64 张手写数字的图
像和正解。
注：学习数据的64 张图像实例收录在附录A中。
由于是单色二值图像，所
以像素信息用0和1表示
编号
学习率
集一贩
妇等定
6
二角
关于正解的含义，
请参考3-3节
在从单元格L3 开始的范围内按顺序分配6×4的块状区域，读入64张图像的数据和正
解。在各个6x4的块状区域中，在左上的4×3 区域设置图像的值，右下的2×1区域
设置正解变量1n、12的值。
2进行权重和偏置的初始设置
权重和偏置一开始是未知的，需要由我们求出。然而如果没有“出
发点”就无法展开讨论。因此我们利用正态分布随机数（2-1节）来设置
作为“出发点”的初始值。此外，我们还要设置学习率，为适当的小的
正数。
注：学习率，的设置大多需要反复试错。同样地，对于权重和偏置的初始值，为了取得
好的结果，也可能需要多次变更设置。
深度学习的数学.indd 155
2019/7/2 10:47:13156
第4章神经网络和误差反向传播法
AABCD
EFG
数字0和1的识别
2
3
4
5
or-
设置学习率
8
9
心
b
10
0.490 .348-0073 -0.189
日92
0.837
-0.071 -3.617
0.536
-0.023 -1.717
权重和偏置的初始值。
-1.456
-0.556 0.852
利用正态分布随机数
14
0.442
-0.53710080.526
o
坚境啡
1.072
0.733
0.823
9二
-0.453 -0.014 -0.027
.876
2.305
18
0.654
1.389
1.246 -1.169
19
0.057
-0.183-0.743
在从单元格D10开始
20
-0.4610.331 0.449
的范围内分配权重（w
21
-1.296 1.569
-0.471
22
1 0.388 0.803 0.029 -1.438
和偏置（b）的区域，
23
20.025 -0.79 1.553 -1.379
合计由47个参数构成
3计算出神经单元的输出值以及平方误差C
对于第1张图像，我们根据权重和偏置来求各个神经单元的加权输
人、激活函数的值和平方误差C。
L17
C
&[=(087-L15)^2+(0$8-M15)^2)/2
ABCI
LmN
数字0和1的识别
编号
平方误差C
4
输
5
坐习高中
g
02
正解
9
1次
10
0.490 0.348 0.073-0.185
1
0.837
-0.071
1.997
12
536
-0.023
0.004
0.880
0.175
9
-1.456
-0.556
0.852
z
0.105
0.442
-0.537
1.008
利用4-1节式(2)
15
楼
1.072
-0.7330.823
0.3270.141
16
会
0453 -0.014
-0.02
a'zb
0.220
0.121
17
4根据误差反向传播法计算各层的神经单元误差8
首先计算输出层的神经单元误差6；(4-3节式(3)),然后根据“反
向”递推式计算（4-3节式(15)）。
深度学习的数学.indd 156
2019/7/2 10:47:134-4 用Excel体验神经网络的误差反向传播法157
5根据神经单元误差计算平方误差C的偏导数
根据④中求出的6，计算平方误差C关于权重和偏置的偏导数（4-2
节式(11)）。
L25
vO
f& {=L$3:N$6*L21}
IABCD
MNO
1
数字0和1的识别
编号
3
一国小国日9鸟文出
乐一
*粉
层
正解
海商海部
1
次
-0.071
-3.617
5.465
1.997
-1.552
-0.536
-0.023
-1.717
.56
-056 0.852
鸿打制咖粤
贸营
0.004-0.880 0.175
a'e},)
0.004 0105 0:144
0.442
-0.537 1008 0.526
雪德咀
1.072
-0.73 0823
管
z
-0.724
-1.804
o
0327 0.141
9上空
0.453
-0.014 -0.027
a'(z';)
0.2200.121
-0.427
1.876
2.303
236
0.654
1.389
1.246
-1.169
BCa'
-0.673 0.141
2R
0.057
-0.183
-0.743
算
*管
4利用4-3节式(3)
-0.148 017
-0.461
 0.331
0.49
7
隐萤
Ew83
-0.057 -0.1335 0.022
司刘民高闵品与器民同石科网茗网品
-1296
1.569
-0.471
0.0-0.014 0.003
熊银区
0.388
0.803 0.029
-1.438
002
2091535-1379
④利用4-3节式(15)
梯度
中国专车有专家售区
路
CrOn
粤鲜斯
C-Gb
BCoN
BChob
0.082
0.01
000
00000
平
m
0.0
00
中国通鱼脑
雲撕咀
0.121
仁账淝鼋堰吵粲
0.0
0.0
0.014
-0.014
-0413

噩掸山
2
国博 售膳售部
W
-0.396
o

-0.932
国鲜业
00
0.0
0.003
37
1
0.542
-1.939 -0.135
-2.491
输出

-001
-0.13
-0.026-0.148
38-2 -1.18 -2.106 -1.028 -3.263
0.000 0.015 0.0030.017
⑤利用4-2节式(11）
6计算出代价函数C和它的梯度VC,
到目前为止，我们取第1张图像作为学习数据的代表进行了计算
我们的目标是对全部数据执行同样的计算，并将结果加起来。因此，这
里需要对全部64张图像的学习数据复制目前建立的工作表。
深度学习的数学.indd 157
2019/7/2 10:47:13158
第4章
神经网络和误差反向传播法
HⅠJ
LMNO
EJFG
编号
64
每<咀
国郫
冬
复制64份图
正解
n
)
像数据
1
次
舛打愀眯堙
管
-5.465
1.997-1.552
-0.090
2.483
-1.392
2,
0.004
0.880
0.175
0.477
0.923
0.199
a'kz)
0.004
0.105 0.144
0.249
0.071
0.159
管
3
-0.724
-1.804
-0.505
-1.88
aj
0.327
0.141
0.376
0.143
a'{e',)
0.220
0.121
0.235
0.123
0.2368
0.4377
-0.673
0.376
哈
管
8C/a
0.141
-0.857
3
开0
-0148 0017
0.088
-0.105
隐藏
Xw8
-0.057
302
0.032
0.154
-0.161
令
0.000
-0.014 0003
0.008
0.011
-0.026
8Clow
aClab
8C/86
aC/aw
aC!8b
00
0.000
0.000
0.008
00
0008
00000
0.008
0.0
0.0
卧仅账棵母堰吓燕
00
000
0.000
000
X%
08
国居务
噩掸咀
-0.014
E
01600
-0.014
-0.014
-0.014
0.009
0.0
011
0.011
-0.014
0.011
0.00
-0.014
o.01
00
-0.014
00
201
0.003
国务
600
0.003
0.003
0.003
-0.028
0.00
0.026
0.0
-0.026
0g
00
-0026
CS
00
AR
-0026
5200
0.003
0.003
000
菜
中国
输出
-0.001
-0.13 -0.026
-0.148
0.075
0.042
0.081
0.018
0.088
0.000 0.015 0.003 0.017
-0.106 -.050-0079 -0021-0.105
将从单元格L10到038的64个块状区域复制到右边。
将64份数据复制完毕后，对平方误差C以及⑤中求出的平方误差C
的偏导数进行加总，这样就得到了代价函数C，和它的梯度VC(下图）。
深度学习的数学.indd 158
2019/7/2 10:47:134-4 用Excel体验神经网络的误差反向传播法
159
G39
+
f&=SUM(L17:JG17)
ABCD
E
GEIT
利用4-1
数字0和1的识别
编号
节式(4)
一→山6一国·日日斗日：99日国日司支斗购为将为团利R同两科网x奖制与
输
位
举习车”
食
模
式
02
正解
t1
1
0.490 0.348 0.073 -0.185
0.837
-0.071
.3617
-5.465 1.997 -1.552
-0.536
-0.023
-1.71
隐藏
z
涎
层
0.004 0.880.175
1.456
-0.56
0.852
打制画粤
a'(z';)
0.004
0.105 0.144
0.442
-0.537
1.008
0.526
坚换虾
粉迎
-0.724 -1.804
1.072
-0.733
0.823
味
0.327
0.141
-0.453 -0.014 -0.027
a'(r)0.2200.121
-0.427
1.876
2.30
0.2368
0.654
1.389 1246
-1.169
aC/8a?
-0.673 0.141
0.057
-0.183
-0.743
舛五
输出
层
-0.148 0.017
-0.461 0.331
0.449
隐藏
3o 0057 01302
1.296
1.569
-0.47
0.000-0.0140.003
0.388 0.803 0.029 -1.438
0.025
-0.79 1.553 -1.379
梯
BC1/G
8C-v6b
aC/aN
aC/ab
0.040
0.068
00
0.000
0.000
6000
0015 0103
N
0.093
0.080
#
02
E
S
坚搬山
-0.019
0.193
0.121
照
0.481
-0.34
m
-0.295
m
福
串
0187
雲撼咀
0.01
-0.014
AE
-0.287
0.396
淝鼋堰叩蠡
#
国通鲜店
0.014
-0.491 -0.794
0.037
-.0.92
-004
0.01
0.003
0.003
0.003
0.003
0.016-0.959
-0.086
0.003
0.000
0.003
0.016-0.922 -0.129
0300003
-0117 -0.889-0.163
0.003
0.0030:003
1
0342-19-0135 2491
输出
-0.001-0.13
-0.026 -0.148
三
-1.158
层
0.000.015 0.003 0.017
39
1次
C+20.255]]
利用4-1节式(8)
Ma
备注矩阵的和、差与 Excel
Excel 中没有计算矩阵的和、差以及常数倍的函数，这是因为 Excel不需
要使用函数。例如，想要计算A1:B3 与 P1:03存储的两个矩阵的和，并将结
果存储到X1:Y3 时，选定区域X1:Y3，并将A1:B3和 P1:03用“+”号联结，
同时按Ctrl + Shift键就可以了（即进行数组计算）。利用这种方法，计算式
的输入就变简单了。
深度学习的数学.indd 159
2019/7/2 10:47:13160第4章神经网络和误差反向传播法
7根据6中求出的梯度，更新权重和偏置的值
利用梯度下降法的基本式（4-1节式(8))，求出新的权重和偏置的
值。用Excel实现时，在上一个表的下面制作下图所示的表，并在其中嵌
入用于更新的公式（4-1 节式（9)）。
AABCD
1
数字0和1的识别
g
一no
华盛餐
芯好图雨日33
梯度
aC"i6n
GCrab
0.040
0.068-0.022
-0.015 0.103 -0.013
0.014
0.093
-0.02
0.0000.080-0.011
-019 0.193 -0295 0.21
0.481
0.589
-0.394
西剁8高剁网
膳啡
-0.534 0.645
-0.413
287
0.187
-0.396
4-1节式(8)
0.491
-0.7940
0.037
-0.932
0.016 -0.959 -0.086
0.016 -0.922 -0.129
-0.117
-0.889 -0.163
37
家
1
0.542
-1.939 -0.135 -2.491
窝界导日
0
2
-1.158 -2.106 -1.028 -3.263
20.255
利用4-1节式(9）
98子9
0.482 0.35077-0.201
0.840 -0.091
-3.614
0.533
-0.041 -1.713
-1.456
-0.572
0.85
46
0.446
-0.575
1.0670.501
1.168
-0.851
0.902
盼
93
城
0.346
-0.143 0.055
50
1.239 --0.982
利用梯度下降法的关系式（4-1
51
0.054
0.009 -0.726
节式(8)、式(9))，计算出新
9
0.464
0.516 0.475
的权重和偏置。在第1次计
53
-1.273
1.746 -0.438
54
10.280 1.191 0.056 -0.940
算②～6的块状区域后空出
20.257 -0.369 1.759 -0.727
1行，开始第2次计算。
更新后的权重和偏置的值
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 160
2019/7/2 10:47:134-4 用Excel体验神经网络的误差反向传播法
161
8反复进行3～7的操作
利用⑦中计算出的新的权重w和偏置b，再次进行从③开始的处理。
AABCD
EF
GHIJ
MNO
数字0和1的识别
编号
3
输
回城
学习车的
6
0.2
食
复制③～⑦
正解
中计算出的
公
2
次
9g
0.420335 0.077 -0201
相应部分
0.840 -0.091
-3.614
z?
5002.461 -0.846
44
0.533
-0.041
-1.713
中营

0.004
0.921
0300
45
1.456 -0.572
0.855
a'e;}
0.004
0.072
0.210
48
0.446
47
雪擦山
1.168
-0.851
0.902
场
0.176
0.538
0544
0.369
99品
-0.346
-0.143
0.055
a'(z`)
0.248
0.233
0.369
1.838
0.1720
0.753 -1.230
-0.982
馆
-0.456
0.369
51
0.720
0.11
0.086
88
0.464
0.516
0.475
隐藏
Ew}
-0010-0.166 0.145
-1.273 1.746 -0.438
0.000-0.012 0.030
54
1.191 0.056-0.940
88
田e
8C!:b
68
0.0
0.00
化
B品
E
BCian
0.00
=
0.0
G
0.321
0.222
0.012
0.012
0.012
62
雪剪
0.554
0.751
-0.44
QK
隐
0.012
0.0
63
06AS
0.847
食
0.012
0.00
0.013
照
64
名集山
0.012
0.012
f
0.030
0.030
030
3 5 8
0.030
0.000.030
0.549 -1.019 0.48
0.030
0.000.030
0.113-0.620.291
06860030
69
0.928 -0.494 0.773 -0.136
00-0104-0.034-0.1
7o82
9091-036-0.88-1314
管
0.000 0.079 0.026 0.086
2次C14428
将这样计算出的从41行到71行的1个块状区域复制50份到下面，
进行50次计算。
注：这里的50 并没有特别的含义，只是作为一个齐整的数字使用。
深度学习的数学.indd 161
2019/7/2 10:47:14162

第4章神经网络和误差反向传播法
ABCDYFGNIJ
-Nn
数字0和l的识别
编号
4
输
拉
5
<吧
式
6
租
B
正解
计算出的权
B8n
50次
1578
0.441 0.791 -0.114 0.250
重和偏置值
1579
0.859 0.301 3.69
-4.794 4.516 -4.68
150
-0.484 0.316 -1.939
管警
z^,
1581
1.432
-0.120
0.653
遮打
0.008 0.989 0.017
a'(z?)
0.008
0.0110.017
152
0.631
-2.044
1.517
-0.374
3.143-
-3.243
1583
警德咀
1.847
-1.631
1.157
K删画
管
0.959
0.038
蒸蜜蜜
0.781 -1.377
0.7
a'(z})
0.0400.036
0.479
0.73 -0.828
C
0.0016
-0.106
0.452-0.110
0.039
-0.041
0.038
1587
-1.047
0.869 -1.302
营
8C!a?
53
-0.0020.001
A68
-1.343
1.311 -0.516
舛打·
Ew8'
0.004 -0.009 0.010
1589
1.745
3.221
-161
噜营
0.000000
159
3
-130 3576 -3.040-0332
1591
52
L1.445-2-408
4.055 -0.941
1592
梯度
8C"yfow
BCr!6Bb
eCian
aClab
19
-0.007
0.009
0.0000
0.00
0.000
1594
0.00
00
0.004
000000
国酱酱园
00
-0
0.01
00
00
0.0
0.006
-08
0.011
0.00
0.000
0.0
0.033
0.061
-0.018
0.038
0.000
国蜜蜜星园
雪膳
00
0.00
0.005
-0.016 0.011
长咪排
隐
0.00
-0.021
-0.010.001
藏
00
0.00000
0.058
-0.047
H品年
层
0.000
0000
0.043
0.00
0.000
00
0.027
-033 028-0.018
0.00000
星蜀面
0.019 0.003 0.013
教
0.000
0.000 0.000
0.013
0.006
0.020
0.000
0.000
0.000
0.024
4-0.032 0.027
0.000
0.000
0.000
1605
0.028
-0.0700.077
-0.033
输出
0.00-0.00
0.000
-0.002
1606
- 2 00210062 0.066 0.010
层
0000100
1607
50次
0.245
50次计算后的代价函数的值
将41行到71行的1个块状区域复制 50份到下面。
通过以上步骤，计算就结东了。我们来看看代价丽数.的值。
代价函数Cr=0.245
学习数据由64张图像构成，每张图像平均为0.004。根据平方误差
的式子（4-1节式(3)),每张图像的最大误差为1,因此可以说以上步骤
算出的是一个很好的结果。
此外，通过跟踪50次代价函数的计算结果，可以直观地理解梯度下
降法的含义。代价函数C.的值随着每次迭代面减小，这从逻辑上看也是
深度学习的数学.indd 162
2019/7/2 10:47:144-4 用Excel体验神经网络的误差反向传播法
163
理所当然的，而梯度下降法的优点就是减小的速度最快。
次数
CT
次数
CT
次数
CT
次数
CT
次数
CT
1
20.255
11
2.214
21
1.030
31
0.580
41
0.353
2
14.428
12
2.000
22
0.968
32
0.550
42
0.338
3
12.243
13
1.827
23
0.911
33
0.522
43
0.323
4
9.924
14
1.680
24
0.859
34
0.496
44
0.310
5
7.581
15
1.553
25
0.810
35
0.471
45
0.297
6
5.679
16
1.441
26
0.765
36
0.448
46
0.285
7
4.332
17
1.342
27
0.723
37
0.426
47
0.274
8
3.451
18
1.252
28
0.683
38
0.406
48
0.264
9
2.868
19
1.171
29
0.647
39
0.387
49
0.254
10
2.488
20
1.097
30
0.612
40
0.370
50
0.245
备注关系式的矩阵表示
用矩阵表示式子，有时会使式子变简洁。例如，4-3节的式(4)和式（15)
可以用矩阵简洁地如下表示。
oc
q)
af
a'(zi)
式（4)
司
oC
a'(z)
aa}
8
W1
w21
a'(z?)
g3)
式（15）：
号
wi2
w52
O

a'(z3}
(*)
号
Wi3
nzs
号
a'(z})
这里的の表示Hadamard 乘积（2-5节。
用计算机进行计算时，将式(*）改写为以下形式会比较方便。
8
rt
a'(z[)
wi2
05
茶
wi}司
·)
a'(z)
w31
w3,
wz人8
a'(z})

深度学习的数学.indd 163
2019/7/2 10:47:14164第4章神经网络和误差反向传播法
用新的数字来测试
我们创建的神经网络是用于识别手写数字0、1的。因此
我们用新的手写数字来确认它能否正确地识别数字0、1。
下面的Excel工作表是利用第8步得到的权重和偏置，输
人右边的数字图像并处理的例子
L16
f&=IF(L4)M14,0,1)
HNowisotoo
数字0和1的识别测试
编号
位模式
0
图像的
0
位模式
、b的信
日日国名
0.441
0.791
-0.114
0.250
隐藏层
子
-5.001
1.795
-7.073
0.859
0.301
-3.699
000
aj
0.007
0.858
0.001
-0.484
0.316
-1.939
00
o'(z?)
0.007
0.1220001
-1.432
-0.120
0.653
0.00
输出层
zi
723
-2.993
寸国与黑司司
鹦撅咄
0.631
-2.044
1.517
-0.374
0.938
0.048
1.847
-1.631
1.15
00
0.781
-1377
0.777
0.0
判定
0.479
0.573
-0.828
00
-0.106
0.452
0110
0039
1.047
0.869
-1.302
.00
输出层第2个神经单元的输
1.343
1.311
-0.516
0.00
出值比第1个神经单元的
-1.745
3.221
-1.611
00
22
-1.308
3.576
.040
-0.332
小，因此判断为0
崇
2
1.445
-2.408
4.055
-0.941
⑧中得到的权重和偏置的值
利用8中得到的权重和偏置，对新的数据计算输出层的神经单元输出。如果第2个神
经单元的输出值比第1个神单元的小，就判断为0。
人来判断的话可能会认为“那也许是0”，而神经网络也
判断为“”
下面的工作表是输入右边所示的数字图像时的例子。人来
判断的话会认为“那也许是1”，而神经网络也判断为“1”
深度学习的数学.indd 164
2019/7/2 10:47:144-4 用Excel体验神经网络的误差反向传播法
165
IABC
GHIJKPQR
数字0和1的识别测试
o一口
编号
位模式
0
图像的
0
位模式
9
、b的位
同中斗华当上国R村N
0.441
0.791
-0.114
0.250
隐藏层
交
1.424 -3.337 5.782
0.859
0.301
-3.69
00
0.806
0.0340.997
-0.484
0.316
1.939
0.00
o'(z?)0.156
0.033
0.003
1.432
-0.120
00
输出层
184
邈撩呶
0.631
-2044
-0.374
0.0130.985
1.847
-1.631
1.157
0.781
-.377
0.77

判定1
0.479
0.573
-0.828
-0.106
0.452
0.110
0S
-1.047
0.869
1.302
-1.343
1.11
0.516
O
输出层第2个神经单元的输
1.745
3.221
1.611
00
和
1
.308
.576
.040
-0.332
出值比第1个神经单元的
23
景
2
1.445 -2.408
4.055 -0.941
大，因此判断为1
利用8中得到的权重和偏置，对新的数据计算输出层的神经单元输出。如果第2个神
经单元的输出值比第1个神单元的大就判断为1。
Ma
备注矩阵计算与Excel函数
如前面的备注所述，在神经网络的计算中，利用矩阵常常会使式子变得
简单，计算也变得更容易。因此，在使用Excel时，建议也利用这个特点。
Excel中有以下矩阵函数，它们在神经网络的计算中经常被用到。
MMULT
计算矩阵的乘积
TRANSPOSE
计算矩阵的转置
Excel 中没有计算Hadamard乘积的函数，但我们可以将矩阵作为数组
来简单地处理。
深度学习的数学.indd 165
2019/7/2 10:47:14深度学习的数学.indd 166
2019/7/2 10:47:14.5
深度学习和卷积神经网络
深度学习是人工智能的一种实现方法。本章我们将考察作
为深度学习的代表的卷积神经网络的数学结构。
深度学习的数学.indd 167
2019/7/2 10:47:14168第5章深度学习和卷积神经网络
5-1
小恶魔来讲解卷积神经网络的结构
深度学习是重叠了很多层的隐藏层(中间层）的神经网络。这样的
神经网络使隐藏层具有一定的结构，从而更加有效地进行学习。本节我
们就来考察一下近年来备受关注的卷积神经网络的设计思想
使网络具有结构
卷积神经网络是当下正流行的话题，尚且难以总结一般理论。这里，
我们利用一个最简单的例题来考察一下卷积神经网络的思想。如下所示，
这个例题是由前面考察过的例题整理而成的，它虽然简单，但是能够很
好地帮助我们理解卷积神经网络的结构。
例题建立一个卷积神经网络，用来识别通过6×6像素的图像读取的
手写数字1、2、3。图像的像素为单色二值。
首先，我们来介绍一下作为这个例题的解答的卷积神经网络的示例，
如下页的图所示。
图中用圆圈将变量名圈起来的就是神经单元，从这个图中我们可以
了解到卷积神经网络的特点。隐藏层由多个具有结构的层组成。具体来
说，隐藏层是多个由卷积层和池化层构成的层组成的。它不仅“深”，而
且含有内置的结构。
注：卷积层的英文是 convolution layer。这里展示的是最原始的卷积神经网络，实际的
网络更为复杂。
深度学习的数学.indd 168
2019/7/2 10:47:15-1 小恶魔来讲解卷积神经网络的结构169
风修
作为例题的解答示例的卷积神经网络的
下标为像素编号)
图。本章我们将讲解这个神经网络。图中
的神经单元名是后面将要考察的输出变量
名（5-3节。
卷积层
池化层
aB)aDaB)aD
输出层
DBBaB
DO(
思路
人们是如何想到这样的结构的呢?如果我们了解了卷积神经网络的
思路，就可以在各种领域中进行应用。这里我们也同样请第1章登场的
恶魔”来讲解。
在1-5节考察过的神经网络中，住在隐藏层的恶魔具有各自偏好的
模式。恶魔对自己偏好的模式做出反应，输出层接收这些信息，从而使
神经网络进行模式识别成为可能。
本节登场的恶魔与之前的恶魔性格稍微有点不同。虽然他们的共同
点都是具有自己偏好的模式，但是相比第3章登场的恶魔坐着一动不动
这里的恶魔是活跃的，他们会积极地从图像中找出偏好的模式，我们称
之为小恶魔。
为了让这些小恶魔能够活动，我们为其提供工作场所，那就是由卷
积层与池化层构成的隐藏子层。我们为每个小恶魔准备一个隐藏子层作
为工作场所。
深度学习的数学.indd 169
2019/7/2 10:47:15170
第5章深度学习和卷积神经网络
提供能让小恶魔活
动的工作场所(外
侧的框。这个隐
藏子层的编号为1。
活跃的小恶魔积极地扫描图像,检查图像中是否含有自己偏好的模
式。如果图像中含有较多偏好的模式，小恶魔就很兴奋，反之就不兴奋
此外，由于偏好的模式的大小比整个图像小，所以兴奋度被记录在多个
神经单元中。
小恶魔扫描图像数据，根据检测到的偏好模式的多少而产生兴奋，其兴奋度会被记录在
卷积层的神经单元中。神经单元名中F1 的F为Filter的首字母，1为隐藏子层的编号。
深度学习的数学.indd 170
2019/7/2 10:47:155-1小恶魔来讲解卷积神经网络的结构
171
(ail(al
aH)(a)(
(ay）(ass)
它们形成卷积层
一个子层
注：
般用子
于扫描的过滤器的大小是5×5
这里为了使结果变简单，我们使用如图所
示的3×3的大小。
活跃的小恶魔进一步整理自己的兴奋度，将兴奋度集中起来，整理
后的兴在度形成了池化层。
卷积层的
池化层的
子层
aHaBaB)aH
池化层的建立。小恶魔将扫描结果
中田临型
的兴奋度（a，等）进一步集中起来，
整理为池化层的神经单元。池化层中
浓缩了小恶魔所偏好的模式的信息。
神经单元名中P1的P为Pooling的
首字母，1为隐藏子层的编号。
深度学习的数学.indd 171
2019/7/2 10:47:16172第5章深度学习和卷积神经网络
因此，池化层的神经单元中浓缩了作为考察对象的图像中包含了多
少小恶魔所偏好的模式这一信息。
1-5节介绍的恶魔每人有一个偏好模式，本节的小恶魔每人也只有一
个偏好模式。因此，要识别数字1、2、3，就需要让多个小恶魔登场。这
里我们比较随意地假定有3个小恶魔。输出层将这3个小恶魔的报告组
合起来，得出整个神经网络的判定结果。
与第1章相同,输出层里也住着3个输出恶魔，这是为了对手写数
字1、2、3分别产生较大反应。
dBdD
输出层
BBBB
HaDaDaD
输出层将3个小恶
中临
魔的报告进行汇总。
ddde
为了分别对手写数
字1、2、3产生较
®®DD
特征映射
大反应，需要3个
输出恶魔。
以上就是利用小恶魔来解答例题的方法。卷积神经网络就是按照这
·思路建立神经网络的卷积层和池化层的。
如前所述，第1章登场的隐藏层的恶魔是静态的,他们只是观察数
据然后做出反应。而本章的小恶魔是动态的，他们会积极地扫描图像
整理兴奋度并向上一层报告。由于这些小恶魇的性格特点,卷积神经网
络产生了我们前面学习过的简单神经网络所没有的优点。
D对于复杂的模式识别问题，也可以用简洁的网络来处理。
国整体而言，因为神经单元的数量少了，所以计算比较轻松。
而卷积神经网络之所以在各种领域备受瞩目，也是得益于这样的性质
此外，目前为止我们的讨论都是假定小恶魔住在神经网络的隐藏层
和所有的科学理论一样，模型是否正确，取决于用它做出的预测是否能
深度学习的数学.indd 172
2019/7/2 10:47:165-1 小恶魔来讲解卷积神经网络的结构173
够很好地解释现实情况。众所周知,现在卷积神经网络已经有了一些显
著的成果，例如能够识别出 YouTube 上的猫的图像等
那么，神经网络是如何实现这里考察的小恶魔的活动的呢？我们将
在下一节考察数学上的实现方法。
小恶魔的人数
在前面的说明中，登场的小恶魔一共有3人。这里的人数不是预先
确定的。如果我们预估用5个模式能够区分图像，那么就需要有5个小
恶魔。这样一来，我们就应当准备好5个由卷积层和池化层形成的隐藏
子层。
卷积层
池化层
火丫
输出层
港
s
复杂的文字图像
如果图像变得复杂，卷积层和池化层形成的隐藏子层的数目也相应地增加。对于需
要多少个隐藏子层等问题，往往需要进行反复试错来确定。
而且，在识别猫的图像的情况下，隐藏层的结构本身也需要变得更
复杂。这就是深度学习的设计人员可以大展身手的地方。
深度学习的数学.indd 173
2019/7/2 10:47:16174

第5章深度学习和卷积神经网络
将小恶魔的工作翻译为卷积神
5-2
经网络的语言
我们在5-1节考察了卷积神经网络的思路。通过设想能够寻找偏
好模式的活跃的小恶魔，从而理解了卷积神经网络的设计思想。本节
我们来看看如何将小恶魔的工作替换为数学计算。这里考察的例题与
上一节相同。
例题建立一个神经网络，用来识别通过6×6像素的图像读取的手写
数字1、2、3。图像像素为单色二值
从数学角度来考察小恶魔的工作
X
下面我们从数学角度来考察5-1节的小恶魔的工作。首先我们请小
恶魔S登场。假定这个小恶魔S喜欢如下的模式S
小恶魔S偏好的模式S
(S为Slash（/）的首字母。）
注：模式的大小通常为5×5。这里为了使结果变简单，我们使用图中所示的小的3×3
模式。
假设下面的图像“2”就是要考察的图像。我们将手写数字2作为它
的正解。
图像“2”。从数学角度考察小恶魔
处理这个图像的过程。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 174
2019/7/2 10:47:165-2将小恶魔的工作翻译为卷积神经网络的语言175
小恶魔S首先将偏好的模式S作为过滤器对图像进行扫描。我们
将这个过滤器命名为过滤器S。接下来、我们实际用过滤器、扫描救个图
像“”
相似度=2
相似度=1
相似度=0
相似度=
相似度=0
相似度=0
相似度=1
相似度=2

相似度=0
相似度=0
相似度=3
相似度=0
萧社


相似度=0
相似度=3
相似度=1
相似度=1
各个图像下面的“相似度”表示过滤器S的灰色格子部分与扫描图
像块的灰色格子部分吻合的地方的个数。这个值越大，就说明越符合小
恶魔偏好的模式。
注：这个相似度是像素为单色二值（即0与1）时的情况，关于更一般的模式的相似
度，我们将在附录C 中讨论
我们将这个相似度汇总一下，如右表所示。这就是
210
0012
根据过滤器S得到的卷积（convolution）的结果，称头
0030
特征映射（feature map）
0311
深度学习的数学.indd 175
2019/7/2 10:47:16176第5章深度学习和卷积神经网络
这就是在5-1节登场的小恶魔执行的扫描结果。
注：这样的过滤器的计算称为卷积。
卷积层中的神经单元将这一卷积的结果作为输入信息。各神经单元
将对应的卷积的值加上特征映射固有的偏置作为加权输入（下图）。
2+b
加权输入
卷积的结果
1+b
CmBBm
XO1
0012
0+b
母母母
0
O
030
H((zB)(Gm
031X
卷积层的神经单元的
HDDm
加权输入。请注意偏
置6是相同的。此外，
小恶魔S在编号1的
1+b
隐藏子层中活动。
卷积层的各个神经单元通过激活丽数来处理加权输人，并将处理结
果作为神经单元的输出。这样卷积层的处理就完成了
zB)cBeB)(zB
aaBaBD(n
EH)(zH)(z")(
激活函数
a#)aBaB
Z2
a, )(an
卷积层神经单
zH)z")z")(
aH)(as
元通过激活函
数将加权输入
卷积层神经单元的加权输入
卷积层神经单元的输出
转换为输出。
通过池化进行信息压缩
这个例题的卷积层神经单元数目比较少，因此可以简单地列出输出
值。不过，在实际图像的情况下，卷积层神经单元的数目是十分庞大的
因此，就像5-1节提到的那样，需要进行信息压缩操作，然后将压缩结
果放进池化层的神经单元中。
深度学习的数学.indd 176
2019/7/2 10:47:175-2 将小恶魔的工作翻译为卷积神经网络的语言177
压缩的方法十分简单，只需要将卷积层神经单元划分为不重叠的
2 ×2的区域，然后在各个区域中计算出代表值即可。本书中我们使用最
有名的信息压缩方法最大池化（max pooling)，具体来说就是将划分好的
各区域的最大值提取出来。
最大值
最大值
最大值
最大值
最大池化的结果。
池化层的输入和输
卷积层
池化层
出为相同的值。
注：池化操作通常在2×2的区域中进行，但也并非一定这样。
这样一来
·张图像的信息就被集中在紧凑的神经单元集合中了。
图像“2"
过滤器S
卷积层
池化层
我们通过下面的例子来复习上述计算过程。
例利用前面所示的图像“2”和过滤器S来实际计算卷积层和池化层中神
经单元的输入输出值。设特征映射的偏置为-1(阈值为1)，激活丽数
为Sigmoid函数。
按照下图的顺序进行计算，如下所示
深度学习的数学.indd 177
2019/7/2 10:47:17178
第5章深度学习和卷积神经网络
卷积层（加权输入
相似度
2101
0012
0030
0311
J

（特征映射）
卷积层（输出
图像“2"
过滤器S
0.73 0.50 0.27 0.50
池化层
0.27 0.27 0.50 0.73
0.73 0.73
0.27 0.27 0.88 0.27
0.88 0.88
0.27 0.88 0.50 0.50
注：池化层的输入和输出相同。为了简化，神经单元也用方框表示。
问题与前面的例一样，计算用过滤器S处
理右边的图像“1”和“3”时卷积层和池化
层中神经单元的输入输出值。
图像“1”
图像“3
解按照与例相同的步骤，可以得到如下图所示的结果
图像“1'
相似度
卷积层（加权输入
021
1
O
O
000
0（偏置为-1）
0
1
7

0
1
11
（特征映射）
卷积层(输出
过滤器S
0.27 0.73 0.50 0.50
池化层
0.50 0.50 0.50 0.50
0.73 0.50
0.27 0.50 0.50|0.50

0.50 0.50
0.27 0.50 0.50 0.50
图像“3'
相似度
卷积层(加权输入）
萧中社江社
2
0
0-1
0
0
0
(偏置为-17
2
0
12
（特征映射）
卷积层(输出
过滤器S
0.73 0.50 0.27 0.50
池化层
0.27 0.27 0.50 0.73
0.73 0.73
0.27 0.50 0.73 0.27

0.50 0.73
0.50 0.50 0.50 0.73
这里为了简化
申经单元
；也用方框表示
深度学习的数学.indd 178
2019/7/2 10:47:185-2将小恶魔的工作翻译为卷积神经网络的语言179
从上面的例和问题可以了解到，数字“2”的图像的池化结果是由比
数字“1”“3”的图像的池化结果大的值构成的。如果池化层神经单元的
输出值较大，就表示原始图像中包含较多的过滤器S的模式。由此可知。
过滤器S对手写数字“2”的检测发挥了作用。此外，做出判断的是输出
层。与我们在第1～4章考察的神经网络一样,输出层将上一层（池化
层）的信息组合起来，并根据这些信息得出整个网络的判断结论
如上所示，我们将5-1节考察的小恶魔的工作通过数学思路表现了
出来。然而，只有数学思路还不能进行计算。在下一节，为了能够实际
进行计算，我们会将这些思路用数学式子表示出来。
Mer
备注体验真正的深度学习
本书中用到的深度学习的具体例子只是为了帮助读者了解深度学习的结
构，并不足以实际应用。读者在通过本书了解了深度学习的结构后，就可以
尝试下表所示的服务平台的试用版。
服务名称
说明
TensorFlow
由谷歌提供。可以免费地体验真正的深度学习
Azure
微软的云计算服务平台。也可以体验深度学习
由国际商业机器公司（IBM）提供。从传统的机器学习出发，
Watson
之后也引入了深度学习的技术
Amazon Machine
由亚马逊提供。特点是提供向导，可以按部就班地创建机器
Learning
学习模型
深度学习的数学.indd 179
2019/7/2 10:47:18180
第5章深度学习和卷积神经网络
5-3
卷积神经网络的变量关系式
要确定一个卷积神经网络,就必须具体地确定过滤器以及权重、偏
置。为此，我们需要用数学式来表示这些参数之间的关系。
确认各层的含义以及变量名、参数名
与前面一样，我们通过下面的例题进行讨论。
例题建立一个神经网络，用来识别通过6×6像素的图像读取的手写
数字1、2、3。图像像素为单色二值。学习数据为96张图像。
在5-1 节中，作为解答示例，我们展示了如下的卷积神经网络的图。
图像
(下标为像素编号）
卷积层
/-og
输入层
aPapa)a
池化层
输出层
卷积
30
卷积
我们把确定这个卷积神经网络所需的变量、参数的符号及其含义汇
总在下表中。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 18
2019/7/2 10:47:185-3卷积神经网络的变量关系式181
位置
符号
含义
输入层
5s
神经单元中输入的图像像素（i行j列)的值。与输出值相同
用于建立第k个特征映射的过滤器的i行i列的值。这里为
过滤器
wj^
了简化，考虑3×3大小的过滤器（通常采用5×5大小）
z
卷积层第k个子层的i行；列的神经单元的加权输入
卷积层第k个子层的i行i列的神经单元的偏置。注意这些
bF*
卷积层
偏置在各特征映射中是相同的
卷积层第k个子层的i行j列的神经单元的输出（激活函数
a*
的值）
池化层第k个子层的i行j列的神经单元的输人。通常是前
z]j*
一层输出值的非线性函数值
池化层
池化层第k个子层的i行j列的神经单元的输出。与输入值
g4
z*一致
从池化层第k个子层的i行j列的神经单元指向输出层第n
w2,
个神经单元的箭头的权重
输出层
z,
输出层第n个神经单元的加权输入
b,
输出层第n个神经单元的偏置
a,
输出层第n个神经单元的输出（激活函数的值）
正解为1时，1,=1,12=0,1,=0
学习数据
tn
正解为2时，t,=0, t,=1,$t,=0
正解为3时，1,=0,12=0,t,=1
这些变量和参数的位置关系如下图所示。
注：图中的标记遵循3-1节的约定。
深度学习的数学.indd 181
2019/7/2 10:47:18182第5章深度学习和卷积神经网络
变量一览
XnXnxDrn
1s)(e
Xran(r2)Xx2z)(2n)(X2z)(X2
输入层
(下标为像素编号
过滤器1
wil wil wl
CararMrairara
过滤器3
w? wDwB
wlwl wl
过滤器2
w泉w影w赋
wl wlwl
wiw器wB
wlwB wB
wf ww
好w器w品
池化层
输出层
花方误差
{({.-4)}`+(1,-42}+(1,-a")}
正解
12
与神经网络不同的是，卷积神经网络中考虑的参数增加了过滤器这
个新的成分
深度学习的数学.indd 182
2019/7/2 10:47:195-3卷积神经网络的变量关系式183
接下来，我们会逐层考察在今后的计算中所需的参数和变量的关
系式。虽然有些内容与5-1节、5-2节有所重复，但我们要从数学上-
股化的角度来弄清楚。请读者对照着5-1节、5-2节阅读,并尝试理解数
学式
输入层
在例题中，输入数据是6×6像素的图像。这些像素值是直接代人到
输入层的神经单元中的。这里我们用x表示所读人的图像的i行j列位置
的像素数据，并把这个符号用在输人层的变量名和神经单元名中
X1X2.
图像
输入层的变量名
16
CnrnArnyrnriBrn
Km)rnYJa)rzE2
mDDrDrD
Eiaycin
X3JEsDYEsDJKE5)<Ss)xso
输入层神经单元的位
置与对应图像的像素
（下标为像素编号）
EaYEYJEaYESY
位置一致。
在输入层的神经单元中，输入值和输出值相同。如果将输入层i行
j列的神经单元的输出表示为a，那么以下关系式成立（a的上标Ⅰ为
Input的首字母）。
a}, = x,
过滤器和卷积层
就像5-1节5-2节所考察的那样，小恶魔通过3×3大小的过滤器
来扫描图像。现在，我们准备3种过滤器(5-1节）。此外，由于过滤器
的数值是通过对学习数据进行学习而确定的，所以它们是模型的参数。
如下图所示，这些值表示为w,w （k=1,2,3)
深度学习的数学.indd 183
2019/7/2 10:47:19184
第5章深度学习和卷积神经网络
过滤器1
过滤器2
过滤器3
wil
wB
w鸮
wi
w劈
w?
wi
wi w
21
w5l
w罪
w男
w浆
w浆
w3
w3
构成过滤器的数
值是模型的参
w引
w'2
w y
w翳
w影
w景
w京
w拉
w 33
数。此外，F为
Filter的首字母。
注:过滤器也称为核（kernel）。
过滤器的大小通常为5×5。本书中为简单起见，使用更为紧凑的
3×3大小。此外，也不是必须准备3种过滤器。当计算结果与数据不
致时，我们需要更改这个数目
现在，我们利用这些过滤器进行卷积处理（5-2节）。例如，将输入
层从左上角开始的3×3区域与过滤器1的对应分量相乘，得到下面的卷
积值c刊（c为convolution的首字母）。
c=wflx,+wx2+wx,++w,
这就是5-2 节中称为“相似度”的值。
wWfxn+w,x2+.+wx,.
V'31
V32
1f
过滤器1
输入层
依次滑动过滤器，用同样的方式计算求得卷积值明, l。这
样一来，我们就得到了使用过滤器1的卷积的结果。另外，关于这些数
值的数学含义，请参照附录C
-般地，使用过滤器飞的卷积的结果可以如下表示。这里的i、j
为输人层中与过滤器对应的区域的起始行列编号（i、j为4以下的自
然数）。
深度学习的数学.indd 184
2019/7/2 10:47:195-3卷积神经网络的变量关系式185
c*=wx,+wx+j$+n++5*+n2+2
这样得到的数值集合就形成特征映射
我们给这些卷积值加上一个不依赖于i、j的数b*
zJ^ =w%x,+wxyu+mf*ij+2++%f*x+2n2+b"
(1)
输入层(图像数据）
过滤器k
wf
偏置
加权输入
+bA
E
把输入层的相应区域
与过滤器的对应分量
相乘，再加上偏置
就得到式(1)。
考虑以z·作为加权输入的神经单元，这种神经单元的集合形成卷积
层的一个子层。b*为卷积层共同的偏置。
激活函数为 a(z)，对于加权输人z”，神经单元的输出a萨可以如下
表示。
aj^=a(zj^)
(2)
深度学习的数学.indd 185
2019/7/2 10:47:20186
第5章深度学习和卷积神经网络
式（1)、式(2)中变量和参数的关系。图
中是构成卷积层第1个子层的神经单
元集合。各个神经单元的加权输入为
式（1),输出为式(2。请注意它们具有
共同的偏置。此外，这个图的标记遵循
3-1节的约定。
问题试着写出卷积层第1个子层的1行2列的神经单元的加权输人
z1与输出，的式子。激活函数为 Sigmoid 丽数。
解z"=wlx,twx,+wxa+w.x, wxz, +wx2a
+wgx+,+wg&s+b"
a3
1+exp(一z号)
输入层（图像数据
过滤器1
wFR
偏置
问题中变量和参数的关系。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd180
2019/7/2 10:47:205-3卷积神经网络的变量关系式187
池化层
卷积神经网络中设置有用于压缩卷积层信息的池化层。在5-1节、
5-2节中，我们把2×2个神经单元压缩为1个神经单元，这些压缩后的
神经单元的集合就形成了池化层。
将4个神经单元
压缩为1个
卷积层
将4个神经单元
→压缩为1个
池化层的信息压缩方法。
这里考察的卷积层由
4×4个神经单元构成
将4个神经单元
将4个神经单元
分别使其中的2×2个为
压缩为1个4
压缩为1个
1组压缩为1个。
很多文献也和这里一样，将特征映射的2×2个神经单元压缩为1个
神经单元。通过执行一次池化操作，特征映射的神经单元数目就缩减到
了原先的四分之一。
注:如前所述，并非必须是2×2大小
压缩的方法有很多种，比如较为有名的最大池化法，例如像下图这
样，从4个神经单元的输出au、42、a1、a5,中选出最大值作为代表
最大池化
Max(an, a, an, a21)
4 21
a21
例1下图左边为卷积层的输出值，右边为最大池化的结果。
卷积层的输出
0.27 0.12 0.05 0.12
最大池化
0.05 0.05 0.12 0.27
0.27 0.27
0.05 0.05 0.50 0.05
0.50 0.50
0.05 0.50 0.12 0.12
从神经网络的观点来看，池化层也是神经单元的集合。不过，从计
算方法可知，这些神经单元在数学上是非常简单的。通常的神经单元是
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 187
2019/7/2 10:47:20188第5章深度学习和卷积神经网络
从前一层的神经单元接收加权输入，而池化层的神经单元不存在权重和
偏置的概念，也就是不具有模型参数。
此外，由于输入和输出是相同的值，所以也不存在激活函数的概念
从数学上来说，激活函数a(x)可以认为是恒等函数a(x)=x。这个特性与
输出层的神经单元相似。
卷积层1
卷积层2
卷积层3

池化层由神经
单元构成,但
它们与通常的
神经单元不同。
以上讨论的池化层的性质可以用式子如下表示。这里，k为池化层的
子层编号,i、j为整数，取值必须使得它们指定的参数有意义
z^ =Max(a5t-12-
, a2f 21-1,42%2)
(3)
a`=zJ
z|=Max(a|,q[', a, a)
池化层1
激活函数a(x)=x
卷积层(第1个子层
池化层的神经单元所接收的输入中没有权重和偏置的概念。激活函数可以认为是
ax)=x，例如al=zl
深度学习的数学.indd 188
2019/7/2 10:47:215-3卷积神经网络的变量关系式189
输出层
为了识别手写数字1、2、3,我们在输出层中准备了3个神经单元。
与第3章和第4章中一样，它们接收来自下一层（池化层）的所有神
经单元的箭头（即全连接)。这样就可以综合地考察池化层的神经单元的
信息。
池化层
CaB
输出层
特征映射1

特征映射2
池化层的神经单元和输出
层的神经单元是全连接。
图中的神经单元名使用了
特征映射3
输出变量名(共有12×3
a男)a男
个箭头，这里省略）。
我们将这个图用式子来表示。输出层第n个神经单元（n=1,2,3）的加
权输入可以如下表示。
zn =wi, a" + w2pai +..+ w2,a? + w2pa5+
(4)
+w,4i`+wpa"+w+b,
这里，系数为输出层第n个神经单元给池化层神经单元的输出P
(k=1,2,3;i=1,2;i=1,2）分配的权重，b。为输出层第n个神经单元的
偏置。
例2我们来具体地写出z：的式子。
z=w,a| + w,a' +.+w2l,a? +w2l,a? +:
+ wgl,a`+w?|2aj'`+..+b%
上式中变量和参数的关系如下图所示、
深度学习的数学.indd 189
2019/7/2 10:47:21190

第5章

深度学习和卷积神经网络
卷积层
池化层
子层1
田D
1p9
输出层
V1
a9
af)(a
vo
子层2
,O1
a
a.
为了写出z。而用到的
子层3
变量和参数的关系的简
asi)(as
略图。
我们来考虑输出层神经单元的输出，它们形成了整个卷积神经网络
的输出。输出层第n个神经单元的输出值为α，，激活函数为a(z)，则
a,=a(z;)
(5)
a,(n=1,2,3）中最大值的下标n就是我们要判定的数字。
求代价函数C1
现在我们考虑的神经网络中，输出层神经单元的3个输出为
a、a、a，对应的学习数据的正解分别记为、公(参考3-3节，
以及本节开头的表)。于是，平方误差C可以如下表示。
C=
,{(t,-a8)}+(t,-a9)?+(1,-49)?}
(6)
注：系数一是为了简洁地进行导数计算，不同的文献可能会使用不同的系数，这个系
数对结论没有影响。此外，关于平方误差，请参考2-12节、3-4节。
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 190
2019/7/2 10:47:225-3卷积神经网络的变量关系式191
输出层
平方误差
正解
;{(-q9
t1
本书采用平方误差作为误差函
数。正解变量,在读取数字图
(t,-a9
t2
像“1”时为1，在其他情况下
+
为0；正解变量友在读取数字图像
(1, -ag93}
“2”时为1，在其他情况下为0;
t3
正解变量在读取数字图像“3”
时为1，在其他情况下为0。
将输入第k个学习图像时的平方误差的值记为C，如下所示。
C=
,{t,[Kk]-q%[k])^+(t,[k]-a[k])}+(t,[k]-q[k])}
注：关于变量中附带的[k]，请参考3-1节。
全体学习数据的平方误差的总和就是代价丽数C·。因此，我们现在
考虑的神经网络的代价函数C.可以如下求出。
Cp=C,+C,+:.+Cg
(7)
注:96为例题中学习图像的数目。
这样我们就得到了作为计算的主角的代价函数C。数学上的目标是
求出使代价函数C·达到最小的参数，即求出使代价函数C·达到最小的
权重和偏置，以及卷积神经网络特有的过滤器的分量，如下图所示。
拧之→
5
数学上的目标是实现参数的最
代价函数Cr=
优化。确定权重、偏置以及过
误差的平方和
滤器分量的原理与回归分析相
权重w
同。使代价函数C┐达到最小
偏置b
的参数是最优参数，而这样的
过滤器分量
思路就是最优化。
深度学习的数学.indd 191
2019/7/2 10:47:22192第5章深度学习和卷积神经网络
通过计算确认模型的有效性
前面我们已经多次提到过，要确认目前建立的卷积神经网络是否有
助于数据分析，就要实际使用这个模型进行计算，看得到的结果是否能
够很好地解释给定的数据
下一节，为了确认前面讨论的内容，我们将使用Excel的最优化工具
(求解器)，直接将代价函数最小化，并求出使函数达到最小时的过滤器、
权重和偏置
Me.n
(备注L2池化
本节我们采用了最大池化作为池化的方法。最大池化具体来说就是使用
对象区域的最大值作为代表值的信息压缩方法。除了最大池化之外，还有其
他池化方法，如下所示。
名称
说明
最大池化
使用对象区域的最大值作为代表值的压缩方法
平均池化
使用对象区域的平均值作为代表值的压缩方法
例如，对于4个神经单元的输出值a、4、4、a，使用、(a?+ag+a}+aj
L2池化
作为代表值的压缩方法
深度学习的数学.indd 192
2019/7/2 10:47:225-4 用Excel体验卷积神经网络193
5-4
用 Exce 体验卷积神经网络
本节我们通过 Excel来确认一下前面考察的卷积神经网络能否实际地
发挥作用。
用 Excel确定卷积神经网络
对于下面的例题，我们用Excel来确定卷积神经网络
例题对于在5-3 节的例题中考察的卷积神经网络，确定它的过滤器、
权重和偏置。学习数据的96张图像实例收录在附录B中
注：代价函数使用平方误差C的总和，激活函数使用Sicmoid函数，池化方法使用最
大池化。
接下来，我们逐个步骤地进行计算
①读入学习用的图像数据
为了让卷积神经网络进行学习，需要用到学习数据。因此，我们将
图像读人到工作表中，如下图所示。
第1张图像
编号
1 2 3 4 5 6
电
金
正解
如上图所示，将数字图像保存在工作表中。
由于图像是单色二值图像，我们将图像的灰色部分设置为1，白色部
分设置为0，将正解代入到变量1、12、t；中。学习图像为数字1时t,=1.
深度学习的数学.indd 193
2019/7/2 10:47:22194第5章深度学习和卷积神经网络
图像为数字2时6=1,图像为数字3时：=1,其他情况下变量值为0.
此外，学习用的图像数据全部存放在计算用的工作表中，如下图所示。
将学习数据汇总并读入到计算用的工作表中
1TK
PQVT
VKVL
VMvNvO
编号
输入
位模式
正解
注：如图中P列、Q列所示，图像最右边的2列像素缩小了显示宽度
②设置参数的初始值
我们来设置过滤器、权重和偏置的初始值。这里使用了标准正态分
布随机数（2-1节）。
注：当求解器的执行结果不收敛时，要修改初始值。
福
0.454
.
都总部吧
F
时制净号
1.274
0.201
国尚电动商印
1.323
FM bias
-3363
3.176
山经鸡国际系科商养居店
-1.739
2/P1
A
福
13
过滤器、权重和偏置的
初始值。利用正态分布
随机数来输入
菁
z°g P1
第国封活别电厨停车
0
m
语国季制调外区建科经外革客理鸡鲜料间
-1.818
深度学习的数学.indd 194
2019/7/2 10:47:225-4 用Excel体验卷积神经网络195
③从第1张图像开始计算各种变量的值
根据当前的过滤器、权重和偏置,对于第1张图像,计算出各个神
经单元的加权输人值、输出值和平方误差C的值。计算时利用5-3节的
关系式。
卷积层神经单元的输入
卷积层神经单元的
(5-3节式(1))
输出(5-3节式(2))
M46
vC
fr
=SUIMXMYZ(L9:L1,M43:M45)/2
ABCD
F
GHIJK
MNOPQ
数字1、2、3的识别
编号
〃丝华吕阖烨阁斗厨岗对习戏将囡女购将网岗科将斯奖利岗司房司带9子99
n寸
A
位模式
F1
-0.454
0.358卷积层z”1
-3.363
票
-1277
-3.994
-5.316
4296
-.398
1.64
ien寸
-3.363
酒居
F2
照
m
-3994
-5.316
青营鸡章鲜店
039
福
-2.054
W
m
z
1
-3.176
2.382
1.900
F3
ton
福
国
zR
en一
E
-1.739
月润腾制店
FMbias
-3.363
-3.176
中国美国明日
-4828
中吧
-1.739
Nnt
-1.739
酒源司羽国零国用日
安福
O
5.11
-0.276
5.11
O9L
照
-1.739
E:
3.109
-0.94
1
0福
N
0.003
粥
9
-0.782
辆打咀霉鄏圖暴螺晦
聚国日E
中国牌环店
3.680
周具  鸡|
Nnt
月售摩国承店
S
0.05

a"
中酒店
-n
o
m
月居通鲜息|
国建兽鲜育
门月烤舒客摩家店
o
国区湖国热鲜行
国色国绿都超车
-NnT
S
0.149
0.003
0.043
S
0.043
E
0.006
卓商国酒店
池化层P1
S
S
1678
O层bis
.060
-2.746-1.818
P3
-e-e-
/输出层
国誉膳居客
售客建图牌胸汤所
输出层神经单元的输
入(5-3节式(4））
-em
离
福国州强美目司
输出层神经单
元的输出(5-3
0.038
节式(5)）
池化层神经单元的输入输出(5-3节式(3)）
算出平方误差(5-3节式(6)）
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 195
2019/7/2 10:47:22196第5章深度学习和卷积神经网络
4复制步骤3中建立的各个函数到所有数据中
将处理第1张图像时嵌人的各个函数复制到其他图像数据中,直到
最后一个图像实例（该例题中为第96张）为止
HI了K
J
VKLWWVo
一会
编号
96
位模式
ooe
0
0
s
正解工
三
台
卷积层2
H
-3.363
-3.994
-5.316
-4.296
-5.403
-1.645
-4.826
-9.052
nn-Nnt
-3.363
-3.994
-5.316
-4.296
-3.817
-6.304
-3.363
5.316
-3.363
.
-5.316
#
滋
福
S
-6.391
-3.820
2
3176
4.28
2382
福
.214
-3.176
-4.828
2.382
0.38
-3.176
-4.828
-2.382
4.823
4.381
州
-1.897
福
润明用因建家
-3.176
-4.828
-2.382
4.823
-5.415
-5.631
您
-Nn-Nnt
#
-5.768
-3.109
5.18
5.20
2.48
-6916
-.78
-3.109
5.11
0.305
654
7.89
-1.739
-5.768
-3.109
-5.118
-3.062
-2.828
-3.724
周明国美民
4.771
1.739
5.768
-3.109
511
3.623
4.565
-.890
-.853
0.033
0.018
0.005
0.013
0.004
0.162
0.008
0.00
0.033
0.018
0.005
0.022
0.002
0.033
0.018
0.005
8n
0.078
8.
0.012
8T
03
0.018
00
0.013
0.006
0.343
6020
0.
1
0.040
08
0.085
08
0.229
0.3
00
00
中A
Nnt
P
0.040
0.008
0.008
%
0.085

0.008
0.302
0.001
0.006
0.012
0.08
0.004
O
8m
国馨餐通具
-en-e-ee
0.49
0.03
0.043
00
006
07
0.149
0.003
0.043
0.006
0.576
0.001
0.002
8福
0.043
0.006
0.045
0.056

0.024
0.008
0.006
池化层P1
&a
國国路3心国豆居
0.033
S
2
国居誉警图客
0.043
0.026
0.003
S
0.001
0.53
0.130
0.576
0.056
翰出层
一Nn
-1.841
中·国美德目
照
0.038
居国月国具售国一国酒集居
0.157
复制到96张图像数据中
将处理第1张图像时嵌入的各个函数复制到所有学习数据中(96张图像）。
深度学习的数学.indd 196
2019/7/2 10:47:225-4用Excel体验卷积神经网络

197
⑤算出代价函数C的值
利用5-3节的式(7)求出代价丽数C.的值
G46
C
r
=SUM(L46:V046)
ABCD
HI
K
PQ
┅〃→m〃〃〃日井华四潜日华日明田司司篷网商{薯日蜀R司冽岗网渭阁岗髑将图导日驯导羽期
数字1、2、3的识别
中
销号
位模式
层
创
-1.277
n
老销当岗摩。
-0.454
0.358
卷积层z
S88
-3.363
-3.994
-5.316
-4.296
种乐更
L
.
绿海國商际!
、
楼
22
F3
a
#
-3.176
蓝
4.28
国司晾用际
海华马时日
莱
小
1
1.739
福
-5.768
3.109
博早早司
FM
bias
盛利串国民
-3.363
腾科活拳码团
-3.176
海国园司
1.739
2

&
福
照
Nn-Nn→一Nn一Nn→

-1.739

-5.768
.
-3.109
#
-5.118
-3.680
0.94
s
0.03
0.018
005
0.033
0.018
0.005
0.013
氟刊聪招多枫品华则
a
果国鲜利区
内明调利国会图副部
0.033
0.018
0.005
0.013
同国品
an
国居美鲜客民
0.033
国国誉营店
0.018
黑割鲜售售
福

0.08
国生时同3园明博鲜所
S
0.003
°
国运码银园
国酒客客割
0.085
E
08
同S8
国利糕温牌
0.006
池化层
®
日
Obias
2.060
.746
-1.818
P3
一m-一N
翰出层
•
-rn
C [12.544
周路周3售副目心国明销中
里国具国留销目客心通国区
0038
算出代价函数
平方误差的和
(5-3节式(7)）
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 197
2019/7/2 10:4722198第5章深度学习和卷积神经网络
6利用求解器执行最优化
利用Excel的标准插件求解器，计算出代价函数C.的最小值。如下
图所示，设置单元格地址，并运行求解器。
求解器的设置
规划求郎参数
ea
X
设量目标:心
p3+
国
设置代价函数的单元格
到
〇最大值心
画最小值口心目标值:四
O
通过电改可变单元格心
SES$1Z:$621,$2322:$7339.$I34O:G$40
国
设置过滤器
遵守约束心
权重和偏置的
添加
单元格
更改回
柑除⑩
全部垂置的
装入/保存心
O鲜米X联变鲜宽饮T
选择求解方法；
丰线性CRG
项e
求解方：
孕*昇烤望髡酱搬芬果昇离途瘥孚鉴花塑手日擎。才线胜叔就味闽问题法经羊毛载性规刻引
y2x9
西锅制容动馨店
16
博国路身号9的车所
-0.848
福军石陶用各零到厂
1356
帮助⑩
求解关闭！
F
9日9
福
9
14.76
3.572
4.879
22
z9,|P1
23
石好85
创
a
S&8可
容
集子
.P
福店明月到时酒鲜鸡酒副部

石图显示了求解器算出的过滤
科8高好胃58
洲吴盟
P3
.450

器、权重和偏置的值。由于代价函
2
数C.的值为0,所以可知这个卷积
B
O层bias
居到:男图明有事店
14.764 -21.116-9.164
国日季日国牌建目路国明零配明路副
神经网络完美地拟合了学习数据
心
99$9
C0.000
深度学习的数学.indd 198
2019/7/2 10:47:225-4 用Excel体验卷积神经网络199
测试
为了确认步骤6中得到的过滤器、权重和偏置确定的卷
积神经网络是否能正确地工作，我们试着输入新的数据,例
如右边的图像。卷积神经网络的判断结果是数字“1"，这与
人类的直观感受一致
]l47
vC
&=WLOOKUP(MAX(M43:M45),M43:N45,2,0)
HIJKL
NOPQ
一川司一·山-回小日日9日日：司对时民：足期日奥民局对别网茨冠期际第影导斗斗导事导导、
测试编号
*会
立模式
和钻迎
诫鹘串
®
一医季胡烤美部
·国与国专家肉
·银器明各学司
测试图像的
13.492 -10.765 -16.205
4.986
1
-8.543
-9.218
0.288
5.587
像素模式

2
-13.572
2
3

5
福福
2留
-29.287
文
理
1
5.422
.540
-0.853
FM
-14.706
13.572
-4.879
9.69
11.752
-5.013
o
福
-0.317
0.151
福
-13.079
9.699
11.752
5.013
&
-3.23
1.323
4.649
2.266
13
7580

0.00
0.00
0.0
00
输
福
日季团医新

0.00
Mn
中国居利团
33更名影黄品华利
品同国品E
国鲜都酒禁医用所
1.0
国鲜爱居国鲜拿店
B
0.00
.00
国通酒国店
0.00
宝
04
C
国创营誉国务
日送美鲜国酒
ow
周际国目明背导
月母馨居明居
0.008
00
0.0
008
0.00
0.00
S
0.000
0.001
他化层
a
-0.857
22
00
国酒香团
0层bias
14.764 -21116 -9.164
P3
88
0.009
输出层
国留到国居车
数字
输出最大值的神经
1
16.125
1.000
-1.659
0.000
单元的编号就是判
-9.209
0.000
定的数字
47
判定
6中得到的过滤
器、权重和偏置
数字的判定结果
这个例子中输入了与字母丨相似的数字1的图像。尽管如此，判定结果也是1。
深度学习的数学.indd 199
2019/7/2 10:47:23200
第5章深度学习和卷积神经网络
5-5
卷积神经网络和误差反向传播法
第4章我们考察了多层神经网络的误差反向传播法的结构及其计算
方法。本节我们来考察卷积神经网络的误差反向传播法的结构。其实它
在数学上的结构与误差反向传播法相同。我们通过下面这个之前考察过
的具体例子进行讨论。
例题建立一个神经网络，用来识别通过6×6像素的图像读取的手写
数字1、2、3。过滤器共有3种，其大小为3×3。图像像素为单色二
值，学习数据为96张图像
确认关系式
对于这个例题，我们建立了如下图所示的卷积神经网络并进行了讲
解。接下来，我们来汇总一下前面考察过的关于这个网络的关系式
池化层
卷积层
BBdD
输出层
输入层
CmKDEADIDAIErin
迎ddB
X2DJGmyrnYCrz)(x2D
田dBB
$31X(r32)(x3)(s#)(3s
al)a
x4X2x4s)
国田田®
"sJXs3YXxs3)xs)xs)(
aDa5)a5Yab
aDaD
卷积
CEaYEaYxaYrod
迎田临
注:神经单元的名称使用了输出变量名。
●卷积层
k为卷积层的子层编号，i、j(i,j= 1,2,3,4）为扫描的起始行、列的
深度学习的数学.indd 200
2019/7/2 10:47:235-5 卷积神经网络和误差反向传播法201
编号，有以下关系式成立（5-3节式(1)、式(2))。a(z）表示激活函数。
zJ*=wx,+wxnNxn
+wEf*,u,+"%*+n+5fx;u1+2
+wf+,+g$+nga+n%f5%ng+2+b%
(1)
aj`=a(z*)
●池化层
k为池化层的子层编号（k=1, 2,3)，i、j为该子层中神经单元的行
列编号（i,j=1,2)，有以下关系式成立（这里是最大池化的情况，参考
5-3 节式(3)）。
z* =Maxa4- 292, 22.1 22)
aj*`=z%
(2)
注：Max函数输出（）内最大项的值。

●输出层
n为输出层神经单元的编号（n=1,2, 3)(5-3节式(4)、式(5))，有
以下关系式成立。a(z)表示激活函数。
z`=w2;,a" + w2%za" + w2,a5l +w,a55
+ w2,a? +w%za[? +w2,aji +w2,a,?
+ w1,4f?`+ w9.aj}`+w?,a5i`+ w2,a}+b%
(3)
a,=a(z,)
●平方误差
t、tz、t；为表示学习数据正解的变量，C为表示平方误差的变量，
有以下关系式成立（5-3节式（6)）。
C=
-;{(;-?9)`+(:-=29)+(:,-4;9)}
(4)
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 201
2019/7/2 10:47:23202第5章深度学习和卷积神经网络
梯度下降法是基础
第4章中应用了梯度下降法来确定神经网络的参数。同样地,在确
定卷积神经网络的参数时，梯度下降法也是基础。以C.为代价函数，梯
度下降法的基本式可以如下表示（2-10节）。
(Awil, -, AW . ., -, b,-)
aC,
aC
0Cp
0C,
(5)
n
'ong'
obFi
db'
式子右边的括号中为代价丽数C·的梯度。如式(5)所示，这里以关
于过滤器的偏导数、关于权重的偏导数，以及关于偏置的偏导数作为分
量（共69个分量）。
代价函数C的梯度
oC
oc,
dC,
oC,
(on;,T
'ongt
'B7
'Ob°
儿
关于过滤器
关于输出层神经
关于卷积层神经
关于输出层神
的偏导数
单元的权重的偏
单元的偏置的偏
经单元的偏置
导数
导数
的偏导数
正如第4章中考察的那样，这个梯度的偏导数计算非常麻烦。因此，
人们想出了误差反向传播法，具体来说就是将梯度分量的偏导数计算控
制到最小限度，并通过递推关系式进行计算
省略变量符号中附带的图像编号
从式(5)可以看出，代价函数C.是梯度计算的目标。把从学习数据的
深度学习的数学.indd 202
2019/7/2 10:47:235-5 卷积神经网络和误差反向传播法203
第k张图像得到的平方误差式(4)的值记为G，代价函数C·可以如下求出。
Cr=C,+C, +w+C。（96是学习数据的图像数目）
(6)
从式(6)中也可以看出，代价函数C.是从学习数据的各个图像得到
的平方误差式(4)的和。我们在4-1节考察过，求代价函数C.的偏导数
时，先对式(4)求偏导数，然后代人图像实例，并对所有学习数据求和就
可以了。因此，从现在开始，我们考虑以式(4)为对象的代价函数的计算。
例1求式(5）右边的梯度分量2，时，如果先求式()的C-再求偏导数，
owil
就会浪费不少工夫。首先计算式(4)的平方误差C的偏导数，然后将图像
实例代人式中，算出BC
-[k=1, 2, ….96(96 为全部图像的数目)]，最
awt
后对全部数据进行求和就可以了。这样极大地减少了偏导数的计算次数。
计算方法1（偏导数的计算次数为96次）
将数据代入
dCdC.aC,
aC.
式(4)的C中
>C=CtC,t..tC
Owfl
owfl owl
Ow.
计算方法2（偏导数的计算次数为1次）
对式(4)的C
dC
dC.
求偏导数
一将数据代入
一代入数据后的和
`Ow,
Ow,
利用计算方法2极大地减少了偏导数的计算次数。
之后我们将按照例1的方法进行计算。因此，除了必要的情况之外
不再将图像编号表现在关系式中。
符号战的导入及偏导数的关系
与第4章一样，我们在误差反向传播法中导人名为神经单元误差的。
符号。现在我们考察的例题中，神经单元误差，有两种：一种是严的形
深度学习的数学.indd 203
2019/7/2 10:47:24204第5章深度学习和卷积神经网络
式，表示卷积层第k个子层的i行j列的神经单元误差；另一种是，的形
式，表示输出层第n个神经单元的误差。与第4章一样，这些符号是
通过关于加权输人z、z；(式(1)、式(3)）的偏导数来定义的
aC
oj
8"L0C
0z]%
z%
(7)
例2科
0C
(卷积层第1个子层的1行1列的神经单元的误差）
az|
=.
oC
（输出层第1个神经单元的误差）
2,0
卷积层（子层1
q"'- C
az|
5o
_0C
Oz9
爱车
池化层(子层1）
输出层
例2的变量的位置关系（神经单元的表示请参考3-1节）。
与第 4章的神经网络的情况一样，平方误差C关于参数的偏导数可
以通过这些神经单元误差 简洁地表示。接下来，我们来考察这个事实。
用8/表示关于输出层神经单元的梯度分量
利用式(3)、式（7)和偏导数链式法则（2-8节)，我们可以进行下面
的例3、例4的计算。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 204
2019/7/2 10:47:245-5 卷积神经网络和误差反向传播法205
例3
aCoCz9
0msan29 ong2La
=8a?
例4
oC
aC 0z
b4297800
0z,0
-=a
O
=%
On2zn
020
池化层第2个子
屋的2行1列的
权重
w221
zY
神经单元
bY
oC
平方误差
例3的变量和参数
ow2n
的关系图。
我们可以将例3、例4
一般化为如下的式(8)。这里，n为输出层的
神经单元编号，k为池化层的子层编号，i、j为过滤器的行、列编号（i
j=1,2)。
dC
=6%ag*,
0C
=og
(8)
omp",
0b,
用8/表示关于卷积层神经单元的梯度分量
下面我们来考察关于卷积层神经单元的梯度分量。这里取过滤器分
量的偏导数作为例子。首先,根据式(1),有
z=wlx,+wx, +wx, +wx, +w5!x., + w]x,
+w}sq+n5gs,+n;.i,+bP"
z=wilx,+wx,+wxa+wx,, W.xz, +wx,
+wlx,, +w!x+wxq+bF!
zEl=wlxaa+ wilxas + wi}xa. Wwls + w!xss Wlxso
+wa+w!xo+"n!:x+b"
深度学习的数学.indd 205
2019/7/2 10:47:25206第5章深度学习和卷积神经网络
利用这些式子，可以得到下式。
Ot adn
0z|
dz
2z月
=X,·
Ompfi`.i,
(9)
根据链式法则,有
ac
0C azl
OC ozB
aC azl
onazf on
az, owl
2al owH
(10)
把8的定义式(7)和式(9)代入式（(10）中，得到
8%g=+4+&:+8%.
ac
(11)
aC
Oz
owl
dC
owa
=o
Ozf
卷积层(子层1）
wilwl wl
图像X
V5
wI
wl
平方
误差
过滤器
az,
OwF
D
式(11）的右边第一项和最后一项的变量关系图。
我们可以很容易地将式(11)扩展到过滤器的其他分量。设飞为过滤
器的编号（这里与卷积层的编号相同)，i、i为过滤器的行、列编号（i、
j= 1,2,3 )，将上式进行一般化，如下所示。
aC
=+区2省小8
(12)
owi*
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 206
2019/7/2 10:47:255-5 卷积神经网络和误差反向传播法207
注：这是像素数为 6×6、过滤器大小为3×3时的关系式。在其他情况下，需要根据实
际情况对该式进行相应的改变。
此外，代价函数关于卷积层神经单元的偏置的偏导数可以如下求得
卷积层各个子层的所有神经单元的偏置都是相同的，例如对于第一个特
征映射来说，可以得到下面的关系式。这与式(12)是一样的
oC
0C ajl
dC 2zl
oC oal
ab
az| ab"
0z, db"
azll obil
(13)
=6|+68!+-+8[!
oC
式（13)的右边第一项和最
az|
=8f|
后一项的变量关系图。
ab
dC
BFI
平方
误差
C
6C
az4n
GzM
oC
abl
Ob'
式（13)可以如下进行推广，其中k为卷积层的子层编号。简而言之
代价函数关于卷积层神经单元的偏置的偏导数，就是卷积层各个子层的
所有神经单元误差的总和。
萧省学心
oC
(14)
注：这是像素数为6×6、过滤器大小为3×3时的式子。在其他情况下，需要根据实际
情况对该式进行相应的改变。
深度学习的数学.indd 207
2019/7/2 10:47:26208第5章深度学习和卷积神经网络
像这样，由式(8)、式（12)和式（14)可知，如果能求出神经单元误差
6，就可以求出式（5)的所有梯度分量。因此我们的下一个课题就是计算
由式(7)定义的神经单元误差。
计算输出层的。
与简单的神经网络（4-3节）的情况一样，计算神经单元误差也
是利用数列的递推关系式（2-2节)。首先求出输出层的神经单元误差8
接着通过递推关系式反向地求出卷积层的神经单元误差司
下面我们先来求输出层的神经单元误差6。激活函数为a(z)，n为该
层的神经单元编号,根据定义式(7),有
6;0=
ac ac a. aC
ga'(z;)
0z% aa%0zTa9
(15)
根据式(4),有


dC
a,
=a,-t, (n=1, 2, 3)
(16)
将式(16)代入到式(15)中，就得到了输出层的神经单元误差。
6, =(a,-t,)a'(z,)
(17)
建立关于卷积层神经单元误差的“反向”递推关系式
与神经网络的情况一样（4-3节)，接下来要做的就是建立“反向”
递推关系式。我们以!为例进行考察。根据偏导数的链式法则，有
洲=
oCdC az Oaft ozl oafl
ozHa2 a' o2| af" ozH
(18)
dC Oz oafl ozl dafl oC Oz: oat ozl cal
oz" al zH oal ozH
T0z9 al azP a ozi
深度学习的数学.indd 208
2019/7/2 10:47:265-5 卷积神经网络和误差反向传播法
209
afl
2zF
卷积层(子层1）
0z
afl afl
ac
aR
Gzy
输出层
z9
aP
亚方
误差
池化层（子层1）
oC
2MdP
式（18)的右边的变量关系图
把式（18）中的公因式提取出来，就可以像下面这样进行简化
ac oz°
dC 0z9
o=
ac az? af azil al
0z` a
Gz; 0a'
o2 au 2 af| ozl
(19)
根据式(3),有
0z
0z9
al
-= wl,
a'
=wi.
0zg
= win
(20)
aaf
再根据式(2)，有
a'=zf,zl=Max(afl, a[, al, a)
(21)
根据式(21)中的al=丹，可得
等·
(22)
此外，由于a科、,、引、a男在进行池化时形成一个区块，所以
Max(al, ay, ad,a,)的偏导数可以如下表示。
深度学习的数学.indd 209
2019/7/2 10:47:27210
第5章深度学习和卷积神经网络
Ozl
(1（在区块中a，是最大时）
aan
1o（在区块中a科不是最大时)
(23)
tyaff
也可以记为a(z烈)，把6的定义式（)以及式（20)～(23)
0z|
代入式（19)，可得
s"={8Pw2ln+6w2n+68w2}x1
x（当州在区块中最大时为1，否则为0）×a'(z)
(24)
其他的神经单元误差也可以用同样的方式进行计算，因此上式可以
推广如下。
8j*={On2l+6"w2;+6w2j,}
×(当a影在区块中最大时为1，否则为0)×a'(z)
(25)
这里，k、i、i等的含义与前面相同。此外，’、；表示卷积层i行i列的
神经单元连接的池化层神经单元的位置。
例58-18w22+6902+82222}
x（当a是在区块中最大时为1，否则为0）×a'(zl)
卷积层(子层1
(当a弘在区块中最大时为1，否则为0）
a'(z5i)
输出层
80
池化层
平方
误差
文块
65
例5中出现的变量的关系。
深度学习的数学.indd 210
2019/7/2 10:47:275-5 卷积神经网络和误差反向传播法
211
这样我们就得到了输出层和卷积层中定义的神经单元误差 的关系
式（也就是递推关系式)。输出层的神经单元误差6已经根据式(17)得到
了，因此利用关系式（25)，即使不进行导数计算，也可以求得卷积层的神
经单元误差6。这就是卷积神经网络的误差反向传播法的结构。
卷积层
输出层
误差反向传播法的结构。只要求
出输出层的神经单元误差6，就
可以简单地求出卷积层的神经
单元误差。
问题证明例5的关系式。
解与式(24)的证明一样，如下所示。
6[!=
aC
[C az9
0C 0z9
ac az? af, oz, al
0zI
az* a,,
0z, 0a
0z9 aay, 25, a z,l
0z0
O1
0z9
0z,0
ag,=nL,
公
aft ng
a=zt z% =Max(a, a, ad, al)
aa'
's1,Q0
(1（在区块中a男是最大时)
Gz5
a
0（在区块中a，不是最大时)
al
由于
也可以记为a(z恩)，所以根据以上式子可得
z%
65! ={6w2, +89w2 +6w2z} x1
×（当以在区块中最大时为1，否则为0）× a(zl）
这样就得到了例5的式子。
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 2l
2019/7/2 10:47:28212第5章深度学习和卷积神经网络
5-6
用Excel体验卷积神经网络的
误差反向传播法
与第4章中考察的神经网络一样，在卷积神经网络中也可以利用误
差反向传播法。下面我们利用前面考察过的以下例题，用Excel实际地
进行计算。
注：计算步骤与4-4 节相同。
例题对于5-5 节中考察的卷积神经网络，我们来确定它的过滤器、权
重、偏置的值。学习数据的96张图像实例收录在附录B中。激活丽数
使用 Sigmoid丽数
作为解答示例的神经网络请参考5-1节，变量和参数的关系式请参
考5-5节。现在，我们来进行具体的计算
读入学习用的图像数据
为了让卷积神经网络进行学习，需要用到学习数据。因此，与5-4
节的步骤①同样地读入图像数据。
HIJ
OP
VM VN
编号
输
位
入
香
〒
0
上
2设置过滤器分量、权重和偏置的初始值
现在的过滤器分量、权重和偏置当然是未知的，需要以初始值为出
发点来求出。因此，我们利用正态分布随机数（2-1节）来设置初始值。
深度学习的数学.indd 212
2019/7/2 10:47:285-6用Excel体验卷积神经网络的误差反向传播法

213
此外还要设置小的正数作为学习率，
AABCD
设置学习率n
oor-oo
12
创2女 9
参数
F1
1277
-0.454
0.358
I
-2.398
-.64
0.899
0.675
2
-1.274
238
2.301
:22R
籼导吧
0.649
-0.39
-2.054
.022
.1204
1.900
F3
869
2.044
1.290
-1.710
-2.091
-2.946
aN3习
0.201
-1.323
0.207
bia
3.363
-3.176
-1.739
P1
-0.276
0.124
O
-0.961
33&8
层
0.718
一早
P2
-3.680
-0.594
0.280
-0.782
重
P3
1.475
-2.010
过滤器分量、权重和
1.085
-0188
偏置的初始值
28
P1
0.010
0.661
ON
1.591
2.189
88
I删
p2
1.728
m
d
0.250
P
0.238
34
2246
35
P1
132
-0.218
m
8河
ONn导洲
3.327
0.061
在从单元格D13开始的区域中
e
0.613
0.218
设置过滤器分量、权重和偏置
38
2.130
-1.678
a9
的初始值。一共由69个参数
P3
1.236
-0.486
-0144
-1.35
构成。这里利用了标准正态分
0层bia
2.060
-2.746
-1.818
布随机数来设置初始值。
Men
：备注学习率，的设置
就像第4章中考察的那样，在设置学习率时需要进行反复试错。如果
1过小，则代价函数C斗 不能迅速地达到最小值，也可能掉进意料之外的极小
值处。反之，如果，过大，则存在代价函数C+不收敛的风险。我们的目标是
将代价函数C1最小化，为了使C+的值变得充分小，需要尝试各种不同的值
来计算。
深度学习的数学.indd 213
2019/7/2 10:47:28214第5章深度学习和卷积神经网络
3算出神经单元的输出值以及平方误差C
对于第1张图像，利用当前给出的过滤器分量、权重和偏置的值来
求出各个神经单元的加权输人、激活丽数的值以及平方误差C。
EAABC
DE
F
GHI了
X
L
M
N
OP
一〃川寸灬山~山司当日绸司对剁闷对搁阉阿鞫则副剥岗囡可闵岗闵闵囡导日导牙孑导等
数宇1、2、3的识别（Sigmoid）
编号
0
0
0
100
匆
0.2
翰
®
0
oooo
0

oo
0
人聪
郫郁
ooo
ooo
1
0
0
-0.794
0.899
0.675
卷
-3.363
-3.994
-5.316
4.296
巢乐
F2
-1.274
2.338
2.301
乐聪具5
-3.363
-3.994
-5.316
-4.296
0.649
039
-2.054
令
-3.176
4.828
-2.382
-4.823
层
-1022
-1.204
-1.900
-3.176
4.828
-2.382
-4.823
F3
.869
2.0
-1.290
-.176
4.828
-2.382
4.823
卷积层神经单
-1,710
-2.091
-2.946
-3.176
-4.828
-2.382
-4.823
0.201
1.23
0.207
翰
n
元的加权输入
-1.739
-5.768
-3.109
-5.118
bis
-3.363
-3.176
-1.739
入
-1.739
-5.768
-3.109
-5.118
(5-5节式(1））
口聪一福例
Pl
-0.276
0.124
-1.739
-5.768
-3.109
-5.118
-0.961
0.718
-1.739
-5.768
-3.109
-5.118
P2
-3.680
-0.594
3
0.033
0.018
0.005
0.013
0.280
0.782
0.033
0.018
0.005
0.013
P3
-1.475
-2.010
0.033
0.018
0.005
0.013

-.08
0.18
卷
o
PI
0.010
北福限发品
0.033
0.018
0.005
0.013
E
-.591
0.661
0.040
0.008
0.085
0.008
柜一福圆
2.189
层
0.040
0.008
0.085
0.008
卷积层神经单
P2
1.728
03
0.040
0.008
0.085
0.008
-0.250
0.085
0.008
元的输出(5-5
的
0.238
福
1.898
品解
0.040
0.008
出
3
0.149
0.003
0.043
0.006
节式(1))
2.246
0.149
0.003
0.043
0.006
Pl
-.22
-0.218
0.149
0.003
0.043
0.006
o
3.527
0.01
0.149
0.00
0.043
0.006
n55
P2
0.613
0.218
'
0.033
0.013
2150
-1678
0.033
0.013
p3
1.236
-0.486
e
n
味
0.040
0.085
-0144
-1235
0.040
0.085
池化层神经单元的输
O层bis
2.060
-2.746
-1.818
a'
0.149
0.043
出(5-5节式(2))
0.149
0.043
交量数
69
常3
29
县
一N
1.300
0.786
输出层神经单元的输
-2.106
0.109
出(5-5节式(3))
841
0.137
47
1次CT
0.038
平方误差(5-5节式(4)）
深度学习的数学.indd 214
2019/7/2 10:47:285-6用Excel体验卷积神经网络的误差反向传播法
215
4根据误差反向传播法计算各层的神经单元误差8
首先，计算输出层的神经单元误差6®（5-5节式（17))。接着，根据
“反向”递推关系式计算8(5-5节式（25))。
⑤根据神经单元误差计算平方误差C的偏导数
根据步骤の中求出的6，计算平方误差C关于过滤器、权重和偏置
的偏导数。
蜜子冒或淡眢羸货欧屑鳖断品或站器减囡技裹郊酱斗一斗鹞芒单斗F曾茗或就哥菡窦板畸
47
1次C 12.544
C
0.038
0层8
-0.036
011 0.016
P
0.000
000.000
0.000
H%
0.00
00
0.00
00
o
0.002
00
00
0.0
鸿打0
和乐咀
"o
0.006
0.0
0.002
0.0
④计算神经单
0.006
0.0
0.002
0.0
元误差(5-5
.
00
0.002
0
02
福
节式(17)、式
0.010
00
03
000
(25))
0.010
0.00
0.003
000
参数的梯度1
0.008
00
-001
000
23
0.008
0.0
-0.001
0.000
F1
S
06
60
0.000
0.0
0.000
%
F2
F2
.
国居售家店
m
⑤平方误差关于过滤
料导咀
购导啡
器的偏导数(5-5
0.0
0.0
F3
F:
0.000
007
00
00
节式(12))
0.000
0.005
000
0m
0005
000
bis
5平方误差关于卷积
bia
016
0040
o惠一癸删
Pl
。
口服一服
00
层神经单元的偏置
2
2a
00
(14))
P3
俱账猥鼋睚叻蠡
a
.
的偏导数(5-5节式
P3
g
国早餐零骨配
1%
0
-0.005
o驵~袅删◎咀宀刷恻
PI
o
瓦
00
⑤平方误差关于输出层
P2
2
0.0
。
D咖~K洲
田
蓝
神经单元的权重的偏
寻数(5-5节式(8)）
0.002
海香国艺
0.0
己
』:
ONn毕删
PI
0.001
0
0.001
0
P2
00
0.001
P3
0.002
002
国居店
0.00
⑤平方误差关于输出层
89O层bias
O层bias
.03600110.016
神经单元的偏置的偏
导数(5-5节式(8)）
深度学习的数学.indd 215
2019/7/2 10:47:28216第5章深度学习和卷积神经网络
⑥计算代价函数C.及其梯度VC
到目前为止,我们以第1张图像作为学习数据的代表进行了考察
我们的目标是把前面的计算结果对全部数据加起来，
得到代价丽数C及
其梯度值。因此，必须把前面建立的工作表复制到全部学习数据的96张
图像上。
GHIJ
K
L
NOF
Ⅰ
VT
VK
VL
VIVI
1次
z
-3.363 -3994 -5.316 4296
33
-5.403
-1.645
-4.826
-9.052
-3.994
-5.316
-4.296
-3.817
-6.304
-6.391
-3.820
狍导呱盎長毕彝〆
-3.363
-394
5.316
.4.296
-2.464
3.99
4.448
-5.918
-3.363
-3.994
-5.316
-4.296
-5.085
z
-0.651
-3.889
7.75
-3.176
4.828
2.32
-4.823
-1.214
0.213
-4.969
-6.732
-3.176
-4.828
-2.382
4.823
-0.838
-3.176
-4.828
-2.382
-4.823
6.504
-5.168
0.569
3.176
4828
2.382
4.823
-4.381
-1.897
-2.490
-.56
-5.415
-5.631
-7.055
-5.458
2
-1.739
1.139
-5.768
3.109
-1.739
3168
-3.109
-.1
-.768
m
-5.120
-2.48
-6.916
-7.723
0.305
-6.554
-7.859
-6.109
-3.109
-1.739
-3.062
-2.828
-3.724
4.771
-5.768
-3.109
-.1
-3.623
a"
0.033
0.018
0.005
0.013
4.565
-6.890
-5.853
0.004
0.162
0.008
000
0.033
0.018
0.005
0.013
0.033
S
0.005
0.013
0.022
0.002
0.002
0.021
遮羽家画堙
0.033
.00
0.013
0.078
0.022
0.012
0.003
0.006
0.343
0.020
瓢呱品彝打
a”
0.040
0.008
0.085
008
0.0
0.040
m
0.085
0.008
0.229
0.553
0.007
0.001
0.040
0.05
0.008
0.302
0.001
0.006
0.638
0.040
0.0
0.085
0.008
0.012
0.130
0.077
0.004
a
0.149
0.003
0.043
0.006
0.004
0.004
0.001
0.149
0.003
0.043
0.006
0.006
0.077
0.001
m
0.149
005
0.043
0.006
0.576
0.001
00
0.002
0.149
03
0.043
0.006
0.045
0.056
0.024
0.008
0.033
0.013
0.026
0.010
0.001
0.003
口
景￥心
0.033
0.013
0.162
0.021
a
0.040
0.085
0.343
0.020
0.040
0.085
0.553
0.638
AJ20
OA7
A

窑哩业粲
P3
000
0.00
0.000
0.00
0.00
000
0.000
000
8
QN®癸删
PI
000
日
Oo
000
0.00
0.0
0.00
000
00
000
000
00
0.000
000
00
O
8
000
0.00

00
00
0.000
ONn刷洲
P1
00
0.001
00
m
5200
00
0.001
00
0.00
P2
00
00
00
00
0.0
0.000
m
P3
0.000
000
00
0.00
0.000
000
0.000
0.000
O层bias
-0.002 0.0010.003
0.000
0.000
0.000
复制函数到96张图像数据上
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权
深度学习的数学.indd 216
2019/7/2 10:47:295-6用Excel体验卷积神经网络的误差反向传播法217
对96张图像复制完毕之后，将平方误差C,以及步骤5中求得的平
方误差·关于参数的偏导数加起来，这样就算出了代价函数的值和梯度
(5-5节式(6。
47
1次C12.5443
0.038
48
0o%;
-0.036
0.011
0.016
子昂可8岗可贸贸贸贸若石斗及古姿8与署哥日：
8"
000
0.00
0.000
0.000
0.00
0.00
0.002
0.0
96张图像的平方误差C
0.002
000
0
算
8'
0.006
00
的总和就是代价函数C
打0
卿导咀
福
(5-5节式(6))
as
8o
gR
0.010
国色鱼
0.003
0.00
居国国鲜利店
0.010
周国鲜鲜居利急鲜店
.0
福牌明民鲜用保家
0.003
0.00
参数的梯度
000
0.000
F1
-0.017
-0.221
-2.303
-3.463
723
S
F1
导呱
F2
0.148
照
1.6
0.052
F2
00
1.599
0.433
6322
0.00
00
0.189
0.927
0341
豹导吨
0.0
00
0.04
"
-0.024
F3
00
0.0
0.031
0.02
0.00
00
0.165
0.177
-0.640
S
0.00
00
00
-2.989
-0.805
-1.156
bias
2.004
0.016
0.040
3
1
T2
o山一毕删
瓦
0.057
-0.041
P1
0.151
-.01
T
普声起巴日P
P2
0.235
-0.077
P2
-001
0.051
0.038
ON一
0
菜
00
P3
0.178
-0.115
P3
00
0115-0126
里
0.005
%
ON~毕删
瓦
%
0.05
叶仅哪洲益集啡
o
瓦
-0.302
致
79
1.515
-0.165
80
oan
0.64
吨N导洲
P2
国香国鲜家
科通国国餐司
心
日西器器冻路浴
B
100
-0.321
P3
0.294
-0.407
ONn毕删
瓦
0.291-0118
0.001
0.0
1.156
0.029
ONn刷删
品
00
0.00
P2
-2.006 -0219
P2
0
-0.181
-0.303
01
87
P3
-1.241
0.004
P3
002
88
-0.045
006
0.002
国居店
89O层bias
-2.500
DOEbias -006 0.01 0016
96张图像的平方误差C的偏导数的总和就是梯度分量的值
口根据6中求出的梯度，更新权重和偏置的值
利用梯度下降法的基本式（5-5节式(5)),更新过滤器、权重和偏置
(2-10节）。为此，在上述6的工作表下面建立新的工作表，计算出更新值
深度学习的数学.indd 217
2019/7/2 10:47:29218
第5章
深度学习和卷积神经网络
59
参数的梯度
司石器名石最发与器因同于科产工P国上P巴冠石剧8石斗8与器器冠示
F1
-0.017
-0221
-2.303
F1
-3.463
0.035
0.073
1.723
-3.677
3.091
卿导吨
F2
-0.148
1.660
-0.052
F2
-1.599
0.33
0.322
料
0.189
0.927
-0.341
鱼
-0.044
1.215
-0.024
F3
0.031
-0.228
0.022
0.165
0.177
-0.640
_二
-2.989
-0.805
bias
ON一毕画
0.057
-0.041
P1
0.151
-0.012
斗北咪北风理影
0
®
0.235
-0.077
2
®
01
0038
时一导
0.178
-0.115
3
O杰~毕御
P1
0.115
-0.126
重
0.067
005
0.198
o
P1
今
-1.515
a
0017
四早
1.00
P3
-0.294
#
-0.302
层

-0.407
画I
ONn￥删
a日
0.291
?
1.156
2.006
照
-0.219
田
-0.181
2
-0.045
菜
o世n癸
S
-006
出
P3
利用5-5节的
0层1
bias
0.580
-1.574
-2500
O园bas
式(5)和2-10
参教
2次
节的式(8)
科别石贸窝际家串昌百斗当百
F1
-1.274
0410
0.819
z
福
1州
剃乐咀
3
m
-1245
2.670
2.312
0.969
0426
2.118
京
1.060
1.390
1.831
好
-1.860
2.287
1.285
m
2.045
-2.950
1.358
0.335
z"
氢瓦
2.765
-3.015
1.508
籼影咀景旱彩集心
Q世一癸删
0237
0133
0.720
2
福
-0579
a'
吕当巨
0.291
-0.790
H用
-1.987
分
0.1
63
日名当目日创当湖 组日贸
ON~毕删
&
0024
0.659
-1.631
P2
2.250
201
036
◇
福
-0.259
2.090
新升依谢通
导呱窑解打
a
利用梯度下降法的基本
q
式（5-5节式(5) )，计算
ONn刷
品
照
1.653
出新的权重和偏置。与第
A&
3.758
0.055
1次计算2～6的块状区
1.014
0.262
aPl
2094
.1618
域空出1行，开始进行
韩
1.484
-0.487
景学吧
aR
第2次计算。
119
-0139
-23
120
O层
2.176
2.432
-1.318
a”
深度学习的数学.indd 218
2019/7/2 10:47:295-6 用Excel体验卷积神经网络的误差反向传播法
219
8反复进行3～的操作
利用⑦中算出的新的权重w和偏置b，再次执行从③开始的处理。把
这样算出的第2 次处理的块状区域复制49份到下面，进行 50 次计算。
级路锁蜜副
3883 参数
F1
#
租
50次
2Fl
3.460
4.512

-6.041
-0.984
-3.460
-4.512
-6.041
-094
-3.460
4.512
6041
F2
-0.704
3.916
儿海烤间
1.93
-3.60
4512
电萧瑞居网新闲酒厨器厨所
涮品咀
6041
3
2.963

1.392
2.967
卿影咀鼋長影
,n
3.640
5.794
3.838
F3
1.819
#
LS
m
-1719
-.368
3012
-3640
0.027
创湖葫钢
-1.687
0.354
辉人
,B
月酒汤动际
2.535
6.482
3.954
国湖湖尚阁
bas
3460
3.640
-2.535
235
6482
3.54
Q叫一毕画
P1
-.532
0733
-235
6482
3.954
-1.781
0.52
235
6.482
川尚等鲜店
3.954
-609
s
m
AS
a
%6
.01
国居家家
0.00
0272
02
a
福
m

照
002
算出的过滤器
国国纳国酒料湖钢国动司国制国汤胡园团部
0.02
分量、
权重和
ON~删
瓦
-0.693
-0467
.02
0.03
0.021
&
中用尽经动家


0.021
偏置
照
新
a
周际到综用司
3149
0.026
a®
创馨家店

A
Cn
2M8
P1
-0408
0.073
02
0.019
通居家店
0.02

Q时删
5.298
-1.77
0.073
02
0.019
0.02
P2
8
m
aP
030
.0
a
1.149
-0.47
.02
创国国瓷
X会
-0.360
-1.480
.06
o层
b as
3.154
-4271
-2.246
一马
照
001
屏打咀
2
门国高司
国意小国鱼
50次计算
3
-2.836
0.055
后代价函
3918
50次CT0
0.003
数的值
把从60行到120行的块状区域复制49份到下面。
通过以上步骤，计算就结束了。我们来看看代价函数C┐的值。
代价函数Cr=0.497
由于学习数据由96张图像构成,每张图像平均为0.005。根据平方
误差的函数（5-5节式(4)),每张图像的最大误差为 3/2 =1.5，因此可以
说以上步骤算出的是一个很好的结果。
深度学习的数学.indd 219
2019/7/2 10:47:29220
第5章深度学习和卷积神经网络
用新的数字来测试
我们创建的神经网络是用于识别手写数字1、2、3
的。我们来确认一下实际上它能否正确识别数字。下面
的Excel工作表是利用步骤⑧中得到的参数并输入右边
的图像进行计算的例子。判定结果为数字“3”
“，病…┅洪监马漫区漫坐公区双区级区期绘公线街者汽外海周路监路器限器装
AABCDEFCHIJKLINOPQ
数字1、2、3的识别（Sigmoid）
瑞号
K
1S
Fl
福
川季车驾 际肉与际
1次
盘
-1.23
位模式

福
瘐郧更
F2
出
写国号
-0.03
国与路菜车
租
-2.56
-9.97
福
-4.57
-7.94
测试用图像的
力
周台导调网所
国南商闵印
周陶海等网
海商网房汤司
食
FMbia
A6
0.35
3.64
-2.53
莉影重品具影源K
25
拉
T
州
西

P
2
0.73
#
1.19
A.
国等酒得等间
当翠网部

月总路送店
3.89
月国居酒
明甲湖居路
39
自橱务|
利用8中的
霸钥逍名影闻晷瘗科
斌汨谢帆坦
计算结果
".
62
客刻学到生司
种影聪品新丑
月居当务
0.75
3居
a
通客:
AO
8S
.
,
国客写际料医助国牌部
通3S
国季明国酒
%
0.23
0.01
0.00
0.02
0.00
0.09
a”
0.23
0.64
输出层中用于检测数
州
福
足字惠
g
8f
7
"3”的第3个神经
O层biss
3.15
-4.27
-2.25
国恩：
用
单元的输出是最大的
郭玉聪
#
国居库器目
物一一
判定为数字“3”
深度学习的数学.indd 220
2019/7/2 10:47:295-6 用Excel体验卷积神经网络的误差反向传播法
221
Mer
备注跟踪代价函数的值
跟踪50次代价函数的计算结果，就可以实际理解梯度下降法的含义。
从逻辑上看，代价函数·的值当然是随着每次迭代而减小。第4章中我们
已经考察过，梯度下降法的优点就是减小的速度是最快的。
次数
CT
次数
Cr
次数
CT
1
12.544
21
1.059
41
0.594
2
10.627
22
1.018
42
0.582
3
11.280
23
0.981
43
0.570
4
11.674
24
0.946
44
0.558
5
13.969
25
0.913
45
0.547
6
6.007
26
0.883
46
0.536
7
3.365
27
0.854
47
0.526
8
2.277
28
0.828
48
0.516
9
2.076
29
0.804
49
0.506
10
1.921
30
0.781
50
0.497
11
1.787
31
0.759

12
1.671
32
0.739
13
1.569
33
0.719
14
1.479
34
0.701
15
1.399
35
0.683
16
1.327
36
0.667
17
1.263
37
0.651
18
1.205
38
0.636
19
1.152
39
0.621
20
1.104
40
0.608
不过，用计算机执行误差反向传播法时，也存在代价函数C┐不减小的情
况。就像第4章中考察的那样，可以认为原因是学习率和初始值不合适。在
这种情况下，可以修改学习率和初始值重新进行计算。
深度学习的数学.indd 221
2019/7/2 10:47:29222
附录A
A
训练数据(1)
以下是第1章、第3章以及第4章的例题中建立的神经网络的学习
数据。用4×3像素画出数字0、1。考虑到实际情况，学习数据中也会出
现相同的图像
编号
910111213141516
123123123123123123123123123123123123123123123123
正解
000
00
编号
正角
编号
12312312312312312312312312312312312312312312312
正解
编号
正解
注：图像中的乡
或老出现斑点
羊的东西
受到了噪声的影响
深度学习的数学.indd 222
2019/7/2 10:47:29训练数据（2）223
B
训练数据2)
以下是第5章的例题中建立的神经网络的训练数据。用6×6像素画
出数字1、2、3。图像像素为单色二值（0和1)
编号
12345612345612345612345612345612
3456123456123456
冬
正解1
3456123456123
45612
45612345612345
1561
深度学习的数学.indd 223
2019/7/2 10:47:29224
附录B
456123456
6121345612
61
123
客
123456123456123456123
8
上

123456123456123



L2
123456123456123456123456123456123456123456123456
12345612345612345612345612345
456123456123
123456123456123456123456123456123456123456123456
深度学习的数学.indd 224
2019/7/2 10:47:29用数学式表示模式的相似度225
用数学式表示模式的相似度
卷积神经网络的特征映射的值以图像和过滤器的相似度作为输入信
息。相似度可以利用下面的定理进行计算。
由3×3像素构成的两个数组A、F如下图所示。A、F的相似度
可以像下面这样求出
相似度=W1Xn+W2X2+Wg3g++W3gx33
(1)
A
F
XX12X13
W12 w
这个定理可以利用向量的性质来说明。就像2-4节考察的那样，当
两个向量α、b相似时，它们的内积α·b较大。我们可以认为内积α·b的
大小表示两个向量的相似性。
a·b=|al|b|cosO (0为两个向量的夹角）
两个向量的内积是它们的箭头长度乘以夹角的余弦。夹角越
接近0，余弦的值越大。也就是说，当向量相似时，内积的
值较大。
为了利用这个性质，我们将A、F看作以下向量
A=(11, X12, 13, 21, 22, X23 X31, X82 Xs3)
F (w, ,2 W2, 2 W3,1yg, 2a, %3)
这样一来，两个向量的内积A·F就与上述的式(1)的右边一致（2-4
节。也就是说，我们可以把式（1)解释为相似度
深度学习的数学.indd 225
2019/7/2 10:47:30版权声明
DEEP LEARNING GA WAKARU SUGAKU NYUMON
by Yoshiyuki Wakui, Sadami Wakui
Copyright C 2017 Yoshiyuki Wakui, Sadami Wakui
All rights reserved.
Original Japanese edition published by Gijyutsu - Hyoron Co., Ltd., Tokyo
This Simplified Chinese language edition published by arrangement with
Gijyutsu - Hyoron Co., Ltd., Tokyo in care of Tuttle - Mori Agency, Inc., Tokyo
本书中文简体字版由Gijyutsu - Hyoron Co.. Ltd.授权人民邮电出版社独
家出版。未经出版者书面许可，不得以任何方式复制或抄袭本书内容。
版权所有，侵权必究
深度学习的数学.indd 226
2019/7/2 10:47:30微信连接
回复
查看相关书单
O
微博连接
关注@图灵教育每日分享厂好书
0
QQ连接
图灵读者官方群I:218139230
图灵读者官方群:164939616
图灵社区
iTuring.cn
在线出版，电子书，《码农》杂志，图灵访谈
图灵社区会员 ChenyangGao(2339083510@qq.com)专享尊重版权
深度学习的数学.indd 10
2019/7/2 10:46:32图文直观
穿插235幅插图和大量具体示例讲解，对易错点、重点反复
说明，通俗易懂。
结合Excel实践
书中使用Excel进行理论验证，读者可下载随书附带的
Excel示例文件，亲自动手操作，直观地体验深度学习。
只需基础的数学知识
适合数学基础薄弱的深度学习初学者阅读，有一定基础的读
者也可以通过本书加深理解。
图灵社区：iTuring.cn
热线:(010)51095186转600
分类建议 计算机/计算机数学
ISBN 978-7-115-50934-5
人民邮电出版社网址：www.ptpress.com.cn
定价:69.00元看完了
如果您对本书内容有疑问,可发邮件至contact@turingbook.com,会
有编辑或作译者协助答疑。也可访问图灵社区，参与本书讨论。
如果是有关电子书的建议或问题,请联系专用客服邮箱:
ebook@turingbook.com.
在这可以找到我们:
微博@图灵教育:好书、活动每日播报
微博@图灵社区：电子书和好文章的消息
微博@图灵新知:图灵教育的科普小组
微信图灵访谈：ituring interview,讲述码农精彩人生
微信图灵教育：turingbooks
图灵社区会员 ChenyangGao(2339083510@qq.com）专享尊重版权